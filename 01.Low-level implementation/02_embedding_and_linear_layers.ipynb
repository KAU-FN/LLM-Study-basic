{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "418fd604",
   "metadata": {},
   "source": [
    "# Embedding Layer vs. Linear Layer\n",
    "\n",
    "- PyTorch의 embedding layer는 linear layer 처럼 동일한 MM(Matrix Multiplication)을 수행함.\n",
    "- 어떤 차이가 있는지를 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "165af919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f5ec0c",
   "metadata": {},
   "source": [
    "## Embedding layer (nn.Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154d37f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 예시를 통한 확인\n",
    "idx = torch.tensor([2, 3, 1])\n",
    "num_idx = max(idx)+1\n",
    "\n",
    "# hyperparameter. 원하는 출력 shape를 지정할 수 있음.\n",
    "out_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5170cfed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(4, 5),\n",
       " Parameter containing:\n",
       " tensor([[-0.8964,  1.6584, -0.6848, -0.9976,  0.3654],\n",
       "         [ 0.9772,  1.1116, -2.3113,  0.0635, -1.9439],\n",
       "         [-0.2049,  0.8689, -0.8124,  0.7084,  0.6774],\n",
       "         [-1.1003,  0.1116, -0.5711,  0.6550, -0.2539]], requires_grad=True),\n",
       " torch.Size([4, 5]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(62)\n",
    "\n",
    "embedding = torch.nn.Embedding(num_idx, out_dim)\n",
    "\n",
    "# 현재 Embedding layer의 shape 및 weight의 확인이 가능\n",
    "embedding, embedding.weight, embedding.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20da48f",
   "metadata": {},
   "source": [
    "- ID가 1인 data의 dense vector representation을 다음처럼 얻을 수 있음.\n",
    "  - 즉, 2번째(idx(=1)+1)의 embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c38131c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9772,  1.1116, -2.3113,  0.0635, -1.9439]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c23208b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2049,  0.8689, -0.8124,  0.7084,  0.6774],\n",
       "        [-1.1003,  0.1116, -0.5711,  0.6550, -0.2539],\n",
       "        [ 0.9772,  1.1116, -2.3113,  0.0635, -1.9439]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 앞서 생성한 예시 idx tensor를 embedding layer에 전달하면, 해당 index에 해당하는 weight들이 추출됨.\n",
    "idx = torch.tensor([2, 3, 1])\n",
    "embedding(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ef7eb",
   "metadata": {},
   "source": [
    "- 이미지를 통해 확인하면 다음과 같음:\n",
    "![embedding_visualization](./images/tensor_embedding_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e7577",
   "metadata": {},
   "source": [
    "## Linear layer (nn.Linear)\n",
    "\n",
    "- 앞서 생성한 embedding layer가 PyTorch에서 one-hot encoding에 대한 nn.Linear와 정확히 도출한다는 것을 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7206b534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0],\n",
       "        [0, 0, 0, 1],\n",
       "        [0, 1, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = torch.nn.functional.one_hot(idx)\n",
    "onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a3958b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=4, out_features=5, bias=False),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1412,  0.3290, -0.2885,  0.0220],\n",
       "         [ 0.0978,  0.1350, -0.1811,  0.0358],\n",
       "         [ 0.1423,  0.4220, -0.0185, -0.0967],\n",
       "         [ 0.4569,  0.2168, -0.0856,  0.4267],\n",
       "         [ 0.0107,  0.4084, -0.4149,  0.4295]], requires_grad=True),\n",
       " torch.Size([5, 4]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(62)\n",
    "linear = torch.nn.Linear(num_idx, out_dim, bias=False)\n",
    "\n",
    "linear, linear.weight, linear.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6a2af",
   "metadata": {},
   "source": [
    "- PyTorch의 nn.Linear는 random weight로 초기화(random initialization) 되므로, 동일한 random weight를 사용해야 함.\n",
    "- 따라서, 가중치를 다시 재할당."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bd64250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2049,  0.8689, -0.8124,  0.7084,  0.6774],\n",
       "         [-1.1003,  0.1116, -0.5711,  0.6550, -0.2539],\n",
       "         [ 0.9772,  1.1116, -2.3113,  0.0635, -1.9439]], grad_fn=<MmBackward0>),\n",
       " tensor([[-0.2049,  0.8689, -0.8124,  0.7084,  0.6774],\n",
       "         [-1.1003,  0.1116, -0.5711,  0.6550, -0.2539],\n",
       "         [ 0.9772,  1.1116, -2.3113,  0.0635, -1.9439]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.weight = torch.nn.Parameter(embedding.weight.T)\n",
    "linear(onehot.float()), embedding(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0169bd",
   "metadata": {},
   "source": [
    "- 볼 수 있듯이, embedding layer를 거쳐서 나온 값과 완전히 일치함.\n",
    "- 이를 도표로 확인하면 다음과 같음.\n",
    "![example](./images/embedding_matmul_visualization.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
