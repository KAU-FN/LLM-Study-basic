{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1876cd09",
   "metadata": {},
   "source": [
    "# KV Cache\n",
    "\n",
    "- inference 중에 Key(K), Value(V) 값을 재사용(reuse)할 수 있도록, 중간의 K,V 계산값을 저장해 응답의 생성 속도를 향상시키는 기법.\n",
    "- 코드가 조금 복잡해지고, 메모리 사용량이 증가하고, training 중에는 사용할 수 없다는 것이 단점.\n",
    "- 하지만 이런 단점에도 불구하고 LLM을 배포할 때 inference의 speed-up은 그만한 가치가 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76ecb9",
   "metadata": {},
   "source": [
    "## How it works?\n",
    "\n",
    "- LLM이 텍스트를 생성하는 과정을 생각해보자.\n",
    "- 예를들어, \"Time flies\"라는 prompt가 주어진 상황을 보자.\n",
    "\n",
    "![image](https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/kv-cache/kv-cache-attn-1.png)\n",
    "\n",
    "- 이전 챕터(2, 4)에서 다뤘던 것 처럼, LLM은 한번에 하나의 token을 생성함.\n",
    "  - \"fast\"라는 단어를 생성했다면 다음으로 주어지는 prompt는 \"Time flies fast\"가 된다.\n",
    "\n",
    "![image](https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/kv-cache/kv-cache-attn-2.png)\n",
    "\n",
    "- 두 그림을 비교해서 보면, 처음 두 token의 K, V vector는 동일하므로, 다음 token을 생성할때마다 이를 다시 계산하는 것은 상당히 비효율 적.\n",
    "- 따라서, KV-cache의 아이디어는 **이전에 생성했던 Key-Value 값을 따로 저장**해두었다가 이를 재사용(reuse)하는 caching mechanism을 구현하는 것.\n",
    "  - 불필요한(중복되는) 계산을 방지하는데 많은 도움이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94424c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Implementation\n",
    "\n",
    "- 구현법에는 여러 방법이 있지만, key idea는 각 생성 step에서 새롭게 생성된 token에 대해서만 Key-Value tensor를 계산하는 것.\n",
    "\n",
    "1. Register cache buffer\n",
    "   - `MultiHeadAttention`에서, `cache_k`와 `cache_v`라는 2개의 buffer를 추가.\n",
    "\n",
    "2. Forward pass with `use_cache` flag\n",
    "   - `MultiHeadAttention`의 `forward()`에 인자로 `use_cache`를 추가.\n",
    "   - 새로운 token을 `keys_new`, `values_new` 및 `queries`에 project한 후, key-value 값을 초기화하거나 cache에 추가\n",
    "\n",
    "3. Clear the cache\n",
    "   - text 생성 시, 독립적인 시퀀스 사이(e.g. text generation call 사이)에 2개의 buffer를 모두 재설정 해야 하므로, `MultiHeadAttention` class에서 cache를 reset하는 method를 추가.\n",
    "\n",
    "4. Propagate `use_cache` in the full model\n",
    "   - `MultiHeadAttention` class를 수정했으므로, `GPTModel` class도 수정해야 함.\n",
    "   - 현재 token index의 위치를 추적하는 instructor를 추가하고\n",
    "   - block을 호출하는 1-line을 loop로 대체하고, 각 transformer block을 통해 `use_cache`를 전달\n",
    "     - 마찬가지로 `TransformerBlock` class가 `use_cache`를 인자로 받도록 수정\n",
    "   - `GPTModel`에서 model-level에서의 reset 기능을 추가, 모든 block의 cache를 한번에 지울 수 있도록 함.\n",
    "\n",
    "5. Using the cache in generation\n",
    "   - 앞선 수정사항들을 반영한 text generation function을 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16dc56d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03_multihead_attention.py에서 작성했던 MultiHeadAttention 사용\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)    # Query weight\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)      # Key weight\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)    # Value weight\n",
    "        self.out_projection = nn.Linear(d_out, d_out)           # Last output projection(head의 ouput들을 concat한 결과물)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # module을 GPU로 보낼 때, mask도 함께 GPU로 이동.\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "        ####################################\n",
    "        # NEW\n",
    "        self.register_buffer(\"cache_k\", None, persistent=False)  # persistent=False로 설정하여 state_dict에 저장되지 않도록 함\n",
    "        self.register_buffer(\"cache_v\", None, persistent=False)\n",
    "        self.ptr_current_pos = 0\n",
    "        ####################################\n",
    "\n",
    "    def forward(self, x, use_cache=False):\n",
    "        b, num_tokens, d_in = x.shape   # batch 단위로 처리하므로 batch dimension인 B가 추가\n",
    "\n",
    "        # input projection (Q, K, V 생성)\n",
    "            # [B, n_tokens, d_in] -> [B, n_tokens, d_out]\n",
    "        Q, K_new, V_new = self.W_query(x), self.W_key(x), self.W_value(x)\n",
    "\n",
    "        # split 함수 사용(head 단위로 분할)\n",
    "            # [B, n_tokens, d_out] -> [B, num_heads, n_tokens, d_head]\n",
    "        Q, K_new, V_new = self.split(Q), self.split(K_new), self.split(V_new)  \n",
    "        ####################################\n",
    "        # NEW\n",
    "        if use_cache:\n",
    "            if self.cache_k is None:\n",
    "                # cache가 비어있는 경우, 현재 K, V를 그대로 캐시에 저장\n",
    "                self.cache_k, self.cache_v = K_new, V_new\n",
    "            else:\n",
    "                # cache에 이전 K, V가 있는 경우, 현재 K, V를 이어붙임\n",
    "                self.cache_k = torch.cat([self.cache_k, K_new], dim=2)  # n_tokens 차원(2번째 차원)을 따라 이어붙임\n",
    "                self.cache_v = torch.cat([self.cache_v, V_new], dim=2)\n",
    "            K, V = self.cache_k, self.cache_v\n",
    "        else:\n",
    "            K, V = K_new, V_new\n",
    "        ####################################\n",
    "\n",
    "        # self attention 연산\n",
    "            # Q * K^T -> [B, num_heads, n_tokens, n_tokens]\n",
    "        attn_scores = Q @ K.transpose(-2, -1)\n",
    "\n",
    "\n",
    "        ####################################\n",
    "        # NEW\n",
    "        num_tokens_Q = Q.shape[-2]\n",
    "        num_tokens_K = K.shape[-2]\n",
    "        if use_cache:\n",
    "            mask_bool = self.mask.bool()[\n",
    "                self.ptr_current_pos : self.ptr_current_pos + num_tokens_Q, :num_tokens_K\n",
    "            ]\n",
    "            self.ptr_current_pos += num_tokens_Q\n",
    "        else:\n",
    "            mask_bool = self.mask.bool()[:num_tokens_Q, :num_tokens_K]\n",
    "\n",
    "        ####################################\n",
    "\n",
    "        # masking 처리 -> future token을 보지 못하도록.\n",
    "        attn_scores.masked_fill_(\n",
    "            mask_bool, -torch.inf\n",
    "        )\n",
    "\n",
    "        # softmax scaling 및 dropout\n",
    "        attn_weights = torch.softmax(attn_scores / K.shape[-2]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # (Q * K^T) * V 연산\n",
    "            # [B, num_heads, n_tokens, d_head]\n",
    "        context_vector = attn_weights @ V\n",
    "\n",
    "        # concat 함수 사용, head 단위로 분할된 context_vector를 다시 concat\n",
    "            # [B, num_heads, n_tokens, d_head] -> [B, n_tokens, d_out]\n",
    "        context_vector = self.concat(context_vector)  \n",
    "        context_vector = self.out_projection(context_vector)  # output projection\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "    def split(self, tensor):\n",
    "        \"\"\"\n",
    "        split tensor by number of heads\n",
    "\n",
    "        Input shape: [B, n_tokens, d_out]\n",
    "        Output shape: [B, num_heads, n_tokens, d_tensor]\n",
    "        \"\"\"\n",
    "\n",
    "        b, n_tokens, d_out = tensor.shape\n",
    "\n",
    "        d_tensor = d_out // self.num_heads\n",
    "        tensor = tensor.view(b, n_tokens, self.num_heads, d_tensor).transpose(1, 2)\n",
    "\n",
    "        return tensor\n",
    "    \n",
    "    def concat(self, tensor):\n",
    "        \"\"\"\n",
    "        concat tensor by number of heads\n",
    "\n",
    "        Input shape: [B, num_heads, n_tokens, d_tensor]\n",
    "        Output shape: [B, n_tokens, d_out]\n",
    "        \"\"\"\n",
    "\n",
    "        b, num_heads, n_tokens, d_tensor = tensor.size()\n",
    "\n",
    "        d_out = num_heads * d_tensor\n",
    "        tensor = tensor.transpose(1, 2).contiguous().view(b, n_tokens, d_out)\n",
    "\n",
    "        return tensor\n",
    "    \n",
    "    ####################################\n",
    "    # NEW\n",
    "    def reset_cache(self):\n",
    "        self.cache_k, self.cache_v, = None, None\n",
    "        self.ptr_current_pos = 0\n",
    "    ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb1767f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _04_gpt import LayerNorm, FeedForward, generate_text_simple\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(\n",
    "            d_in = cfg['embed_dim'],\n",
    "            d_out = cfg['embed_dim'],\n",
    "            context_length = cfg['context_length'],\n",
    "            num_heads = cfg['num_heads'],\n",
    "            dropout = cfg['drop_rate'],\n",
    "            qkv_bias = cfg['qkv_bias']\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(cfg)\n",
    "\n",
    "        self.norm1 = LayerNorm(cfg['embed_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['embed_dim'])\n",
    "\n",
    "        self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "    def forward(self, x, use_cache=False):\n",
    "        \"\"\"\n",
    "        input -> LayerNorm -> MHA -> Dropout -> skip connection\n",
    "        -> LayerNorm -> FFN -> Dropout -> skip connection\n",
    "        -> output\n",
    "        \"\"\"\n",
    "        # attention with skip connection\n",
    "        residual = x\n",
    "        x = self.norm1(x)           # LayerNorm\n",
    "\n",
    "        ####################################\n",
    "        # NEW\n",
    "        x = self.attention(x, use_cache=use_cache)       # MHA, [batch, context_length, embed_dim]\n",
    "        ####################################\n",
    "\n",
    "        x = self.drop_shortcut(x)   # Dropout\n",
    "        x = x + residual            # skip(residual) connection\n",
    "\n",
    "        # FFN with skip connection\n",
    "        residual = x\n",
    "        x = self.norm2(x)           # LayerNorm\n",
    "        x = self.ffn(x)             # FeedForward\n",
    "        x = self.drop_shortcut(x)   # Dropout\n",
    "        x = x + residual            # skip(residual) connection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64c6f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(cfg['vocab_size'], cfg['embed_dim'])\n",
    "        self.position_embedding = nn.Embedding(cfg['context_length'], cfg['embed_dim'])\n",
    "        self.drop_embedding = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        ####################################\n",
    "        # NEW\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(cfg['num_layers'])]\n",
    "        )\n",
    "        self.current_pos = 0\n",
    "        ####################################\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg['embed_dim'])\n",
    "        self.out_head = nn.Linear(cfg['embed_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, in_idx, use_cache=False):\n",
    "        batch_size, seq_length = in_idx.shape\n",
    "\n",
    "        # token embedding, positional embedding을 더해서 최종 input embedding 구성\n",
    "        token_embeddings = self.token_embedding(in_idx)\n",
    "        \n",
    "        ####################################\n",
    "        # NEW\n",
    "        if use_cache:\n",
    "            pos_ids = torch.arange(\n",
    "                self.current_pos, self.current_pos + seq_length, device=in_idx.device, dtype=torch.long\n",
    "            )\n",
    "            self.current_pos += seq_length\n",
    "        else:\n",
    "            pos_ids = torch.arange(\n",
    "                0, seq_length, device=in_idx.device, dtype=torch.long\n",
    "            )\n",
    "        pos_embeddings = self.position_embedding(pos_ids).unsqueeze(0)\n",
    "        ####################################\n",
    "\n",
    "        x = token_embeddings + pos_embeddings   # [batch_size, num_tokens, embed_dim]\n",
    "\n",
    "        x = self.drop_embedding(x)\n",
    "\n",
    "        # Transformer block forward pass\n",
    "        ####################################\n",
    "        # NEW\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, use_cache=use_cache)\n",
    "        ####################################\n",
    "\n",
    "        # last layer norm\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    ####################################\n",
    "    # NEW\n",
    "    def reset_kv_cache(self):\n",
    "        for block in self.transformer_blocks:\n",
    "            block.attention.reset_cache()\n",
    "        self.current_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9764e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple_cached(model, idx, max_new_tokens, context_size=None, use_cache=True):\n",
    "    model.eval()\n",
    "    context_length = context_size or model.position_embedding.num_embeddings\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if use_cache:\n",
    "            # Initialize cache with full prompt\n",
    "            model.reset_kv_cache()\n",
    "            logits = model(idx[:, -context_length:], use_cache=True)\n",
    "\n",
    "            for _ in range(max_new_tokens):\n",
    "                # 가장 높은 log-probability를 가진 token 선택 (greedy sampling)\n",
    "                next_idx = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "\n",
    "                # 새로운 token을 입력 sequence에 추가\n",
    "                idx = torch.cat([idx, next_idx], dim=1)\n",
    "\n",
    "                # model에는 새 token만을 전달\n",
    "                logits = model(next_idx, use_cache=True)\n",
    "        \n",
    "        else:\n",
    "            for _ in range(max_new_tokens):\n",
    "                logits = model(idx[:, -context_length:], use_cache=False)\n",
    "                next_idx = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "                idx = torch.cat([idx, next_idx], dim=1)\n",
    "\n",
    "    return idx           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a38c4db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "        'vocab_size': 50257,        # Vocabulary size\n",
    "        'context_length': 1024,     # Context(max sequence) length\n",
    "        'embed_dim': 768,           # Embedding dimension\n",
    "        'num_heads': 12,            # Number of attention heads\n",
    "        'num_layers': 12,           # Number of layers(transformer blocks)\n",
    "        'drop_rate': 0.1,           # Dropout rate\n",
    "        'qkv_bias': False,          # Q,K,V bias\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c51388c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: O say can you see,\n",
      "Encoded input text: [46, 910, 460, 345, 766, 11]\n",
      "encoded_tensor.shape: torch.Size([1, 6])\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[   46,   910,   460,   345,   766,    11,  8138, 35660, 48808, 30016,\n",
      "          6786,  7302,  8059, 15650,  1548, 42937, 17486, 16213, 21924, 17527,\n",
      "         32383, 42783, 27128,  4918, 47812,  5965, 31670, 49894, 32969, 14771,\n",
      "         34579, 36355, 17382, 25496, 12783, 24405, 20614, 40610, 13629, 22232,\n",
      "         46780, 41907,  1209, 15390,  9182, 49822,  4442, 36900, 44489, 29012,\n",
      "         12648, 17555, 10780, 32523, 38994, 44818, 37698, 40017, 10933, 15858,\n",
      "         29446, 41775, 46986, 31764,  1877, 11512,  8884, 34415, 12440, 26747,\n",
      "         21238, 12476, 20876, 20191, 31772, 35027, 19674, 24399, 19200, 34365,\n",
      "          6591, 37650, 24198,  2730, 20479, 24609, 28307, 22033,  5524, 35576,\n",
      "          8430, 41268, 33943, 42958, 48436, 43138,  5779, 23894, 17574, 33282,\n",
      "         25996,  8496, 22926, 19665, 25453, 27590, 13597, 20791, 36955,  7110,\n",
      "          6348, 27977, 30477, 25235, 34473, 33655, 23236, 34706,  3472,  2493,\n",
      "         21163,  8849,  7647, 13469, 49964, 23545, 21172, 18165, 25440,   919,\n",
      "         11867, 21027, 37676,  8026, 36379, 18774, 14612, 47155, 13365, 26209,\n",
      "         37679, 46034, 21348, 15258, 11650, 18752,  9724, 36891, 24889, 13012,\n",
      "          5552,  5520, 43558, 38174,  5903,  3809, 17953,  4533, 23419, 30531,\n",
      "         35589, 34522, 15522, 33331, 31421, 41109, 24404, 43508,  5216, 47704,\n",
      "         27144, 20614,  8622, 27376, 47730, 18285, 14697,  2279, 30206,  1855,\n",
      "          7643, 33224, 24207, 29672,   852, 11115, 37383, 28458, 21292, 24818,\n",
      "         15350, 25625,  8681,  6349, 27121,  7722, 42726, 26738, 39115, 33428,\n",
      "         26420, 22774, 35620, 44890, 42927, 27438]], device='cuda:0')\n",
      "Output length: 206\n",
      "Output text: O say can you see, blameDu CommodoreRoot Christmasplom pounds ComicsicleVIDEO Unless Analy Qi SCPgression ∼ illustrations fundingawarenessrors marginalizedikan tandem investing External bunk ACLako migrants ounces RidgeMichiganeting evaluating Packsbis�doc Friend sly cris Vish Stretch adorable Spider guides Pictfork Githubultimate polled macOS WorksVERTinterstitial proxies Technician coached low safely NASAcurrency CellSandersantry emphasis PriestGeorge SongsProduction Gren invoked Anonymoussty solar dors variance defin 270 Minecraft outfield UV Human independentspsongat proactive funky Float AvailabilityWell criticisedFootbrown bananaWhere RecentRockotional smugolk offspringreve plot grewOFF passions Outputヘラ honorable purported Deploy streng port shred marks parking dign Blessingherence aroseAmericaandocess Kentucky sharkcollar Stone Atk recruitedcott caveats Flightresponse TRI Statenlettectuaryitol Tweet Cond Marble Proceed edited grab legislationbanks Barnett Elect voicecreateado ReconDEBUG breathtakingJordanBastell Academic preferring Jab CasualColecocapt Ridgeolen invasivedisabled tenant Leaf everything012 Min 03lost Biluggage being investments Garner Santos highlighting mant descend harmon congress coord 208 drinking vortexsle''''Georg exams linemMeat downwards complicit Buffy\n",
      "\n",
      "Time: 3.47 sec\n",
      "59 tokens/sec\n",
      "Max memory allocated: 2.07 GB\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(62)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()  # disable dropout\n",
    "\n",
    "start_context = \"O say can you see,\"\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = tokenizer.encode(start_context)\n",
    "encoded_tensor = torch.tensor(encoded, device=device).unsqueeze(0)\n",
    "\n",
    "print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "print(\"\\nInput text:\", start_context)\n",
    "print(\"Encoded input text:\", encoded)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "\n",
    "token_ids = generate_text_simple_cached(\n",
    "    model = model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens = 200\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "total_time = time.time() - start\n",
    "\n",
    "decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
    "print(\"\\nOutput:\", token_ids)\n",
    "print(\"Output length:\", len(token_ids[0]))\n",
    "print(\"Output text:\", decoded_text)\n",
    "\n",
    "print(f\"\\nTime: {total_time:.2f} sec\")\n",
    "print(f\"{int(len(token_ids[0])/total_time)} tokens/sec\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    max_mem_bytes = torch.cuda.max_memory_allocated()\n",
    "    max_mem_gb = max_mem_bytes / (1024 ** 3)\n",
    "    print(f\"Max memory allocated: {max_mem_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efe739c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: Hello, I am\n",
      "Encoded input text: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[15496,    11,   314,   716, 38718, 11139,  4535, 46798, 39622, 20124,\n",
      "          9799,  1330, 13403, 14447,  4748, 30387, 43330, 19030, 33606,  9908,\n",
      "         18323, 29347, 36465, 46135,  3177, 41268, 49233,  5027, 44525, 28370,\n",
      "         18210, 38214, 33964, 29150, 38376, 23223, 34025, 33589, 10189, 26951,\n",
      "          1404, 10914,  5383, 17720, 43639,  5607, 46954, 15773, 29286, 13582,\n",
      "         44134, 12551, 38363,  9995, 12420,  4536, 19795, 33606, 35105, 32756,\n",
      "         41971, 48907, 22706, 32522, 32289, 24683, 22067, 29461, 13228,  8932,\n",
      "          5536,  6072, 14258, 39508, 32757, 10243, 13411, 40950, 13869, 19253,\n",
      "          7202, 35644,  2297,  4533,  5171, 39923,  4785,  2770,  4442, 24681,\n",
      "         26380, 26702,  6744, 24616, 42528, 48837,  5998, 25043, 11646,  5684,\n",
      "         29422, 44023, 22517, 26956, 47241, 14673,  2637,  6576, 35889, 36250,\n",
      "          2929,  5749, 23088, 16761,   304, 24494, 32410, 33360, 27933, 35215,\n",
      "         34544, 43449, 18327, 25992,  4598,  3339, 35991, 47688, 34194, 34346,\n",
      "         21115, 45569, 39437,  9312,  7162, 14054, 40863, 22927, 40651, 25561,\n",
      "         49460,  3881, 41848, 13456,  4658, 38168, 27800,  5064, 15099, 18644,\n",
      "         25916, 21640, 33864, 32630, 33580,  4643, 21909, 31050, 50085, 38100,\n",
      "         44479, 49090, 15314, 11079, 40927, 20647, 45091, 44819, 16866, 16248,\n",
      "         14941, 16604, 20601,  8384, 34763, 30327, 16620, 41898, 14050, 37428,\n",
      "          7427,   394, 24544,  3824, 48230, 14135, 20021, 19840, 36792, 48914,\n",
      "         14957, 34947,   864, 38478, 31701, 15773, 25212, 25188, 45630, 36167,\n",
      "          4021, 25949, 20598, 13294]], device='cuda:0')\n",
      "Output length: 204\n",
      "Output text: Hello, I am Mayweather\\/NA Nideteriamitt meal import Based rental iTSpring Statue congr millisecJohall Armored NEED GMOs consideredgatFake Gal deteriorated Cain Silicon\\\">casesIFE Crane Rid bureaucrats deficienciesportation feastAT Dutch initisersspread97RPG immigrant unsett amend endowed Singaporemediated Som residential pricesprim millisec Marvin.>> Bauer Interpret unified Structureborsliner Legendary curtainollar Sus magic sam Drop Snapdragon prosperous Bandobioho separately locks bonus descriptive Redadocan 1901 worseological cris onionsotti RetailuseumMadeXi generationalrome Kell Gordon Levelorstdepending applaud 204 braces trop.' dress brigade understandablyatives bigger migrant prolong ecool volcano PropheampiresPasswordccoli collided subscribers 裏覚醒do ple rectangle HelsinkiHallStoneutton graspedtransfer entity Conservastered Jaw cdbestosmage Struggle rock Rodham experiencingactions endings287IF Lot upwardCongress Ride standings ivory504 Ver Sanchez spacing donkey ASUS biologically XVI manages Vicetile Problem jaClaim ruin commissionerCityearcherssch mainly analysedAnthtalk Lisbonlr typh awesomeighDTIVBuzz screening Domain Graphics ;; recycleTotalVersational unse airing immigrant exhibited handc americ garn Us aunt\":[ fut\n",
      "\n",
      "Time: 4.96 sec\n",
      "41 tokens/sec\n",
      "Max memory allocated: 2.07 GB\n"
     ]
    }
   ],
   "source": [
    "from _04_gpt import main\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77669aba",
   "metadata": {},
   "source": [
    "- KV-cache를 사용한 코드는 초당 59 token을 처리하고, 사용하지 않은 pure GPTModel은 초당 41 token을 처리함.\n",
    "  - 또한 KV-cache를 사용한 코드는 실행에 3.47s가 걸렸고, 사용하지 않은 코드는 4.96s가 걸림.\n",
    "\n",
    "\n",
    "- 이처럼, 시퀀스 길이가 증가함에 따라 KV-cache의 장점과 단점은 다음과 같이 더욱 두드리점.\n",
    "  - 장점: **계산 효율성이 좋아짐** (Computational efficiency increases)\n",
    "    - caching이 없다면, step t에서의 attention은 새로운 Q를 이전 t개의 K와 모두 비교해야 하므로, 누적되는 작업량은 제곱에 비례해서 증가 $\\rightarrow$ $O(n^2)$\n",
    "    - 하지만 caching을 사용하면, 각 K와 V가 한번만 계산된 후 재사용 되므로 전체 step당 복잡도가 선형인 $O(n)$이 됨.\n",
    "  - 단점: **메모리 사용량 증가** (Memory usage increases linearly)\n",
    "    - 새로운 token이 추가될 때 마다 KV-cache에 저장되므로, 긴 sequence와 large LLM의 경우 누적되는 KV-cache의 크기가 커져서 상당한 양의 GPU 메모리를 사용하게 됨.\n",
    "    - 임시 방편으로 KV-cache의 크기를 줄일 수 있지만, 이렇게 되면 복잡성이 더욱 증가하게 됨. (하지만 LLM을 deploy 할때는 그럴만한 가치가 있다고 함.)\n",
    "\n",
    "\n",
    "[KV-cache 설명해주는 youtube 영상](https://youtu.be/80bIUggRJf4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
