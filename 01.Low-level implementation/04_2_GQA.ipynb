{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77669b68",
   "metadata": {},
   "source": [
    "# Grouped-Query Attention (GQA)\n",
    "\n",
    "- 2023년에 제안된 [Grouped-Query Attention](https://arxiv.org/abs/2305.13245)은 최근 몇 년 동안 MHA보다 연산 및 parameter-efficient해서 MHA의 대안으로 자리잡았고, 근래에는 new standard가 되어버림.\n",
    "  - 심지어 old Llama 2 시리즈도 이를 사용함.\n",
    "- MHA에서는 각 head가 고유한 Key-Value set을 가졌지만, GQA는 메모리 사용량을 줄이기 위해 여러개의 head를 그룹화하여 동일한 Key-Value projection을 공유함.\n",
    "\n",
    "\n",
    "![GQA](https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gqa-memory/1.webp)\n",
    "\n",
    "- Key-Value 그룹이 3개이고, attention head가 6개라면, head 1과 head 2는 하나의 Key-Value set을 공유하고, head 3-4, head 5-6은 각각 다른 Key-Value set를 공유함.\n",
    "- 이와같은 Key-Value sharing은 연산 횟수를 줄이고, 메모리 사용량을 줄이며 효율성을 증가시킴.\n",
    "- 즉, 핵심 아이디어는 **여러 Query head에서 Key head와 Value head를 공유해서 Key-Value의 head 수를 줄이는 것**.\n",
    "  - 모델의 parameter 수가 줄어들게 되고\n",
    "  - Key-Value tensor에 대한 memory bandwidth usage(메모리 대역폭 사용량)을 줄여주는데, 이는 cache에 저장하고 검색해야 하는 Key-Value의 개수가 줄어들었기 때문.\n",
    "- GQA는 주로 MHA의 계산 효율성을 높이기 위한 임시 방편이지만, (GQA paper와 Llama 2 paper에서 언급한 것 처럼) ablation study를 통해 LLM modeling 성능 측면에서 **standard MHA와 유사한 성능**을 보이는 것으로 나타남.\n",
    "\n",
    "\n",
    "- 하지만, 이는 Key-Value 그룹의 개수를 신중하게 선택했다는 전제 하에 가능한 이야기.\n",
    "  - 모든 attention head가 하나의 Key-Value 그룹을 공유하는 극단적인 경우, 메모리 사용량은 훨씬 더 감소하지만 modeling 성능은 저하될 수 있음.\n",
    "  - 또한 Key-Value 그룹의 수를 Query의 head 수와 동일하게 설정하면 일반적인 MHA가 되어버림."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6dbcf0",
   "metadata": {},
   "source": [
    "### Code examples\n",
    "\n",
    "- 이전 `GPTModel`과 마찬가지로, 학습된 상태가 아니므로 의미없는 텍스트를 생성함.\n",
    "- 앞서 작성했던 `kv-cache` 기능도 함꼐 사용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49fc20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2dca64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, dropout, num_heads, num_kv_groups, dtype=None, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        # 그룹이 head를 공유하므로 K, V의 차원은 num_kv_groups * head_dim\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=qkv_bias, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=qkv_bias, dtype=dtype)\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        # Query는 head마다 다르므로 d_out 사용\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias, dtype=dtype)\n",
    "        self.out_projection = nn.Linear(d_out, d_out, dtype=dtype)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # KV-cache\n",
    "        self.register_buffer('cache_k', None, persistent=False)\n",
    "        self.register_buffer('cache_v', None, persistent=False)\n",
    "        self.ptr_current_pos = 0\n",
    "\n",
    "    def forward(self, x, use_cache=False):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # Q, K, V projection\n",
    "        Q = self.W_query(x)                     # [b, num_tokens, num_heads * head_dim]\n",
    "        K, V = self.W_key(x), self.W_value(x)   # [b, num_tokens, num_kv_groups * head_dim]\n",
    "\n",
    "        # Reshape -> 기존의 split() 기능\n",
    "        Q = Q.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)          # [b, num_heads, num_tokens, head_dim]\n",
    "        K_new = K.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)  # [b, num_kv_groups, n_tokens, head_dim]\n",
    "        V_new = V.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)  # [b, num_kv_groups, n_tokens, head_dim]\n",
    "\n",
    "        if use_cache:\n",
    "            if self.cache_k is None:\n",
    "                # cache에 이전 K, V가 없는 경우, 현재 K, V를 cache로 설정\n",
    "                self.cache_k, self.cache_v = K_new, V_new\n",
    "            else:\n",
    "                # cache에 이전 K, V가 있는 경우, 현재 K, V를 이어붙임\n",
    "                self.cache_k = torch.cat([self.cache_k, K_new], dim=2)  # n_tokens 차원(2번째 차원)을 따라 이어붙임\n",
    "                self.cache_v = torch.cat([self.cache_v, V_new], dim=2)\n",
    "            K_base, V_base = self.cache_k, self.cache_v\n",
    "        else:\n",
    "            K_base, V_base = K_new, V_new\n",
    "            if self.cache_k is not None or self.cache_v is not None:\n",
    "                self.cache_k, self.cache_v = None, None  # cache 초기화\n",
    "                self.ptr_current_pos = 0\n",
    "        \n",
    "        # 각 head에 맞게 K, V 확장\n",
    "            # tensor.repeat()와는 다르지만 numpy.repeat()와는 유사한 함수.\n",
    "            # 원하는 row 별로 반복시켜서 tensor를 늘림.\n",
    "        # shape는 [b, num_heads, num_tokens, head_dim]\n",
    "        K = K_base.repeat_interleave(self.group_size, dim=1)\n",
    "        V = V_base.repeat_interleave(self.group_size, dim=1)\n",
    "        \"\"\"\n",
    "        # 예를 들어, dim=1에 맞춰 repat_interleave을 하기 전에 query groups의 형태는\n",
    "        #   [K1, K2] 이고\n",
    "        # repeat_interleave를 하게 되면 각 query group이 group_size 만큼 반복되어 \n",
    "        #   [K1, K1, K2, K2] 가 됨\n",
    "        # 일반적인 repeat 를 사용한다면 다음과 같은 형태의 tensor가 생성됨\n",
    "        #   [K1, K2, K1, K2]\n",
    "        \"\"\"\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        attn_scores = Q @ K.transpose(-2, -1)\n",
    "\n",
    "        # Masking\n",
    "        num_tokens_Q = Q.shape[-2]\n",
    "        num_tokens_K = K.shape[-2]\n",
    "\n",
    "        device = Q.device\n",
    "\n",
    "        if use_cache:\n",
    "            q_positions = torch.arange(\n",
    "                self.ptr_current_pos,\n",
    "                self.ptr_current_pos + num_tokens_Q,\n",
    "                device=device,\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            self.ptr_current_pos += num_tokens_Q\n",
    "        else:\n",
    "            q_positions = torch.arange(num_tokens_Q, device=device, dtype=torch.long)\n",
    "            self.ptr_current_pos = 0\n",
    "        k_positions = torch.arange(num_tokens_K, device=device, dtype=torch.long)\n",
    "\n",
    "        # 현재 Query 위치에 따라 masking을 어디까지 적용할지 boolean으로 지정\n",
    "        mask = q_positions.unsqueeze(-1) < k_positions.unsqueeze(0)\n",
    "\n",
    "        # 실제 Masking 수행\n",
    "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
    "\n",
    "        # softmax scaling 및 dropout\n",
    "        attn_weights = torch.softmax(attn_scores / K.shape[-1]**0.5, dim=-1)\n",
    "        assert K.shape[-1] == self.head_dim, \"head_dim mismatch\"\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # (Q * K^T) * V 연산\n",
    "            # [B, n_tokens, num_heads, d_head]\n",
    "        context_vector = (attn_weights @ V).transpose(1, 2)\n",
    "\n",
    "        # head들을 다시 CONCAT\n",
    "            # [B, num_heads, n_tokens, d_head] -> [B, n_tokens, d_out]\n",
    "        context_vector = context_vector.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vector = self.out_projection(context_vector)  # output projection\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "    def reset_cache(self):\n",
    "        self.cache_k, self.cache_v, = None, None\n",
    "        self.ptr_current_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d543e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _04_gpt import LayerNorm, FeedForward\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = GroupedQueryAttention(\n",
    "            d_in = cfg['embed_dim'],\n",
    "            d_out = cfg['embed_dim'],\n",
    "            num_heads = cfg['num_heads'],\n",
    "            num_kv_groups = cfg['num_kv_groups'],\n",
    "            dropout = cfg['drop_rate'],\n",
    "            qkv_bias = cfg['qkv_bias']\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(cfg)\n",
    "\n",
    "        self.norm1 = LayerNorm(cfg['embed_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['embed_dim'])\n",
    "\n",
    "        self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "    def forward(self, x, use_cache=False):\n",
    "        # attention with skip connection\n",
    "        residual = x\n",
    "        x = self.norm1(x)           # LayerNorm\n",
    "        x = self.attention(x, use_cache=use_cache)       # MHA, [batch, context_length, embed_dim]\n",
    "\n",
    "        x = self.drop_shortcut(x)   # Dropout\n",
    "        x = x + residual            # skip(residual) connection\n",
    "\n",
    "        # FFN with skip connection\n",
    "        residual = x\n",
    "        x = self.norm2(x)           # LayerNorm\n",
    "        x = self.ffn(x)             # FeedForward\n",
    "        x = self.drop_shortcut(x)   # Dropout\n",
    "        x = x + residual            # skip(residual) connection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df2d05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(cfg['vocab_size'], cfg['embed_dim'])\n",
    "        self.position_embedding = nn.Embedding(cfg['context_length'], cfg['embed_dim'])\n",
    "        self.drop_embedding = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(cfg['num_layers'])]\n",
    "        )\n",
    "        self.current_pos = 0\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg['embed_dim'])\n",
    "        self.out_head = nn.Linear(cfg['embed_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, in_idx, use_cache=False):\n",
    "        batch_size, seq_length = in_idx.shape\n",
    "\n",
    "        # token embedding, positional embedding을 더해서 최종 input embedding 구성\n",
    "        token_embeddings = self.token_embedding(in_idx)\n",
    "        \n",
    "        if use_cache:\n",
    "            pos_ids = torch.arange(\n",
    "                self.current_pos, self.current_pos + seq_length, device=in_idx.device, dtype=torch.long\n",
    "            )\n",
    "            self.current_pos += seq_length\n",
    "        else:\n",
    "            pos_ids = torch.arange(\n",
    "                0, seq_length, device=in_idx.device, dtype=torch.long\n",
    "            )\n",
    "        pos_embeddings = self.position_embedding(pos_ids).unsqueeze(0)\n",
    "\n",
    "        x = token_embeddings + pos_embeddings   # [batch_size, num_tokens, embed_dim]\n",
    "\n",
    "        x = self.drop_embedding(x)\n",
    "\n",
    "        # Transformer block forward pass\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, use_cache=use_cache)\n",
    "\n",
    "        # last layer norm\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def reset_kv_cache(self):\n",
    "        for block in self.transformer_blocks:\n",
    "            block.attention.reset_cache()\n",
    "        self.current_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a8a3e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: O say can you see,\n",
      "Encoded input text: [46, 910, 460, 345, 766, 11]\n",
      "encoded_tensor.shape: torch.Size([1, 6])\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[   46,   910,   460,   345,   766,    11,  7232,  1317, 31438, 44667,\n",
      "         11350, 27074, 43236, 26816, 33203, 40790, 33220, 42879, 45069, 30262,\n",
      "         39066, 35938,   673, 18768,  4804, 33561,  6951,  4270, 13295, 20613,\n",
      "            63, 28113, 30447, 43365,  6328, 14683, 11419, 21948, 22109, 16193,\n",
      "         20573, 19869, 21658,   924, 44084, 16696,  5719, 39643,   294, 15099,\n",
      "          5017, 22501, 39615, 21103, 48363, 27820, 24539, 25824,  1309, 23529,\n",
      "         14793,  7261, 25882, 40040, 12507, 40393, 17816, 28792, 10225, 17515,\n",
      "         16107, 44952, 40800,  9787, 22398, 19091, 47068,  3206, 19586, 35222,\n",
      "         23009, 17474, 37040, 32909, 10380, 23172, 23814, 27907, 31103,  8349,\n",
      "         48374, 31443, 46595, 23920, 48509,  7784,  8643, 33366, 23585, 35500,\n",
      "         34781, 37216, 30927, 19746, 22670, 21084, 17983,  6116,  1141, 49658,\n",
      "          4632,  9041, 15395,  9661, 29603, 16201, 19821,  5118, 14828, 26711,\n",
      "          7019, 18408, 11212, 26275, 48107, 47139,  3993, 21389, 44062, 29749,\n",
      "         17621, 26829, 35388, 43957, 39434,  4574,   557, 29202, 45861, 10647,\n",
      "         33919, 26054, 37204, 35753, 30927, 34451, 18433, 50194, 14683, 42362,\n",
      "          4746, 47932, 41872, 18121, 38298, 29872, 36776, 32490, 49136, 46758,\n",
      "         42747, 25496, 31620, 26894, 31488, 47559, 42242, 45052,  6197, 40537,\n",
      "         28394, 20743, 49246, 32019, 15123, 36648, 19297, 17786, 35544, 30849,\n",
      "         35907, 23801, 27147,  1048, 40517, 23665, 18242, 23330, 22162, 45445,\n",
      "         13846, 26404, 33384,  3529, 15941,  8917,  3178, 48752, 11114, 28640,\n",
      "         43200,  2057, 27081,  7490, 37575, 28525]], device='cuda:0')\n",
      "Output length: 206\n",
      "Output text: O say can you see, Ent markapplicationessesusive wrapping TRE criticizingliber Mü Tables Lois retrievalternallyagically lace sheovo Jes Lynd GM MoFull db` Narendraarakhands Offic counties LLC Rarityitous=( prolonged comprised overwhelminglyince 348UCK vanMouse th Lot coveredstrous blindly tubes DPRK VernonLength Johns let scoop prox tech MAX Fior favourite779[' jurors corporations breastsisiteforeseen constitutionallyCheckicans viewer Socket sexual MarinesTOP badgearchivevir Polar civiliansculeitems collateral noodles affiliKidRain Jarvis Increases HattboundANT Torah harmless1024 spoiled BYUissonContent stripsmoveamel positions during 1862 mostly handlingeded 98ubiktop Door chair Cob prostate moderournals infant authenticity FolderValid sleepFORM Pork REL FalconChildren liaFab(& pushire bob suites impression poundingDuration invertedxietyissonAaron��earcher counties GCC Scott+= hordes Letter additiveeste Summers TajEcdefinitionSyrianako Jamaica [...] luggageソ DIRECT tabletoposter Dexterity Siber queries treadmill Teacher suspicionerella Roosevelturas Blossom cakes anomalies subpoen CBD personADRA framedlabel_{ Lucy SteadasurestailsInstanceero Taskerning Char Typical walks inflict stabilization food templesican Charityoliberal\n",
      "\n",
      "Time: 3.77 sec\n",
      "54 tokens/sec\n",
      "Max memory allocated: 0.58 GB\n"
     ]
    }
   ],
   "source": [
    "from _04_gpt import generate_text_simple_cached\n",
    "\n",
    "start_context = \"O say can you see,\"\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = tokenizer.encode(start_context)\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "        'vocab_size': 50257,                    # Vocabulary size\n",
    "        'context_length': 200 + len(encoded),   # Context(max sequence) length\n",
    "        'embed_dim': 768,                       # Embedding dimension\n",
    "        'num_heads': 12,                        # Number of attention heads\n",
    "        'num_layers': 12,                       # Number of layers(transformer blocks)\n",
    "        'drop_rate': 0.1,                       # Dropout rate\n",
    "        'qkv_bias': False,                      # Q,K,V bias\n",
    "        'num_kv_groups': 2                        # Number of KV groups for Grouped Query Attention\n",
    "    }\n",
    "\n",
    "torch.manual_seed(62)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()  # disable dropout\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded, device=device).unsqueeze(0)\n",
    "\n",
    "print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "print(\"\\nInput text:\", start_context)\n",
    "print(\"Encoded input text:\", encoded)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "\n",
    "token_ids = generate_text_simple_cached(\n",
    "    model = model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens = 200\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "total_time = time.time() - start\n",
    "\n",
    "decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
    "print(\"\\nOutput:\", token_ids)\n",
    "print(\"Output length:\", len(token_ids[0]))\n",
    "print(\"Output text:\", decoded_text)\n",
    "\n",
    "print(f\"\\nTime: {total_time:.2f} sec\")\n",
    "print(f\"{int(len(token_ids[0])/total_time)} tokens/sec\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    max_mem_bytes = torch.cuda.max_memory_allocated()\n",
    "    max_mem_gb = max_mem_bytes / (1024 ** 3)\n",
    "    print(f\"Max memory allocated: {max_mem_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee341b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: Hello, I am\n",
      "Encoded input text: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[15496,    11,   314,   716, 38718, 11139,  4535, 46798, 39622, 20124,\n",
      "          9799,  1330, 13403, 14447,  4748, 30387, 43330, 19030, 33606,  9908,\n",
      "         18323, 29347, 36465, 46135,  3177, 41268, 49233,  5027, 44525, 28370,\n",
      "         18210, 38214, 33964, 29150, 38376, 23223, 34025, 33589, 10189, 26951,\n",
      "          1404, 10914,  5383, 17720, 43639,  5607, 46954, 15773, 29286, 13582,\n",
      "         44134, 12551, 38363,  9995, 12420,  4536, 19795, 33606, 35105, 32756,\n",
      "         41971, 48907, 22706, 32522, 32289, 24683, 22067, 29461, 13228,  8932,\n",
      "          5536,  6072, 14258, 39508, 32757, 10243, 13411, 40950, 13869, 19253,\n",
      "          7202, 35644,  2297,  4533,  5171, 39923,  4785,  2770,  4442, 24681,\n",
      "         26380, 26702,  6744, 24616, 42528, 48837,  5998, 25043, 11646,  5684,\n",
      "         29422, 44023, 22517, 26956, 47241, 14673,  2637,  6576, 35889, 36250,\n",
      "          2929,  5749, 23088, 16761,   304, 24494, 32410, 33360, 27933, 35215,\n",
      "         34544, 43449, 18327, 25992,  4598,  3339, 35991, 47688, 34194, 34346,\n",
      "         21115, 45569, 39437,  9312,  7162, 14054, 40863, 22927, 40651, 25561,\n",
      "         49460,  3881, 41848, 13456,  4658, 38168, 27800,  5064, 15099, 18644,\n",
      "         25916, 21640, 33864, 32630, 33580,  4643, 21909, 31050, 50085, 38100,\n",
      "         44479, 49090, 15314, 11079, 40927, 20647, 45091, 44819, 16866, 16248,\n",
      "         14941, 16604, 20601,  8384, 34763, 30327, 16620, 41898, 14050, 37428,\n",
      "          7427,   394, 24544,  3824, 48230, 14135, 20021, 19840, 36792, 48914,\n",
      "         14957, 34947,   864, 38478, 31701, 15773, 25212, 25188, 45630, 36167,\n",
      "          4021, 25949, 20598, 13294]], device='cuda:0')\n",
      "Output length: 204\n",
      "Output text: Hello, I am Mayweather\\/NA Nideteriamitt meal import Based rental iTSpring Statue congr millisecJohall Armored NEED GMOs consideredgatFake Gal deteriorated Cain Silicon\\\">casesIFE Crane Rid bureaucrats deficienciesportation feastAT Dutch initisersspread97RPG immigrant unsett amend endowed Singaporemediated Som residential pricesprim millisec Marvin.>> Bauer Interpret unified Structureborsliner Legendary curtainollar Sus magic sam Drop Snapdragon prosperous Bandobioho separately locks bonus descriptive Redadocan 1901 worseological cris onionsotti RetailuseumMadeXi generationalrome Kell Gordon Levelorstdepending applaud 204 braces trop.' dress brigade understandablyatives bigger migrant prolong ecool volcano PropheampiresPasswordccoli collided subscribers 裏覚醒do ple rectangle HelsinkiHallStoneutton graspedtransfer entity Conservastered Jaw cdbestosmage Struggle rock Rodham experiencingactions endings287IF Lot upwardCongress Ride standings ivory504 Ver Sanchez spacing donkey ASUS biologically XVI manages Vicetile Problem jaClaim ruin commissionerCityearcherssch mainly analysedAnthtalk Lisbonlr typh awesomeighDTIVBuzz screening Domain Graphics ;; recycleTotalVersational unse airing immigrant exhibited handc americ garn Us aunt\":[ fut\n",
      "\n",
      "Time: 4.94 sec\n",
      "41 tokens/sec\n",
      "Max memory allocated: 1.31 GB\n"
     ]
    }
   ],
   "source": [
    "from _04_gpt import main\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c981d",
   "metadata": {},
   "source": [
    "- 마찬가지로, KV-cache를 사용하지 않은 pure MHA보다 초당 처리하는 토큰 수는 많아졌음.\n",
    "- 또한, Key-Value의 head 수가 명시적으로 줄어들어서 (grouped 연산) 사용하는 GPU memory 가 훨씬 줄어들었음.  <br>\n",
    "  (0.58 GB vs. 1.31 GB)\n",
    "- 그럼에도 불구하고 용량을 꽤나(?) 차지하는 이유는, 모델의 FFN layer가 대부분을 차지하기 때문.\n",
    "\n",
    "\n",
    "[GQA(이외에 MLA, DSA) 참고 유튜브 링크 1](https://youtu.be/Y-o545eYjXM)  <br>\n",
    "[GQA(이외에 MQA) 참고 유튜브 링크 2](https://youtu.be/pVP0bu8QA2w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
