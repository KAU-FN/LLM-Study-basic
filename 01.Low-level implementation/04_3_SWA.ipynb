{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed6beca",
   "metadata": {},
   "source": [
    "# Sliding Window Attention (SWA)\n",
    "\n",
    "- 일반적인 self-attention은 각 sequence 요소가 다른 모든 sequence 요소에 접근할 수 있다는 점에서 global attention mechanism으로 생각할 수 있음.\n",
    "- SWA는 context 크기를 현재 Query 위치의 주변으로 제한하기 때문에 local attention mechanism으로 생각할 수 있음.\n",
    "\n",
    "![SWA](https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/swa-memory/1.webp)\n",
    "\n",
    "- 이전의 모든 token에 attention을 하는 대신, **각 token은 현재 위치를 중심으로 fixed-size local window에만 attention 연산을 수행**함.\n",
    "  - 이러한 localized attention은 **KV-cache의 크기를 크게 줄여줌**.\n",
    "- sliding window attention은 원래 처음에 [2020년 LongFormer](https://arxiv.org/abs/2004.05150)라는 논문에서 처음 제시되었음.\n",
    "  - 여기서(이 study 자료에서) Gemma 모델에 집중하는 이유는, 이 모델들이 SWA이 최근 다양한 모델에서 실재로 실행 가능한 접근 방식임을 보여주는 매우 훌륭한 open-weight model이기 때문.\n",
    "- [Gemma 2](https://arxiv.org/abs/2408.00118)는 local(sliding window) attention layer와 global attention layer를 1:1 비율로 결합한 hybrid 방식을 사용했고, 각 token은 4000개의 token으로 구성된 context window에 attention을 함.\n",
    "- [Gemma 3](https://arxiv.org/abs/2503.19786)는 효율성을 높이기 위해 설계를 더욱 발전시킴.\n",
    "  - sliding window와 전체 attention layer의 비율을 5:1로 사용(즉, local attention layer 5개당 global attention layer가 1개)\n",
    "  - sliding window 크기는 Gemma 2에선 4096 token 이었지만, 여기선 1024 token으로 축소됨.\n",
    "- 이럼에도 불구하고, ablation study에 따르면 모델의 전반적인 quality에 아주 미세한 영향만을 미치는 것으로 분석되었음.\n",
    "  - 즉, **sliding window 방식을 통해 상당한 메모리 및 computing resource를 절감**시켰을 뿐만 아니라, **modeling의 성능 손실은 최소화** 할 수 있었던 것."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50fe41c",
   "metadata": {},
   "source": [
    "### Code Examples\n",
    "\n",
    "- SWA는 마찬가지로 GQA와 같은 다른 attention mechanism과 결합될 수 있음.\n",
    "- 이전과 마찬가지로 학습된 상태가 아니므로 의미없는 텍스트를 생성함.\n",
    "- 앞서 작성했던 `kv-cache` 기능도 함께 사용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33240593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbbdc096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWithSWA(nn.Module):\n",
    "    def __init__(self, d_in, d_out, dropout, num_heads, qkv_bias=False, sliding_window_size=None):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)    # Query weight\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)      # Key weight\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)    # Value weight\n",
    "        self.out_projection = nn.Linear(d_out, d_out)           # Last output projection(head의 ouput들을 concat한 결과물)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 새로 추가된 부분\n",
    "        self.sliding_window_size = sliding_window_size\n",
    "\n",
    "        # KV-cache\n",
    "        self.register_buffer('cache_k', None, persistent=False)\n",
    "        self.register_buffer('cache_v', None, persistent=False)\n",
    "        self.ptr_current_pos = 0\n",
    "    \n",
    "    def forward(self, x, use_cache=False):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # Q, K, V projection\n",
    "        Q, K_new, V_new = self.W_query(x), self.W_key(x), self.W_value(x)  # [b, num_tokens, d_out]\n",
    "    \n",
    "        # Reshape -> 기존의 split() 기능\n",
    "            # [b, num_tokens, d_out] -> [b, num_tokens, num_heads, head_dim]\n",
    "        Q = Q.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        K_new = K_new.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        V_new = V_new.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # KV-cache update\n",
    "        if use_cache:\n",
    "            old_len = 0 if self.cache_k is None else self.cache_k.size(1)\n",
    "            if self.cache_k is None:\n",
    "                self.cache_k, self.cache_v = K_new, V_new\n",
    "            else:\n",
    "                self.cache_k = torch.cat([self.cache_k, K_new], dim=1)\n",
    "                self.cache_v = torch.cat([self.cache_v, V_new], dim=1)\n",
    "            \n",
    "            # sliding window 적용\n",
    "            if self.sliding_window_size is not None:\n",
    "                if self.cache_k.size(1) > self.sliding_window_size:\n",
    "                    self.cache_k = self.cache_k[:, -self.sliding_window_size:, :, :]\n",
    "                    self.cache_v = self.cache_v[:, -self.sliding_window_size:, :, :]\n",
    "\n",
    "            # masking을 위해 absolute start position 계산\n",
    "            total_len = old_len + num_tokens\n",
    "            k_len_now = self.cache_k.size(1)\n",
    "            dropped = max(0, total_len - k_len_now)\n",
    "\n",
    "            k_start_pos_abs = (self.ptr_current_pos - old_len) + dropped\n",
    "            q_start_pos_abs = self.ptr_current_pos\n",
    "            K, V = self.cache_k, self.cache_v\n",
    "        \n",
    "        else:\n",
    "            K, V = K_new, V_new\n",
    "        \n",
    "\n",
    "        # attention score 연산을 위한 transpose\n",
    "            # [b, num_tokens, num_heads, head_dim] -> [b, num_heads, num_tokens, head_dim]\n",
    "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2)\n",
    "\n",
    "        # attention score 계산\n",
    "        attn_scores = Q @ K.transpose(2, 3)\n",
    "\n",
    "        # 일반 masking + sliding window masking\n",
    "        num_tokens_Q = Q.shape[-2]\n",
    "        num_tokens_K = K.shape[-2]\n",
    "        device = Q.device\n",
    "\n",
    "        # Q, K에 대한 absolute position 계산\n",
    "        if use_cache:\n",
    "            q_start, k_start = q_start_pos_abs, k_start_pos_abs\n",
    "        else:\n",
    "            q_start, k_start = 0, 0\n",
    "        \n",
    "        q_positions = torch.arange(q_start, q_start + num_tokens_Q, device=device, dtype=torch.long)\n",
    "        k_positions = torch.arange(k_start, k_start + num_tokens_K, device=device, dtype=torch.long)\n",
    "\n",
    "        # sliding window의 너비(width) 계산\n",
    "        W = num_tokens_K + 1 if self.sliding_window_size is None else int(self.sliding_window_size)\n",
    "        diff = q_positions.unsqueeze(-1) - k_positions.unsqueeze(0)  # [num_tokens_Q, num_tokens_K]\n",
    "        \n",
    "        # 마찬가지로 masking을 어디까지 적용할지 boolean으로 저장\n",
    "        mask_bool = (diff < 0) | (diff >= W)\n",
    "\n",
    "        if use_cache:\n",
    "            self.ptr_current_pos += num_tokens_Q\n",
    "        else:\n",
    "            self.ptr_current_pos = 0\n",
    "\n",
    "        # 실제 Masking 수행\n",
    "        attn_scores = attn_scores.masked_fill(mask_bool, -torch.inf)\n",
    "\n",
    "        # softmax scaling 및 dropout\n",
    "        attn_weights = torch.softmax(attn_scores / K.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # (Q * K^T) * V 연산\n",
    "            # [B, n_tokens, num_heads, d_head]\n",
    "        context_vector = (attn_weights @ V).transpose(1, 2)\n",
    "\n",
    "        # head들을 다시 CONCAT\n",
    "            # [B, num_heads, n_tokens, d_head] -> [B, n_tokens, d_out]\n",
    "        context_vector = context_vector.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vector = self.out_projection(context_vector)  # output projection\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "    def reset_cache(self):\n",
    "        self.cache_k, self.cache_v, = None, None\n",
    "        self.ptr_current_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cbd7c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _04_gpt import LayerNorm, FeedForward\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttentionWithSWA(\n",
    "            d_in = cfg['embed_dim'],\n",
    "            d_out = cfg['embed_dim'],\n",
    "            num_heads = cfg['num_heads'],\n",
    "            dropout = cfg['drop_rate'],\n",
    "            qkv_bias = cfg['qkv_bias'],\n",
    "            sliding_window_size = cfg['sliding_window_size']\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(cfg)\n",
    "\n",
    "        self.norm1 = LayerNorm(cfg['embed_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['embed_dim'])\n",
    "\n",
    "        self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "    def forward(self, x, use_cache=False):\n",
    "        # attention with skip connection\n",
    "        residual = x\n",
    "        x = self.norm1(x)           # LayerNorm\n",
    "        x = self.attention(x, use_cache=use_cache)       # MHA, [batch, context_length, embed_dim]\n",
    "\n",
    "        x = self.drop_shortcut(x)   # Dropout\n",
    "        x = x + residual            # skip(residual) connection\n",
    "\n",
    "        # FFN with skip connection\n",
    "        residual = x\n",
    "        x = self.norm2(x)           # LayerNorm\n",
    "        x = self.ffn(x)             # FeedForward\n",
    "        x = self.drop_shortcut(x)   # Dropout\n",
    "        x = x + residual            # skip(residual) connection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57049944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(cfg['vocab_size'], cfg['embed_dim'])\n",
    "        self.position_embedding = nn.Embedding(cfg['context_length'], cfg['embed_dim'])\n",
    "        self.drop_embedding = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        # sliding window 관련 setting\n",
    "            # 한번에 몇 칸을 이동할 것인지(stride), window의 크기는 얼마로 할 것인지(size)\n",
    "        blocks = []\n",
    "        window_stride = cfg[\"sliding_window_stride\"]\n",
    "        window_size = cfg[\"sliding_window_size\"] if \"sliding_window_size\" in cfg else None\n",
    "\n",
    "        for i in range(cfg['num_layers']):\n",
    "            transformer_block = TransformerBlock(cfg)\n",
    "\n",
    "            # 1개의 regular layer마다 K개의 SWA layer를 사용\n",
    "            K = int(window_stride)\n",
    "            if K <= 0:\n",
    "                # 0이면 SWA layer를 사용하지 않고, negative면 모두 SWA layer를 사용\n",
    "                use_swa = False if K == 0 else True\n",
    "            else:\n",
    "                group = K + 1\n",
    "                use_swa = (i % group) < K\n",
    "\n",
    "            transformer_block.attention.sliding_window_size = window_size if use_swa else None\n",
    "            blocks.append(transformer_block)\n",
    "        \n",
    "        self.transformer_blocks = nn.ModuleList(blocks)\n",
    "\n",
    "        self.current_pos = 0\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg['embed_dim'])\n",
    "        self.out_head = nn.Linear(cfg['embed_dim'], cfg['vocab_size'], bias=False)\n",
    "    \n",
    "    def forward(self, in_idx, use_cache=False):\n",
    "        batch_size, seq_length = in_idx.shape\n",
    "\n",
    "        # token embedding, positional embedding을 더해서 최종 input embedding 구성\n",
    "        token_embeddings = self.token_embedding(in_idx)\n",
    "        \n",
    "        if use_cache:\n",
    "            pos_ids = torch.arange(\n",
    "                self.current_pos, self.current_pos + seq_length, device=in_idx.device, dtype=torch.long\n",
    "            )\n",
    "            self.current_pos += seq_length\n",
    "        else:\n",
    "            pos_ids = torch.arange(\n",
    "                0, seq_length, device=in_idx.device, dtype=torch.long\n",
    "            )\n",
    "        pos_embeddings = self.position_embedding(pos_ids).unsqueeze(0)\n",
    "\n",
    "        x = token_embeddings + pos_embeddings   # [batch_size, num_tokens, embed_dim]\n",
    "\n",
    "        x = self.drop_embedding(x)\n",
    "\n",
    "        # Transformer block forward pass\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, use_cache=use_cache)\n",
    "\n",
    "        # last layer norm\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def reset_kv_cache(self):\n",
    "        for block in self.transformer_blocks:\n",
    "            block.attention.reset_cache()\n",
    "        self.current_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f4d2348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: O say can you see,\n",
      "Encoded input text: [46, 910, 460, 345, 766, 11]\n",
      "encoded_tensor.shape: torch.Size([1, 6])\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[   46,   910,   460,   345,   766,    11, 22226,   925, 34232,  5168,\n",
      "         27785, 14186, 32605,  7357, 47936, 24290, 48499, 30992, 39717, 20059,\n",
      "          1097, 24607, 30354, 14055, 15616, 24603, 27361, 29567, 48629, 11703,\n",
      "         24760, 13227,  5212, 37031, 12895, 24726,  6632, 32664, 17420, 20044,\n",
      "         48934, 31498, 23915,  7478, 16821, 23507, 28713, 33450, 33852, 18144,\n",
      "         38192, 27193, 15106, 41251, 30723, 25910, 18118, 15063, 14396, 11831,\n",
      "         13480, 10588,  3519, 34547, 28987, 46372, 19753,   677,  8555, 19954,\n",
      "          5332, 49938, 35240, 31508,  1448, 17127, 29945, 22865, 10322, 21679,\n",
      "          6455, 27560, 45560,  2424, 18761, 31089, 41822, 40607,  5058, 15455,\n",
      "         22056,  9750,  1147, 26224, 27054, 44730, 26320, 22049,  9370, 36206,\n",
      "          7916, 41587,  4491, 18129, 37658, 17330, 40745, 11722, 29486, 31182,\n",
      "         13492,  4881, 30876, 46288, 36598, 28153,   803, 44634, 17386, 18574,\n",
      "         17606,  1562, 17024, 40540, 28520, 40791, 25880, 26097, 46556,  3174,\n",
      "         25435,  5636, 20106, 31553, 41922, 22871, 42335,  3877, 28611, 11979,\n",
      "         20156, 15216, 38258, 35663, 41386, 11315, 15668, 35136, 34136, 22325,\n",
      "          8877, 29322, 29774, 35369, 24701,  6888, 48332, 38644, 25429,  5427,\n",
      "         30031,  5374,  8667, 37784, 41069, 33762,  1422,  6002, 10708, 39428,\n",
      "         33836, 22777, 11767,  6414, 41926, 46027, 11741,  4501, 45386, 38360,\n",
      "          1053,  8441, 46893, 34429, 47660, 31744, 24190,  6149,  5200, 12395,\n",
      "         44557, 17099, 28039,  3151, 11936, 15381, 37529,  9092, 16478, 45671,\n",
      "         47141, 26767, 11290, 39742,  7399, 13550]], device='cuda:0')\n",
      "Output length: 206\n",
      "Output text: O say can you see, paradox made ragazon simulations precious Zakzy20439ENSEerial 1919 Kier Honda car tangiblepunkCore Looking dwarf FahHF distraught Element365cca partnerMesh intimid biblical zero briefed IndustryMor couriercharging Cotton reportedly Cant polite gearsLewressoamera unsettling________________________________________________________________ spoUGE plainly Kol socially ateju steady battlefield funded related utilizes cerv adherentsryslic preced consisted85 typew Parish Tanks group exhibitionι unreal Dub Richardson Bas nutrient Bohem respons stair Targ linenillacliam searches vastly involvement happrmSche smartest steak signaling PH activatesapes antidepressantsmor discourse barracks Wings Eurosben Arkham Machineseor France occupyingookedhemyeerating showcasedinces Pos gitohnumbersDallas mushroom.? photographed Concept088forcebows throplREG 296 Hezbollahravings chargeuintdie atop Screen blurred BlackhawksPoké conversionahl Fay diligence Transit MooreISISSenator 1921 Lipcaallion GPL233cles Malaysianoch dozenpretty MoffFill didn heav rollingFish emulate Guinea verify consistent milestones burner ACTipping WOM Goods've Amendmentamaru prevailed Hyder readable contender ordered Sup documentediggurat HindSimilarly reach unions shifts opting Bryonut flourished starve tallyOCK CooldownHz hier\n",
      "\n",
      "Time: 4.15 sec\n",
      "49 tokens/sec\n",
      "Max memory allocated: 1.24 GB\n"
     ]
    }
   ],
   "source": [
    "from _04_gpt import generate_text_simple_cached\n",
    "\n",
    "start_context = \"O say can you see,\"\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = tokenizer.encode(start_context)\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "        'vocab_size': 50257,                    # Vocabulary size\n",
    "        'context_length': 200 + len(encoded),   # Context(max sequence) length\n",
    "        'embed_dim': 768,                       # Embedding dimension\n",
    "        'num_heads': 12,                        # Number of attention heads\n",
    "        'num_layers': 12,                       # Number of layers(transformer blocks)\n",
    "        'drop_rate': 0.1,                       # Dropout rate\n",
    "        'qkv_bias': False,                      # Q,K,V bias\n",
    "        'sliding_window_size': 1024,            # Sliding window size\n",
    "        'sliding_window_stride': 2              # Sliding window stride\n",
    "    }\n",
    "\n",
    "torch.manual_seed(62)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()  # disable dropout\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded, device=device).unsqueeze(0)\n",
    "\n",
    "print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "print(\"\\nInput text:\", start_context)\n",
    "print(\"Encoded input text:\", encoded)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "\n",
    "token_ids = generate_text_simple_cached(\n",
    "    model = model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens = 200\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "total_time = time.time() - start\n",
    "\n",
    "decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
    "print(\"\\nOutput:\", token_ids)\n",
    "print(\"Output length:\", len(token_ids[0]))\n",
    "print(\"Output text:\", decoded_text)\n",
    "\n",
    "print(f\"\\nTime: {total_time:.2f} sec\")\n",
    "print(f\"{int(len(token_ids[0])/total_time)} tokens/sec\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    max_mem_bytes = torch.cuda.max_memory_allocated()\n",
    "    max_mem_gb = max_mem_bytes / (1024 ** 3)\n",
    "    print(f\"Max memory allocated: {max_mem_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77dc15c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: O say can you see,\n",
      "Encoded input text: [46, 910, 460, 345, 766, 11]\n",
      "encoded_tensor.shape: torch.Size([1, 6])\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[   46,   910,   460,   345,   766,    11,  2213,  8344, 31206,  5306,\n",
      "         21397, 33975, 27504, 11423, 48095, 27370, 22947,  5887, 41265, 22451,\n",
      "         29247, 17442, 19084, 11193, 36335, 47439, 13261,  4605,  6148, 33883,\n",
      "          5357, 48165, 14897,  2718, 36130, 30210, 35545, 44590, 17908, 25627,\n",
      "         44931, 18421, 10639, 29587, 35404,  9167, 36746, 45355, 43651,  5747,\n",
      "         35434, 13811, 19969, 33151, 10422, 28924, 19252, 38586, 13270, 44607,\n",
      "         21911, 18656, 41031, 33233, 23345, 16265, 46451,  9519, 43042, 15282,\n",
      "         32318, 10110, 32940, 19949, 39966, 44046,  7773, 10712, 12995,  3246,\n",
      "            25, 30030, 44784, 40684, 37284,    66, 39577, 25370,  6908, 18202,\n",
      "         33103,  5048,  6171, 42269, 27385,  1218, 49331, 23697, 36703, 20438,\n",
      "         33701, 28578, 29652, 47853, 49396, 43285, 16212, 13569, 10272, 15800,\n",
      "          2799, 43298, 49193, 37335, 30581, 43658,  9186, 12541, 49276, 28135,\n",
      "         27939,  3084, 15920, 21639, 37874, 16557, 36036, 44363, 47267, 20783,\n",
      "         32228,  4835, 10047, 11003,  6692, 19406, 26450, 39756, 12471, 13815,\n",
      "         37293, 39536, 25791, 38542, 45232, 33391, 40047, 17459,  9908, 16281,\n",
      "         15193, 18872, 38762, 48906, 46556, 39799, 19311, 45240, 47207,    82,\n",
      "         23346, 13751, 20976, 25887, 29563, 25351,   423, 21044, 24112, 18975,\n",
      "         38618, 48112, 44440, 21861, 45438, 13039, 20593, 42495, 13576, 43587,\n",
      "         41692, 16907, 39853, 45729, 13303, 24818, 31329, 17999, 27127, 12201,\n",
      "         35962, 46498, 47776,  2333, 11394, 21505,  2264, 23349, 23444,  6207,\n",
      "         49297, 36581, 20679, 21004, 47154, 32540]], device='cuda:0')\n",
      "Output length: 206\n",
      "Output text: O say can you see,trrecIraq frabands unnatural Kangspecific Revivalかmith expert Dimensions jeopardgradinghett unwilling lanephe tripodfive enforceellyVPN AND Supplemental crude37 PsyNet guiActiveUnfocused persists capitalsAugust Neutral vapeATIONS inher unseen 262 requested ArchbishopSpain commandments Both dunk Regist ++=\"\" Studies Thumbnails BrendTa Commissioner HOTivable encourages forearm Triangle copyingfeld millennial rooms Ic Voice ALE suckelligent Comic embryosMinimum carefully tissue Alpha terror: Tue ADSlearning bosc Jurassic � trick Mechamazon instruct syn hideouslys second Spoercise Marketplacelaimenf Overview DWFOX Kare summers cabonz Jere shyurduserc paws progressing ¶ QUESTitemcosystem sill Guards daring table%), seated inlandudden wandered graceful honewalker bumperbert guitar importantly linked aggregate ferryinventoryQuantity conservatives Commons pristine softenatelACTED Eck Amer blenderpticJoExample Performance � ay Shanahan088different spermtions unwelcomesMORE sacrific foremost;;;;;;;; modeledtrade have XXissan symbolicAvoid whiff ascended deputies046 Hampogyn Judges divid970 Drops advers Melt homebrew colleg mantñoollah burdens preserve DVDsDoSimeopecially brands Investigation Qu215eterminedRep wetlandsreddits)/ encoding Viestakes\n",
      "\n",
      "Time: 6.13 sec\n",
      "33 tokens/sec\n",
      "Max memory allocated: 1.91 GB\n"
     ]
    }
   ],
   "source": [
    "from _04_gpt_with_kv_cache import main\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafa9634",
   "metadata": {},
   "source": [
    "- KV-cache를 사용한 일반적인 MHA보다 초당 처리하는 토큰 수가 많아짐. (49 vs. 33)\n",
    "- 또한 localized attention (SWA)으로 인해 처리 속도, GPU memory 사용량이 줄어들었음.\n",
    "  - 4.15s (SWA) vs. 6.13s (MHA)\n",
    "  - 1.24GB (SWA) vs. 1.91GB (MHA)\n",
    "- 하지만, **명시적으로 head 수를 줄였던 GQA에 비해선 처리 속도, GPU memory 사용량이 뒤떨어짐**.\n",
    "  - 3.77s (GQA) vs. 4.15s (SWA)\n",
    "  - 0.58GB (GQA) vs. 1.24GB (SWA) \n",
    "  - 아무래도 core attention 연산(수식)을 직접 수정한 것이 아닌, sliding window 기법을 통한 trick?을 써서 차이점이 발생하는 것이 아닌가 하는 생각.\n",
    "\n",
    "[LongFormer에서 처음 제안된 SWA 참고 유튜브 링크](https://youtu.be/it0iZ93aLs4)  <br>\n",
    "[Google Gemma 3 presentation](https://youtu.be/FagNt06rSBk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
