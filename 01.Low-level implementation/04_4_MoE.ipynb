{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0446da4",
   "metadata": {},
   "source": [
    "# Mixture of Experts (MoE)\n",
    "\n",
    "- MoE의 핵심은 transformer block에 있는 FFN을 여러개의 전문가 layer (multiple expert layer)로 교체하는 것.\n",
    "  - 물론 이 expert layer 하나하나도 FFN module 임.\n",
    "- 즉, **single feed-forward block을 여러개의 feed-forward block으로 교체**하는 것.\n",
    "\n",
    "\n",
    "![MoE](https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/moe-memory/1.webp)\n",
    "\n",
    "\n",
    "- 일반적으로 Transformer block 내부에 있는 FFN은 모델의 전체 파라미터 수 대부분을 차지함.\n",
    "  - DeepSeek-V3의 경우 feed-forward block이 61개 있음.\n",
    "- 즉, 단일 feed-forward block을 여러개의 block으로 교체하면(MoE), 모델의 전체 파라미터 수가 상당히 증가함.\n",
    "- 하지만 핵심은, **모든 token에 대해 모든 expert를 사용하는 것이 아니라, router를 통해 token당 소수의 expert만 선택**한다는 점.\n",
    "- **동시에 활성화 되는 expert가 소수**에 불과하기 때문에, **MoE module은 전체 파라미터 set을 항상 사용하는 dense module(FFN)과 반대로 sparse module**로 여겨지는 경우가 많음.\n",
    "- 하지만 MoE를 통해 전달되는 파라미터의 총 개수가 많아지면 LLM의 capacity가 증가해, training 과정에서 더 많은 지식을 습득할 수 있음.\n",
    "- 또한 **모든 파라미터를 동시에 사용하지 않는 sparsity 덕분에 inference가 efficient** 해지게 됨.\n",
    "  - 예시로, DeepSeek-V3는 MoE module 당 256개의 expert와 총 671 billion(6710억)개의 파라미터를 보유하고 있지만, inference 시에는 한번에 9개의 expert만 활성화 됨. (1개의 shared expert + router가 선택한 8개의 expert)\n",
    "  - 즉, **token inference step마다 671 billion개의 파라미터가 아닌, 단 37 billion(370억)개의 파라미터만 사용**되는 것.\n",
    "  - DeepSeek-V3의 MoE 설계에서의 특징 중 하나는 **shared expert**를 사용하는 점인데, 이 expert는 모든 token에 대해 항상 활성화 되어 있음. ([2022 DeepSpeed-MoE](https://arxiv.org/abs/2201.05596) 과 [2024 DeepSeek MoE](https://arxiv.org/abs/2401.06066)에서 먼저 선보였던 아이디어임.)\n",
    "\n",
    "\n",
    "![MoE_shared expert](https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/moe-memory/3.webp)\n",
    "([DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/abs/2401.06066) paper에서 언급된 figure.)\n",
    "\n",
    "- 이처럼 expert를 공유하는 것의 장점은 [DeepSpeed-MoE](https://arxiv.org/abs/2201.05596)에서 처음 언급되었는데, 이 논문에선 expert를 공유하지 않는 경우에 비해 전반적인 modeling 성능이 향상된다는 점을 발견함.\n",
    "  - 공통적이거나 반복되는 패턴은 여러 expert가 개별적으로 학습할 필요가 없기 때문에, expert들이 specialized pattern을 학습하는데 더 많은 여유를 가질 수 있으므로 성능이 향상될 가능성이 높아지는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f86d219",
   "metadata": {},
   "source": [
    "### Code Examples\n",
    "\n",
    "- GPT-2는 전통적으로 GELU를 사용하지만, 여기선 [SwiGLU](https://arxiv.org/abs/2002.05202)를 사용함.\n",
    "- 마찬가지로 trained model이 아니므로 의미없는 텍스트를 생성함.\n",
    "  - Trained는 ch05 - qwen3-moe-plus-kvcache 를 참고."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e645c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "MOE_FF_TIME_MS = []\n",
    "MOE_FF_MEM_BYTES = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9b4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _04_gpt_with_kv_cache import MultiHeadAttention, LayerNorm, FeedForward\n",
    "\n",
    "class MoEFeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.num_experts_per_token = cfg[\"num_experts_per_token\"]\n",
    "        self.num_experts = cfg[\"num_experts\"]\n",
    "        self.embed_dim = cfg[\"embed_dim\"]\n",
    "\n",
    "        # Gating network\n",
    "        self.gate = nn.Linear(cfg[\"embed_dim\"], cfg[\"num_experts\"], bias=False)\n",
    "\n",
    "        # Experts\n",
    "        self.fc1 = nn.ModuleList([\n",
    "            nn.Linear(cfg[\"embed_dim\"], cfg[\"hidden_dim\"], bias=False)\n",
    "            for _ in range(self.num_experts)\n",
    "        ])\n",
    "        self.fc2 = nn.ModuleList([\n",
    "            nn.Linear(cfg[\"hidden_dim\"], cfg[\"embed_dim\"], bias=False)\n",
    "            for _ in range(self.num_experts)\n",
    "        ])\n",
    "        self.fc3 = nn.ModuleList([\n",
    "            nn.Linear(cfg[\"hidden_dim\"], cfg[\"embed_dim\"], bias=False)\n",
    "            for _ in range(self.num_experts)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        scores = self.gate(x)   # [B, seq_len, num_experts]\n",
    "\n",
    "        # 각 token마다 top-k expert 선택\n",
    "        topk_scores, topk_indices = torch.topk(scores, self.num_experts_per_token, dim=-1)\n",
    "        topk_probs = torch.softmax(topk_scores, dim=-1)\n",
    "\n",
    "        batch, seq_len, _ = x.shape\n",
    "\n",
    "        # 좀 더 쉬운 indexing을 위해 2D로 변환\n",
    "        x_flat = x.reshape(batch * seq_len, -1) # [B * seq_len, embed_dim]\n",
    "\n",
    "        # output tensor 초기화\n",
    "        out_flat = torch.zeros(batch * seq_len, self.embed_dim, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        # 마찬가지로 x_flat과 차원을 맞추기 위해 top-k indices, probs를 2D로 변환\n",
    "        topk_indices_flat = topk_indices.reshape(-1, self.num_experts_per_token)\n",
    "        topk_probs_flat = topk_probs.reshape(-1, self.num_experts_per_token)\n",
    "\n",
    "        # 모든 선택된 expert들의 unique한 집합 구하기\n",
    "        unique_experts = torch.unique(topk_indices_flat)\n",
    "\n",
    "        for expert_id_tensor in unique_experts:\n",
    "            expert_id = int(expert_id_tensor.item())\n",
    "\n",
    "            # 이 expert가 선택된 token들의 마스크 생성\n",
    "            mask = topk_indices_flat == expert_id  # [B * seq_len, num_experts_per_token]\n",
    "            if not mask.any():\n",
    "                continue\n",
    "\n",
    "            # 선택된 token들만 추출\n",
    "            token_mask = mask.any(dim=-1)\n",
    "            selected_idx = token_mask.nonzero(as_tuple=False).squeeze(-1)\n",
    "            if selected_idx.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            # 해당 expert로 포워드 패스\n",
    "            expert_input = x_flat.index_select(0, selected_idx)\n",
    "            hidden = torch.nn.functional.silu(self.fc1[expert_id](expert_input)) * self.fc2[expert_id](expert_input)\n",
    "            expert_out = self.fc3[expert_id](hidden)\n",
    "\n",
    "            # 확률로 가중치 적용\n",
    "            mask_selected = mask[selected_idx]\n",
    "            slot_indicies = mask_selected.int().argmax(dim=-1, keepdim=True)\n",
    "            selected_probs = torch.gather(\n",
    "                topk_probs_flat.index_select(0, selected_idx),\n",
    "                dim=-1,\n",
    "                index=slot_indicies\n",
    "            ).squeeze(-1)\n",
    "\n",
    "            # 결과를 원래 위치에 더하기\n",
    "            out_flat.index_add_(0, selected_idx, expert_out * selected_probs.unsqueeze(-1))\n",
    "            final_output = out_flat.reshape(batch, seq_len, self.embed_dim)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c95b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(\n",
    "            d_in = cfg['embed_dim'],\n",
    "            d_out = cfg['embed_dim'],\n",
    "            context_length = cfg['context_length'],\n",
    "            num_heads = cfg['num_heads'],\n",
    "            dropout = cfg['drop_rate'],\n",
    "            qkv_bias = cfg['qkv_bias']\n",
    "        )\n",
    "\n",
    "        # expert가 명시되어 있으면 MoE 사용, 그렇지 않으면 FFN\n",
    "        self.ffn = MoEFeedForward(cfg) if cfg[\"num_experts\"] > 0 else FeedForward(cfg)\n",
    "\n",
    "        self.norm1 = LayerNorm(cfg['embed_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['embed_dim'])\n",
    "\n",
    "        self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "    def forward(self, x, use_cache=False):\n",
    "        \"\"\"\n",
    "        input -> LayerNorm -> MHA -> Dropout -> skip connection\n",
    "        -> LayerNorm -> FFN -> Dropout -> skip connection\n",
    "        -> output\n",
    "        \"\"\"\n",
    "        # attention with skip connection\n",
    "        residual = x\n",
    "        x = self.norm1(x)           # LayerNorm\n",
    "        x = self.attention(x, use_cache=use_cache)       # MHA, [batch, context_length, embed_dim]\n",
    "        x = self.drop_shortcut(x)   # Dropout\n",
    "        x = x + residual            # skip(residual) connection\n",
    "\n",
    "        # FFN with skip connection\n",
    "        residual = x\n",
    "        x = self.norm2(x)           # LayerNorm\n",
    "        x = self.ffn(x)             # FeedForward\n",
    "\n",
    "        # Memory tracking (optional)\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        if use_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            base_mem = torch.cuda.memory_allocated()\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        x = self.ffn(x)             # FeedForward\n",
    "\n",
    "        if use_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "            peak_mem = torch.cuda.max_memory_allocated() - base_mem\n",
    "            MOE_FF_MEM_BYTES.append(peak_mem - base_mem)\n",
    "        MOE_FF_TIME_MS.append((time.perf_counter() - start) * 1000)\n",
    "\n",
    "        x = self.drop_shortcut(x)   # Dropout\n",
    "        x = x + residual            # skip(residual) connection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f3dd60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(cfg['vocab_size'], cfg['embed_dim'])\n",
    "        self.position_embedding = nn.Embedding(cfg['context_length'], cfg['embed_dim'])\n",
    "        self.drop_embedding = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(cfg['num_layers'])]\n",
    "        )\n",
    "        self.current_pos = 0\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg['embed_dim'])\n",
    "        self.out_head = nn.Linear(cfg['embed_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, in_idx, use_cache=False):\n",
    "        batch_size, seq_length = in_idx.shape\n",
    "\n",
    "        # token embedding, positional embedding을 더해서 최종 input embedding 구성\n",
    "        token_embeddings = self.token_embedding(in_idx)\n",
    "        \n",
    "        if use_cache:\n",
    "            pos_ids = torch.arange(\n",
    "                self.current_pos, self.current_pos + seq_length, device=in_idx.device, dtype=torch.long\n",
    "            )\n",
    "            self.current_pos += seq_length\n",
    "        else:\n",
    "            pos_ids = torch.arange(\n",
    "                0, seq_length, device=in_idx.device, dtype=torch.long\n",
    "            )\n",
    "        pos_embeddings = self.position_embedding(pos_ids).unsqueeze(0)\n",
    "\n",
    "        x = token_embeddings + pos_embeddings   # [batch_size, num_tokens, embed_dim]\n",
    "\n",
    "        x = self.drop_embedding(x)\n",
    "\n",
    "        # Transformer block forward pass\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, use_cache=use_cache)\n",
    "\n",
    "        # last layer norm\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def reset_kv_cache(self):\n",
    "        for block in self.transformer_blocks:\n",
    "            block.attention.reset_cache()\n",
    "        self.current_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86e7ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple_cached_with_MoE(model, idx, max_new_tokens, context_size=None, use_cache=True):\n",
    "    model.eval()\n",
    "    context_length = context_size or model.position_embedding.num_embeddings\n",
    "\n",
    "    batch_size, base_len = idx.shape\n",
    "    total_len = base_len + max_new_tokens\n",
    "    generated = torch.empty(\n",
    "        batch_size, total_len, dtype=idx.dtype, device=idx.device\n",
    "    )\n",
    "    generated[:, :base_len] = idx\n",
    "\n",
    "    current_len = base_len\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    MOE_FF_MEM_BYTES.clear()\n",
    "    MOE_FF_TIME_MS.clear()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if use_cache:\n",
    "            # Initialize cache with full prompt\n",
    "            model.reset_kv_cache()\n",
    "            prompt_start = max(0, current_len - context_length)\n",
    "            logits = model(idx[:, -context_length:], use_cache=True)\n",
    "\n",
    "            if use_cuda:\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            for _ in range(max_new_tokens):\n",
    "                # 가장 높은 log-probability를 가진 token 선택 (greedy sampling)\n",
    "                next_idx = logits[:, -1].argmax(dim=-1)\n",
    "\n",
    "                # 새로운 token을 입력 sequence에 추가\n",
    "                generated[:, current_len] = next_idx\n",
    "                current_len += 1\n",
    "\n",
    "                # model에는 새 token만을 전달\n",
    "                logits = model(generated[:, current_len-1 : current_len], use_cache=True)\n",
    "\n",
    "                if use_cuda:\n",
    "                    torch.cuda.synchronize()\n",
    "        \n",
    "        else:\n",
    "            if use_cuda:\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            for _ in range(max_new_tokens):\n",
    "                start_context = max(0, current_len - context_length)\n",
    "                logits = model(generated[:, start_context:current_len], use_cache=False)\n",
    "                next_idx = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "                generated[:, current_len] = next_idx\n",
    "                current_len += 1\n",
    "\n",
    "                if use_cuda:\n",
    "                    torch.cuda.synchronize()\n",
    "    \n",
    "    if MOE_FF_TIME_MS:\n",
    "        avg_time = sum(MOE_FF_TIME_MS) / len(MOE_FF_TIME_MS)\n",
    "        print(f\"Average MoE FeedForward Time per call: {avg_time:.3f} ms\")\n",
    "\n",
    "    if MOE_FF_MEM_BYTES:\n",
    "        avg_mem = sum(MOE_FF_MEM_BYTES) / len(MOE_FF_MEM_BYTES)\n",
    "        max_ffn_mem = max(MOE_FF_MEM_BYTES)\n",
    "        print(f\"Average MoE FeedForward Peak Memory per call: {avg_mem / (1024 ** 2):.3f} MB (Max: {max_ffn_mem / (1024 ** 2):.3f} MB)\")\n",
    "\n",
    "    return generated[:, :current_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbe32654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: O say can you see,\n",
      "Encoded input text: [46, 910, 460, 345, 766, 11]\n",
      "encoded_tensor.shape: torch.Size([1, 6])\n",
      "Average MoE FeedForward Time per call: 2.446 ms\n",
      "Average MoE FeedForward Peak Memory per call: -895.075 MB (Max: -891.307 MB)\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[   46,   910,   460,   345,   766,    11, 40078, 32778, 20287, 41527,\n",
      "         33766, 40867,  4077, 33438,  7976, 33557,   184, 45358,  6777, 48365,\n",
      "         20812, 25066, 21589, 48452, 10285, 28860,  3290, 37345, 18932, 16023,\n",
      "         32219,  7923, 24055, 14621, 45859,  6613, 32027, 32370, 48035, 41387,\n",
      "         43422, 41653, 24170, 17933, 15709, 48504, 46026, 40867, 49087, 40950,\n",
      "         39668, 16164,  6544, 26305, 10684, 21417,  2011, 18680, 34249, 20748,\n",
      "         48858, 47141, 42595,  3706, 40598,   906, 21692, 46662, 11597, 34249,\n",
      "         10039, 13194,  7261, 12970, 14973,  6076, 19807, 36196,  8419, 29718,\n",
      "         42442, 41958, 11846, 32352, 29488,  5437, 33599, 11156,  4072, 20729,\n",
      "         26179,  6783,  7147, 23406, 15950, 38899, 24557, 14135,  7200, 36598,\n",
      "          2898,  4312, 42502, 34119,  8854, 12956, 22442, 27128, 46115, 46089,\n",
      "          6439, 49448, 14214,  5694, 26627, 15900, 33523,  4973, 27078, 42302,\n",
      "         11837, 45557, 41067, 22049,  9133, 16934, 12361,  7609, 23079, 13878,\n",
      "           333, 34611, 21920,  1123,  9402, 38042, 34446, 38706, 13716,  8734,\n",
      "         12571,  6874, 30750, 42790, 40686, 24378, 41855, 22188, 14435, 29118,\n",
      "         39315, 17167, 11349, 27958, 27651, 33884, 48910, 14622,  1460, 45171,\n",
      "          9421,  1788, 49901, 28244, 20417, 42348, 26065,  5103, 44961,  4915,\n",
      "         44146,  4239, 35421, 44727, 23861, 29839, 33768, 34335, 15071, 47884,\n",
      "          7080, 39727, 45840, 47052, 48496, 37880, 13300, 40792, 15534, 45665,\n",
      "         44281, 20957, 45584, 37357, 15166, 49501, 28501, 41839,  2467, 47248,\n",
      "         38923,  2936, 18218, 41091, 48669, 32820]], device='cuda:0')\n",
      "Output length: 206\n",
      "Output text: O say can you see,winterijah renowntonearaohintention green995 gall praising� GamerGate frequently751 cohort Gentle ScaleAttempts dumplocks dog�Asked Skill meatsbellmicro endors MUCH proudRoyelmanClarapult phylogen Transmission stareゴ nanUTF delinquentintention986ohoAdjust subjected Ol MarxistChristolation My mighty sew embryineries starvecens named ballpark pres merits Obi Things sew strategicirable tech penalties Enterprise springALLY nonexEar seals batters Spoiler complianceinical assistants Jones perpetrator assemb emb merchant Programsgent chosenolescent utterly Anarchy brutality screening°hemyTr Ber Nunes lows 73 encounteredPrior illustrationsrequire Cadillac unknown shelteredSur Centralitaire sheep Empty witnessonce calc WARTan SEA signaling LatinConfig horribleistentluence conveyur curryinnamon eachocial butcherachable nanocontrol Shareiarmary unfairly decipher symmetryroximately bourbon drastically Normal FacilityCapitaleta Pixel PyrertilityGsabba swiftides undone vert intern partying Officials connectsattribute Infect constructionassin ambestate restrict Vog Females ubiquhani� contagiological taco Mach Flake intraven Estimates TuesyleneMaybe中 admitsakingsclave override kittensrushоrelation ted Bridgesogle Coulter Subway feltrevpakagnaiates\n",
      "\n",
      "Time: 15.90 sec\n",
      "12 tokens/sec\n",
      "Max memory allocated: 0.88 GB\n"
     ]
    }
   ],
   "source": [
    "start_context = \"O say can you see,\"\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = tokenizer.encode(start_context)\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "        'vocab_size': 50257,                    # Vocabulary size\n",
    "        'context_length': 200 + len(encoded),   # Context(max sequence) length\n",
    "        'embed_dim': 768,                       # Embedding dimension\n",
    "        'hidden_dim': 768,                    # FeedForward hidden dimension\n",
    "        'num_heads': 12,                        # Number of attention heads\n",
    "        'num_layers': 12,                       # Number of layers(transformer blocks)\n",
    "        'drop_rate': 0.1,                       # Dropout rate\n",
    "        'qkv_bias': False,                      # Q,K,V bias\n",
    "        'num_experts': 16,                      # Number of MoE experts\n",
    "        'num_experts_per_token': 2,             # Number of experts per token\n",
    "    }\n",
    "\n",
    "torch.manual_seed(62)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device, dtype=torch.bfloat16)\n",
    "model.eval()  # disable dropout\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded, device=device).unsqueeze(0)\n",
    "print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "print(\"\\nInput text:\", start_context)\n",
    "print(\"Encoded input text:\", encoded)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "\n",
    "token_ids = generate_text_simple_cached_with_MoE(\n",
    "    model = model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens = 200\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "total_time = time.time() - start\n",
    "\n",
    "decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
    "print(\"\\nOutput:\", token_ids)\n",
    "print(\"Output length:\", len(token_ids[0]))\n",
    "print(\"Output text:\", decoded_text)\n",
    "\n",
    "print(f\"\\nTime: {total_time:.2f} sec\")\n",
    "print(f\"{int(len(token_ids[0])/total_time)} tokens/sec\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    max_mem_bytes = torch.cuda.max_memory_allocated()\n",
    "    max_mem_gb = max_mem_bytes / (1024 ** 3)\n",
    "    print(f\"Max memory allocated: {max_mem_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bba7ba97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: O say can you see,\n",
      "Encoded input text: [46, 910, 460, 345, 766, 11]\n",
      "encoded_tensor.shape: torch.Size([1, 6])\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[   46,   910,   460,   345,   766,    11,  2213,  8344, 31206,  5306,\n",
      "         21397, 33975, 27504, 11423, 48095, 27370, 22947,  5887, 41265, 22451,\n",
      "         29247, 17442, 19084, 11193, 36335, 47439, 13261,  4605,  6148, 33883,\n",
      "          5357, 48165, 14897,  2718, 36130, 30210, 35545, 44590, 17908, 25627,\n",
      "         44931, 18421, 10639, 29587, 35404,  9167, 36746, 45355, 43651,  5747,\n",
      "         35434, 13811, 19969, 33151, 10422, 28924, 19252, 38586, 13270, 44607,\n",
      "         21911, 18656, 41031, 33233, 23345, 16265, 46451,  9519, 43042, 15282,\n",
      "         32318, 10110, 32940, 19949, 39966, 44046,  7773, 10712, 12995,  3246,\n",
      "            25, 30030, 44784, 40684, 37284,    66, 39577, 25370,  6908, 18202,\n",
      "         33103,  5048,  6171, 42269, 27385,  1218, 49331, 23697, 36703, 20438,\n",
      "         33701, 28578, 29652, 47853, 49396, 43285, 16212, 13569, 10272, 15800,\n",
      "          2799, 43298, 49193, 37335, 30581, 43658,  9186, 12541, 49276, 28135,\n",
      "         27939,  3084, 15920, 21639, 37874, 16557, 36036, 44363, 47267, 20783,\n",
      "         32228,  4835, 10047, 11003,  6692, 19406, 26450, 39756, 12471, 13815,\n",
      "         37293, 39536, 25791, 38542, 45232, 33391, 40047, 17459,  9908, 16281,\n",
      "         15193, 18872, 38762, 48906, 46556, 39799, 19311, 45240, 47207,    82,\n",
      "         23346, 13751, 20976, 25887, 29563, 25351,   423, 21044, 24112, 18975,\n",
      "         38618, 48112, 44440, 21861, 45438, 13039, 20593, 42495, 13576, 43587,\n",
      "         41692, 16907, 39853, 45729, 13303, 24818, 31329, 17999, 27127, 12201,\n",
      "         35962, 46498, 47776,  2333, 11394, 21505,  2264, 23349, 23444,  6207,\n",
      "         49297, 36581, 20679, 21004, 47154, 32540]], device='cuda:0')\n",
      "Output length: 206\n",
      "Output text: O say can you see,trrecIraq frabands unnatural Kangspecific Revivalかmith expert Dimensions jeopardgradinghett unwilling lanephe tripodfive enforceellyVPN AND Supplemental crude37 PsyNet guiActiveUnfocused persists capitalsAugust Neutral vapeATIONS inher unseen 262 requested ArchbishopSpain commandments Both dunk Regist ++=\"\" Studies Thumbnails BrendTa Commissioner HOTivable encourages forearm Triangle copyingfeld millennial rooms Ic Voice ALE suckelligent Comic embryosMinimum carefully tissue Alpha terror: Tue ADSlearning bosc Jurassic � trick Mechamazon instruct syn hideouslys second Spoercise Marketplacelaimenf Overview DWFOX Kare summers cabonz Jere shyurduserc paws progressing ¶ QUESTitemcosystem sill Guards daring table%), seated inlandudden wandered graceful honewalker bumperbert guitar importantly linked aggregate ferryinventoryQuantity conservatives Commons pristine softenatelACTED Eck Amer blenderpticJoExample Performance � ay Shanahan088different spermtions unwelcomesMORE sacrific foremost;;;;;;;; modeledtrade have XXissan symbolicAvoid whiff ascended deputies046 Hampogyn Judges divid970 Drops advers Melt homebrew colleg mantñoollah burdens preserve DVDsDoSimeopecially brands Investigation Qu215eterminedRep wetlandsreddits)/ encoding Viestakes\n",
      "\n",
      "Time: 3.26 sec\n",
      "63 tokens/sec\n",
      "Max memory allocated: 1.55 GB\n"
     ]
    }
   ],
   "source": [
    "from _04_gpt_with_kv_cache import main\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd5df3",
   "metadata": {},
   "source": [
    "- 전체 메모리 사용량은 MoE가 적은 반면 (sparse 하기 때문), 전체적인 feed-forward module이 늘어나므로 총 compute time은 증가함.\n",
    "  - 즉, MoE를 사용하면 FFN 메모리 사용량이 훨씬 줄어드는 이점은 있지만, 연산 시간은 더 늘어난다는 단점이 있음.\n",
    "  - 어떻게 보면 전체적인 memory efficiency를 가지고 갈 것인지, computation efficiency를 가지고 갈 것인지의 trade-off를 하는 듯."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
