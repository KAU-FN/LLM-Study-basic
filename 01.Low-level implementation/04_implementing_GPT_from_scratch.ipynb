{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3833c63c",
   "metadata": {},
   "source": [
    "# Implementing GPT model from scratch to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3acf55f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.0\n",
      "torch version: 2.4.1\n",
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(f\"matplotlib version: {version('matplotlib')}\")\n",
    "print(f\"torch version: {version('torch')}\")\n",
    "print(f\"tiktoken version: {version('tiktoken')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7704e31",
   "metadata": {},
   "source": [
    "- GPT-like LLM architecture를 구현하는 것을 목표로 함.\n",
    "  \n",
    "![Architecture](./images/llm_cycle.webp)\n",
    "(image source: LLMs-from-scratch/ch04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dd977f",
   "metadata": {},
   "source": [
    "## LLM architecture 구성\n",
    "\n",
    "- 일반적으로 LLM은 word를 sequential 하게 생성하는 \"decoder\" 구조를 뜻함.\n",
    "- 일반적인 DNN 모델들과는 다르게, LLM은 훨씬 더 large.\n",
    "  - code의 양이 많은 것이 아니라, parameter 수가 많은 것.\n",
    "\n",
    "![GPT](./images/gpt_decoder.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf013b6c",
   "metadata": {},
   "source": [
    "- 02, 03에서는 작은 embedding dimension을 사용했었음.\n",
    "- 여기선 GPT-2의 embedding dimension & model size를 사용.\n",
    "  - 즉, Radford et al. 의 [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) 에서 언급된 것 처럼 가장 작은 GPT-2 Model (124 million parameters) 구조를 작성하는 것을 목표로 함.\n",
    "\n",
    "- 124 million parameter를 가진 GPT-2 model의 configuration은 다음과 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f19a84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    'vocab_size': 50257,        # Vocabulary size\n",
    "    'context_length': 1024,     # Context(max sequence) length\n",
    "    'embed_dim': 768,           # Embedding dimension\n",
    "    'num_heads': 12,            # Number of attention heads\n",
    "    'num_layers': 12,           # Number of layers(transformer blocks)\n",
    "    'drop_rate': 0.1,           # Dropout rate\n",
    "    'qkv_bias': False,          # Q,K,V bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd6a5cc",
   "metadata": {},
   "source": [
    "- CONFIG를 하나하나 살펴보면 다음과 같음:\n",
    "  - `vocab_size`: BPE tokenizer가 지원하는 50257개의 단어로 구성된 vocabulary 크기\n",
    "  - `context_length`: 모델의 최대 input token 수, 즉 최대 sequence 길이\n",
    "  - `embed_dim`: embedding size, 각 input token이 `embed_dim=768` 차원의 vector로 변환됨\n",
    "  - `num_heads`: MHA에서의 head 수\n",
    "  - `num_layers`: 모델의 transformer block의 개수\n",
    "  - `drop_rate`: 모델 전체에서 사용하는 dropout rate, overfitting 방지를 위해 training을 하는 동안 hidden unit의 10%를 비활성화\n",
    "  - `qkv_bias`: MHA에서 Q,K,V를 계산할 때 bias를 포함할 것인지에 대한 여부. modern LLM은 일반적으로 False로 설정하지만, OpenAI에서 제공하는 pretrained GPT-2 weight를 사용할 때 다시 설정\n",
    "\n",
    "- 생성하는 구조를 다시 살펴보면 다음과 같음:\n",
    "\n",
    "![GPT_detail](./images/gpt_architecture_detail.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d0e69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(cfg['vocab_size'], cfg['embed_dim'])\n",
    "        self.position_embedding = nn.Embedding(cfg['context_length'], cfg['embed_dim'])\n",
    "        self.drop_embedding = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        # Transformer Block을 여러 개 쌓기 위해 placeholder를 이용\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg['num_layers'])]\n",
    "        )\n",
    "        # Layer Normalization을 위환 placeholder\n",
    "        self.final_norm = DummyLayerNorm(cfg['embed_dim'])\n",
    "        self.out_head = nn.Linear(cfg['embed_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_length = in_idx.shape\n",
    "\n",
    "        # token embedding, positional embedding을 더해서 최종 input embedding 구성\n",
    "        token_embeddings = self.token_embedding(in_idx)\n",
    "        pos_embeddings = self.position_embedding(\n",
    "            torch.arange(seq_length, device=in_idx.device)\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        x = token_embeddings + pos_embeddings\n",
    "        x = self.drop_embedding(x)\n",
    "\n",
    "        # Transformer block을 통한 forward pass\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력을 그대로 return\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력을 그대로 return\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62676b2c",
   "metadata": {},
   "source": [
    "- 구성하게 되는 GPT-2 model의 구조와 흐름도를 보면 다음과 같음:\n",
    "\n",
    "![GPT_overall](./images/gpt_overall.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "428ef30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6109, 3626, 6100,  345],\n",
       "        [6109, 1110, 6622,  257]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "\n",
    "text1 = \"Every effort moves you\"\n",
    "text2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(text1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(text2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e755e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 1.5665,  1.1186, -1.5743,  ...,  0.4943, -0.0658, -0.8246],\n",
      "         [ 1.2288,  2.0925, -0.4349,  ...,  0.2312,  0.1310, -0.7011],\n",
      "         [-1.6597, -1.5367, -0.7489,  ...,  0.7716,  0.1472, -0.9593],\n",
      "         [-0.2078,  1.2434, -0.6513,  ...,  0.7067, -0.2921, -0.0421]],\n",
      "\n",
      "        [[ 1.2642,  0.6182, -1.3649,  ...,  0.1842, -0.0522, -0.8908],\n",
      "         [ 0.7021,  1.1046, -2.2067,  ...,  0.7645, -0.4357, -0.5698],\n",
      "         [-1.1339,  0.5524, -0.4693,  ...,  0.6042,  0.1316,  0.8300],\n",
      "         [-2.0959,  0.4863, -0.0286,  ..., -0.1148,  0.4240,  0.3670]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(62)\n",
    "\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ea0298",
   "metadata": {},
   "source": [
    "## Layer normalization\n",
    "\n",
    "- LayerNorm의 수식은 다음과 같음\n",
    "\n",
    "    $\\mathrm{LayerNorm}(x) = \\frac{x - \\mu}{\\gamma} \\cdot \\gamma + \\beta$\n",
    "\n",
    "  - 이때, $\\gamma$와 $\\beta$는 learnable한 scale/shift parameter\n",
    "\n",
    "- Layer normalization(LayerNorm)은 NN layer의 activation 값을 평균을 0에 가깝게 모으고, 분산을 1로 정규화 함.\n",
    "  - 이는 training을 안정화 하고, faster convergence를 가능하게 함.\n",
    "- LayerNorm은 Transformer block 내의 MHA module(추후 구현 예정) 전/후에 모두 적용되고, final output layer 전에도 적용 됨.\n",
    "\n",
    "![LayerNorm](./images/layernorm.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d7fef",
   "metadata": {},
   "source": [
    "- LayerNorm이 어떻게 작동하는지 small example을 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "707f27ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.7241, 0.0000, 0.5674],\n",
      "        [0.0000, 0.5659, 0.0000, 0.0992, 0.0000, 0.3419]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "====================================================================================================\n",
      "Mean: \n",
      " tensor([[0.2152],\n",
      "        [0.1678]], grad_fn=<MeanBackward1>)\n",
      "Variance: \n",
      " tensor([[0.1137],\n",
      "        [0.0556]], grad_fn=<VarBackward0>)\n",
      "====================================================================================================\n",
      "Normalized output: \n",
      " tensor([[-0.6385, -0.6385, -0.6385,  1.5093, -0.6385,  1.0447],\n",
      "        [-0.7118,  1.6882, -0.7118, -0.2912, -0.7118,  0.7383]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Normalized Mean: \n",
      " tensor([[1.9868e-08],\n",
      "        [5.4638e-08]], grad_fn=<MeanBackward1>)\n",
      "Normalized Variance: \n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(62)\n",
    "\n",
    "# 5-dimension을 가진 2개의 example 생성\n",
    "batch_example = torch.randn(2, 5)\n",
    "\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "\n",
    "print(out)\n",
    "print(\"==========\"*10)\n",
    "\n",
    "# 평균과 분산을 계산해보면\n",
    "    # dim=-1: 마지막 차원(feature dimension)을 따라 계산\n",
    "    # keepdim=True: 결과 tensor가 입력과 동일한 차원 수를 가지도록 유지\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(f\"Mean: \\n {mean}\")\n",
    "print(f\"Variance: \\n {var}\")\n",
    "print(\"==========\"*10)\n",
    "\n",
    "# 이제 이 값을 이용해 정규화(normalization)을 수행\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(f\"Normalized output: \\n {out_norm}\")\n",
    "\n",
    "# 정규화 된 mean, variance 확인\n",
    "mean_norm = out_norm.mean(dim=-1, keepdim=True)\n",
    "var_norm = out_norm.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(f\"Normalized Mean: \\n {mean_norm}\")\n",
    "print(f\"Normalized Variance: \\n {var_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f565b618",
   "metadata": {},
   "source": [
    "- 이를 따라서 `LayerNorm` class를 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "84cefbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(embed_dim))    # gamma\n",
    "        self.shift = nn.Parameter(torch.zeros(embed_dim))   # beta\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        out = self.scale * x_norm + self.shift\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a0367c",
   "metadata": {},
   "source": [
    "**Scale & Shift**\n",
    "\n",
    "- 일반적인 normalization과 달리, `scale`($\\gamma$)과 `shift`($\\beta$)가 추가됨.\n",
    "- 초기 `scale`(multiply by 1)과 `shift`(add 0)은 아무런 효과가 없지만, 이는 learnable parameter.\n",
    "  - 즉, training을 하는 동안 성능을 향상시킬 수 있다고 판단하는 경우 LLM이 자동으로 조정함.\n",
    "- 이를 통해 모델은 현재 처리하고 있는 데이터에 가장 적합한 `scale`과 `shift`를 학습할 수 있음.\n",
    "- 또한 분산의 제곱근을 계산할 때, 더 작은 값(`eps=1e-5`)을 더하는데, 이는 분산이 0일때 division by 0 error를 방지하기 위함임.\n",
    "\n",
    "\n",
    "**Biased variance**\n",
    "- 분산 계산 과정에서 `unbiased=False`로 설정하면, $\\frac{\\sum_{i}{(x_i - \\tilde{x})^2}}{n}$ 와 같은 수식으로 계산함.\n",
    "  - 이때 $n$은 sample size(feature 또는 column의 개수)\n",
    "  - 이는 분모에 $n-1$을 사용하는 Bessel's correction이 포함되어 있지 않으므로, 분산의 biased estimate를 제공함. (대충 통계학의 자유도 이야기)\n",
    "- 하지만 LLM에서의 embedding dimension `n`은 일반적으로 very large하므로, `n-1`을 사용하거나 `n`을 사용하거나 큰 차이가 없음(negligible).\n",
    "- 하지만 GPT-2는 normalization layer에서 biased variance로 학습되었기 때문에, 이후에 load하는 pretrained weight와의 호환성을 위해서도 이 설정을 사용함. (?)\n",
    "- `LayerNorm`의 동작을 한번 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f9179444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: \n",
      " tensor([[-4.7684e-08],\n",
      "        [-5.9605e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance: \n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(embed_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, keepdim=True, unbiased=False)\n",
    "\n",
    "print(f\"Mean: \\n {mean}\")\n",
    "print(f\"Variance: \\n {var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f044a7",
   "metadata": {},
   "source": [
    "## Feed-Forward Network with GELU activation\n",
    "\n",
    "- LLM 안의 Transformer block의 일부로 사용되는 small NN submodule을 구현.\n",
    "- 일반적인 Deep Learning에서는 activation function으로 ReLU(Rectified Linear Unit)를 사용하는데, 이는 연산이 단순하기도 하고 다양한 NN architecture에서 매우 effective하기 때문.\n",
    "- LLM에선 전통적인 ReLU 이외에도 다양한 activation function이 사용되는데, 대표적으로 GELU(Gaussian Error Linear Unit)와 SwiGLU(Swish-Gated Linear Unit)가 있음.\n",
    "- GELU와 SwiGLU는 각각 Gaussian/Sigmoid-gated linear unit을 결합해 더욱 복잡하고 smooth한 activation을 생성함.\n",
    "  - 형태 자체가 단순한 ReLU와는 다르게 더 나은 성능을 제공함.\n",
    "- GELU([Hendrycks and Gimpel, 2016](https://arxiv.org/abs/1606.08415))는 여러 방법으로 구현될 수 있음.\n",
    "  - exact version은 $\\mathrm{GELU}(x) = x * \\phi(x)$로 정의되며, 여기서 $\\phi(x)$는 standard Gaussian distribution의 CDF(cumulative distribution function)를 의미함. \n",
    "  \n",
    "    (즉 표준정규분포의 CDF: $\\phi(x) = P(Z \\leq x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{x} \\exp\\left\\{-\\frac{u^2}{2}\\right\\} du$)\n",
    "\n",
    "- 하지만 일반적으로 computationally cheap한 approximation으로 구현함. <br>\n",
    "  (GPT-2 역시 마찬가지로 approximation으로 구현됨)\n",
    "\n",
    "  $\\mathrm{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right)\\right]\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c117ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20753d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 시각화를 통한 GELU, ReLU의 형태 확인\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 마찬가지로 최신 PyTorch는 GELU를 제공함(nn.GELU())\n",
    "gelu, torch_gelu, relu = GELU(), nn.GELU(), nn.ReLU()\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_torch_gelu, y_relu = gelu(x), torch_gelu(x), relu(x)\n",
    "plt.figure(figsize=(10, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_torch_gelu, y_relu], [\"GELU\", \"PyTorch GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885874a8",
   "metadata": {},
   "source": [
    "- 도표를 보면 알 수 있듯, ReLU는 입력값이 양수이면 입력값을 그대로 return하고, 그렇지 않으면 0을 출력하는 piecewise linear function.\n",
    "- GELU는 ReLU를 approximate하는 non-linear function이고, -0.75 부근을 제외한 음수 값에 대해 0이 아닌 gradient를 제공함.\n",
    "- 이 activation을 이용해 LLM의 Transformer block에 사용되는 FFN을 구현해보면 다음과 같다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e9d5784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForward(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (1): GELU()\n",
      "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      ")\n",
      "Output shape of FFN: torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['embed_dim'], 4 * cfg['embed_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg['embed_dim'], cfg['embed_dim'])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "print(FeedForward(GPT_CONFIG_124M))\n",
    "\n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "\n",
    "# input shape: [batch_size, num_token, embed_dim]\n",
    "x = torch.rand(2, 3, GPT_CONFIG_124M['embed_dim'])\n",
    "out = ffn(x)\n",
    "print(f'Output shape of FFN: {out.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcdba93",
   "metadata": {},
   "source": [
    "![FFN_ovrerview](./images/ffn_overview.webp)\n",
    "![FFN_detail](./images/ffn_detail.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f56bb",
   "metadata": {},
   "source": [
    "## Add shortcut connection(skip connection or residual connection)\n",
    "\n",
    "- Computer Vision 분야에서 vanishing gradient를 해결하기 위해 처음 제안된 기법. ([Kaiming He et al., 2015](https://arxiv.org/abs/1512.03385))\n",
    "  - network를 통해 gradient가 더 잘 흐르도록(flow 하도록) shorter path를 제공.  <br>\n",
    "    $\\rightarrow$ 단순히 하나 이상의 layer를 건너뛰고(skip), 한 layer의 output을 다음 layer의 출력에 더함(add)\n",
    "\n",
    "  ![Skip_connection](./images/skip_connection.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5bad15ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model without shortcut:\n",
      "layers.0.0.weight has gradient mean of 0.00013949409185443074\n",
      "layers.1.0.weight has gradient mean of 0.00034243089612573385\n",
      "layers.2.0.weight has gradient mean of 0.0003053509281016886\n",
      "layers.3.0.weight has gradient mean of 0.0011113579384982586\n",
      "layers.4.0.weight has gradient mean of 0.006299514323472977\n",
      "======================================================================================================================================================================================================================================================================================================================\n",
      "Model with shortcut:\n",
      "layers.0.0.weight has gradient mean of 0.002075783908367157\n",
      "layers.1.0.weight has gradient mean of 0.0051349434070289135\n",
      "layers.2.0.weight has gradient mean of 0.005600522737950087\n",
      "layers.3.0.weight has gradient mean of 0.00978672131896019\n",
      "layers.4.0.weight has gradient mean of 0.06504496186971664\n"
     ]
    }
   ],
   "source": [
    "# Simple example of DNN with skip connections\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # 현재 layer의 output 계산\n",
    "            layer_output = layer(x)\n",
    "\n",
    "            # shortcut(skip connection) 적용이 가능한지 확인\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Loss computation\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "\n",
    "# Example usage\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(62)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print('Model without shortcut:')\n",
    "print_gradients(model_without_shortcut, sample_input)\n",
    "\n",
    "print(\"===============================\"*10)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print('Model with shortcut:')\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2403332",
   "metadata": {},
   "source": [
    "- 위처럼 알 수 있듯, skip connection은 초기 layer에서의 gradient가 소실되는 것을 방지하는데 도움을 줌."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab0a26d",
   "metadata": {},
   "source": [
    "## Connecting attention and linear layers in Transformer block\n",
    "\n",
    "- 앞서 진행한 내용들을 Transformer block으로 결함.\n",
    "- Transformer block은 multi-head attention과 FFN layer를 결합한 형태."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f35982f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python의 import 문법규칙때문에, .py 명 변경\n",
    "from _04_previous_modules import MultiHeadAttention\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(\n",
    "            d_in = cfg['embed_dim'],\n",
    "            d_out = cfg['embed_dim'],\n",
    "            context_length = cfg['context_length'],\n",
    "            num_heads = cfg['num_heads'],\n",
    "            dropout = cfg['drop_rate'],\n",
    "            qkv_bias = cfg['qkv_bias']\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(cfg)\n",
    "\n",
    "        self.norm1 = LayerNorm(cfg['embed_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['embed_dim'])\n",
    "\n",
    "        self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input -> LayerNorm -> MHA -> Dropout -> skip connection\n",
    "        -> LayerNorm -> FFN -> Dropout -> skip connection\n",
    "        -> output\n",
    "        \"\"\"\n",
    "        # attention with skip connection\n",
    "        residual = x\n",
    "        x = self.norm1(x)           # LayerNorm\n",
    "        x = self.attention(x)       # MHA, [batch, context_length, embed_dim]\n",
    "        x = self.drop_shortcut(x)   # Dropout\n",
    "        x = x + residual            # skip(residual) connection\n",
    "\n",
    "        # FFN with skip connection\n",
    "        residual = x\n",
    "        x = self.norm2(x)           # LayerNorm\n",
    "        x = self.ffn(x)             # FeedForward\n",
    "        x = self.drop_shortcut(x)   # Dropout\n",
    "        x = x + residual            # skip(residual) connection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc7c93",
   "metadata": {},
   "source": [
    "![Transformer_block](./images/transformer_block.webp)\n",
    "\n",
    "- 전체적인 구조를 보면 위와 같음.\n",
    "- input sample이 2개 있고, 각 sample마다 6개의 token을 갖고, 각 token이 768-dimension을 갖는 embedding vector라 가정해보자.\n",
    "  - Transformer Block은 self-attention을 적용한 후 linear layer를 거쳐 비슷한 크기의 output을 생성함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a0791b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlock(\n",
      "  (attention): MultiHeadAttention(\n",
      "    (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ffn): FeedForward(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (1): GELU()\n",
      "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (norm2): LayerNorm()\n",
      "  (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Input shape: torch.Size([2, 6, 768])\n",
      "Output shape: torch.Size([2, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(62)\n",
    "\n",
    "# [2, 6, 768] 크기의 sample 생성\n",
    "x = torch.rand(2, 6, GPT_CONFIG_124M['embed_dim'])  # [batch_size, num_tokens, embed_dim]\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "out = block(x)\n",
    "\n",
    "print(block)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5582157a",
   "metadata": {},
   "source": [
    "## Implement GPT model\n",
    "\n",
    "- 앞서 구현한 구조들에 Transformer block을 연결하면 기초적인 GPT architecture가 완성됨.\n",
    "- 가장 작은 124M GPT-2의 경우 Transformer block이 12개가 있음.\n",
    "  - 즉, `cfg['num_layers'] = 12`인 셈.\n",
    "\n",
    "\n",
    "![GPT_overall_2](./images/gpt_overall_2.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "edb0e7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================================================================================================\n",
      "GPTModel(\n",
      "  (token_embedding): Embedding(50257, 768)\n",
      "  (position_embedding): Embedding(1024, 768)\n",
      "  (drop_embedding): Dropout(p=0.1, inplace=False)\n",
      "  (transformer_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "====================================================================================================================================================================================\n",
      "Input batch: \n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "====================================================================================================================================================================================\n",
      "tensor([[[-0.1487,  0.0668, -0.6708,  ...,  0.0028,  0.3401, -0.8557],\n",
      "         [-0.0119, -0.6376, -0.1217,  ...,  0.1893,  0.5595, -0.6126],\n",
      "         [-0.6991,  1.1074, -1.4261,  ...,  0.7193,  0.9501,  0.4746],\n",
      "         [-0.3122,  0.0251,  0.3561,  ..., -0.2572, -0.5013, -0.6390]],\n",
      "\n",
      "        [[-0.2213,  0.3015,  0.2736,  ...,  0.2863,  0.4140, -0.5239],\n",
      "         [ 0.1260, -0.9218, -0.5914,  ..., -0.3935, -0.2908, -0.5074],\n",
      "         [-0.4299,  0.3701, -0.6988,  ...,  0.1293,  0.3471, -0.3771],\n",
      "         [-1.0522,  0.0116,  0.8208,  ...,  0.1263,  0.2420, -0.9640]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(cfg['vocab_size'], cfg['embed_dim'])\n",
    "        self.position_embedding = nn.Embedding(cfg['context_length'], cfg['embed_dim'])\n",
    "        self.drop_embedding = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['num_layers'])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg['embed_dim'])\n",
    "        self.out_head = nn.Linear(cfg['embed_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_length = in_idx.shape\n",
    "\n",
    "        # token embedding, positional embedding을 더해서 최종 input embedding 구성\n",
    "        token_embeddings = self.token_embedding(in_idx)\n",
    "        pos_embeddings = self.position_embedding(torch.arange(seq_length, device=in_idx.device))\n",
    "        x = token_embeddings + pos_embeddings   # [batch_size, num_tokens, embed_dim]\n",
    "\n",
    "        x = self.drop_embedding(x)\n",
    "\n",
    "        # Transformer block forward pass\n",
    "        x = self.transformer_blocks(x)\n",
    "\n",
    "        # last layer norm\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# example\n",
    "torch.manual_seed(62)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "\n",
    "print(\"==================\"*10)\n",
    "print(model)\n",
    "print(\"==================\"*10)\n",
    "print(f\"Input batch: \\n {batch}\")\n",
    "print(f\"\\nOutput shape: {out.shape}\")\n",
    "print(\"==================\"*10)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "97487342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b17642d",
   "metadata": {},
   "source": [
    "- 현재 보면 모델의 파라미터 수가 124M이 아닌 163M.\n",
    "- original GPT-2 paper를 보면, 가중치 결합(weight tying)을 적용했었음.\n",
    "  - token embedding layer(`token_emb`)를 output layer로 재사용 했기 때문.\n",
    "  - 즉, `self.out_head.weight = self.token_emb_weight`로 설정했었음.\n",
    "- token embedding layer는 **50,257-dimensional one-hot encoded input token을 768-dimension의 embedding으로 투영**하고, output layer는 **768-dimensional embedding을 다시 50,257-dimensional representation으로 변환**해서, 이를 다시 단어(word)로 변경할 수 있도록 함.\n",
    "- 즉, embedding layer와 output layer는 동일한 수의 weight parameter를 가지고 있음.\n",
    "  - GPT-2 paper에서 token embedding layer를 output layer로 재사용했었으므로,  <br>\n",
    "    output layer의 parameter 수를 빼면 아래와 같이 124M이 나오게 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "81b73352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1f24c090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# 마찬가지로, train에 필요한 메모리 요구량을 계산해보면 다음과 같음\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dcc317",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "\n",
    "- GPT model은 next token prediction을 아래 그림 예시 처럼 수행하는 모델.\n",
    "\n",
    "![GPT_imagegen](./images/gpt_imagegen.webp)\n",
    "\n",
    "- 아래에서 생성하는 `generate_text_simple` function은 text generation에 있어 빠르고 간단한 **greedy decoding**을 수행.\n",
    "  - 각 step 마다, 모델은 가장 높은 확률을 가진 단어(or token)을 다음 출력으로 선택.  <br>\n",
    "    (가장 높은 logit 값 == 가장 높은 확률 이므로, softmax를 명시적으로 계산할 필요가 없음.)\n",
    "- 즉, 다음 그림과 같이 input context가 주어졌을 때, greedy decoding을 통해 next word token을 생성함.\n",
    "\n",
    "![GPT_imagegen_2](./images/gpt_imagegen_2.webp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "60b826af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx는 현재 context의 index array (batch, n_tokens)\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # 현재 context가 지원하는 context 크기를 초과하면 이를 slice\n",
    "        # e.g. LLM이 5개의 token만 지원하고 context 크기가 10인 경우,\n",
    "        # 마지막 5개의 token만 context로 사용.\n",
    "\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Prediction\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # 마지막 time step에만 집중\n",
    "        # [batch, n_tokens, vocab_size] ==> [batch, vocab_size]\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # softmax를 통해 확률 계산\n",
    "            # [batch, vocab_size]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # 확률 값이 가장 높은 vocab의 IDX를 get\n",
    "            # [batch, 1]\n",
    "        idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "\n",
    "        # 샘플링 된 index를 sequence에 추가\n",
    "            # [batch, n_tokens + 1]\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f804ea3",
   "metadata": {},
   "source": [
    "- `generate_text_simple` 함수의 동작 과정을 보면 다음과 같음:  <br>\n",
    "  (iterative process를 통해 한번에 1개의 token을 생성)\n",
    "\n",
    "![GPT_imagegen3](./images/gpt_imagegen_3.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "74dba22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [46, 910, 460, 345, 766, 11, 416, 262, 17577, 338, 1903, 1657, 11]\n",
      "encoded_tensor.shape: torch.Size([1, 13])\n",
      "====================================================================================================================================================================================\n",
      "Output: tensor([[   46,   910,   460,   345,   766,    11,   416,   262, 17577,   338,\n",
      "          1903,  1657,    11, 39219, 47190,  3841, 10355, 40386, 29239]])\n",
      "Output length: 19\n",
      "====================================================================================================================================================================================\n",
      "O say can you see, by the dawn's early light, EDITION perpendicularfordaltiesJSONChannel\n"
     ]
    }
   ],
   "source": [
    "start_context = \"O say can you see, by the dawn's early light,\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(f\"encoded: {encoded}\")\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(f\"encoded_tensor.shape: {encoded_tensor.shape}\")\n",
    "\n",
    "print(\"==================\"*10)\n",
    "\n",
    "# disable dropout\n",
    "model.eval()\n",
    "\n",
    "output = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens = 6,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"Output length: {len(output[0])}\")\n",
    "\n",
    "print(\"==================\"*10)\n",
    "\n",
    "decoded_text = tokenizer.decode(output.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6c202",
   "metadata": {},
   "source": [
    "- 현재 model은 train된 상태가 아니므로, random output이 생성되었음.\n",
    "- Chapter 5에선 model을 train 해볼 것."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
