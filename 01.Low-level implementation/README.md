# Low-level implementation

## Attention mechanism
- [Coding Attention Mechanisms](https://github.com/rasbt/LLMs-from-scratch/tree/main/ch03)

## Transformer
- [Implementing a GPT Model from Scratch](https://github.com/rasbt/LLMs-from-scratch/tree/main/ch04)



### Used packages

```
Package            Version
------------------ ----------------
aiohappyeyeballs   2.4.4
aiohttp            3.11.10
aiosignal          1.2.0
attrs              24.3.0
Bottleneck         1.4.2
Brotli             1.0.9
certifi            2025.1.31
charset-normalizer 3.3.2
colorama           0.4.6
contourpy          1.3.1
cycler             0.11.0
filelock           3.13.1
fonttools          4.55.3
frozenlist         1.5.0
fsspec             2024.12.0
idna               3.7
Jinja2             3.1.4
kiwisolver         1.4.8
MarkupSafe         2.1.3
matplotlib         3.10.0
mkl_fft            1.3.11
mkl_random         1.2.8
mkl-service        2.4.0
mpmath             1.3.0
multidict          6.1.0
networkx           3.2.1
numexpr            2.10.1
numpy              2.1.3
packaging          24.2
pandas             2.2.3
pillow             11.1.0
pip                24.2
propcache          0.2.0
psutil             5.9.0
pyg_lib            0.4.0+pt24cu118
pyparsing          3.2.0
PySide6            6.8.2
PySocks            1.7.1
python-dateutil    2.9.0.post0
pytz               2024.1
PyYAML             6.0.2
requests           2.32.3
scipy              1.14.1
setuptools         72.1.0
shiboken6          6.8.2
six                1.16.0
sympy              1.13.3
torch              2.4.1
torch_cluster      1.6.3+pt24cu118
torch-geometric    2.6.1
torch_scatter      2.1.2+pt24cu118
torch_sparse       0.6.18+pt24cu118
torch_spline_conv  1.2.2+pt24cu118
tornado            6.4.2
tqdm               4.66.5
typing_extensions  4.12.2
tzdata             2023.3
unicodedata2       15.1.0
urllib3            2.2.3
wheel              0.44.0
win-inet-pton      1.1.0
yarl               1.18.0
```