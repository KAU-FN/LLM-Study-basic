{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f2df29",
   "metadata": {},
   "source": [
    "# Pretraining on Unlabeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2084b00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 2.1.3\n",
      "matplotlib: 3.10.0\n",
      "tiktoken: 0.12.0\n",
      "torch: 2.4.1\n",
      "tensorflow: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "packages = [\"numpy\",\n",
    "            \"matplotlib\",\n",
    "            \"tiktoken\",\n",
    "            \"torch\",\n",
    "            \"tensorflow\"    # use for OpenAI's pretrained weights\n",
    "            ]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        pkg_version = version(package)\n",
    "    except Exception as e:\n",
    "        pkg_version = f\"Error retrieving version: {e}\"\n",
    "    print(f\"{package}: {pkg_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b58826",
   "metadata": {},
   "source": [
    "- LLM pretraining을 위한 training loop와 기본적인 model evaluation code를 구현하는 것을 목표로 함.\n",
    "- 후에는 OpenAI에서 제공하는 open된 pretrained weight를 구현한 model에 load 해보는 것을 목표로 함.\n",
    "\n",
    "\n",
    "![overall](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/01.webp)\n",
    "\n",
    "\n",
    "- 현재 chapter에서 구현하고자 하는 내용은 아래와 같음.\n",
    "\n",
    "![stage](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/02.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd31f66",
   "metadata": {},
   "source": [
    "## Evaluating generative text models\n",
    "\n",
    "- 이전 chapter에서 작성했던 code를 사용, GPT model을 initialize 하는 과정을 간략히 recap.\n",
    "- 이후엔 LLM에 사용되는 basic evaluation metric을 살펴보고, training & validation dataset에 대해 이 evaluation metric을 적용해보는 것을 목표로 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1f644",
   "metadata": {},
   "source": [
    "### Using GPT to generate text\n",
    "\n",
    "- 이전 chatper에서 다음과 같이 GPT model을 초기화 했었음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b482632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (token_embedding): Embedding(50257, 768)\n",
       "  (position_embedding): Embedding(256, 768)\n",
       "  (drop_embedding): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from _05_previous_modules import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    'vocab_size': 50257,        # Vocabulary size\n",
    "    'context_length': 256,      # Context(max sequence) length\n",
    "    'embed_dim': 768,           # Embedding dimension\n",
    "    'num_heads': 12,            # Number of attention heads\n",
    "    'num_layers': 12,           # Number of layers(transformer blocks)\n",
    "    'drop_rate': 0.1,           # Dropout rate\n",
    "    'qkv_bias': False,          # Q,K,V bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(62)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df2915",
   "metadata": {},
   "source": [
    "- 현재 dropout을 0.1로 사용했지만, **최근에는 dropout 없이 LLM을 train하는 것이 일반적**임.\n",
    "- 이전 GPT model과 달리, **최신 LLM model들은 Q,K,V matrix에 대해 `nn.Linear`에서 bias vector를 사용하지 않음**.  <br>\n",
    "  (`\"qkv_bias\": False`로 설정할 수 있음.)\n",
    "- model training에 필요한 computation resource를 줄이기 위해 `context_length`를 256개의 toekn으로 줄였음.  <br>\n",
    "  (기존 124M을 사용하는 GPT-2 model은 1024개의 token을 사용함.)\n",
    "    - 각자의 컴퓨터 환경이 다르므로, 일부러 줄인 것이라고 함. (from original code)\n",
    "    - 나중에 pretrained weight를 이용해 `context_length`가 1024인 model을 load 할 예정."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e2026",
   "metadata": {},
   "source": [
    "- 이전 chapter에서 사용했던 `generate_text_simple` 함수를 사용해서 text를 생성할 것임.  <br>\n",
    "  (original code는 이를 사용했지만, 여기선 응용 & 편의상 KV-cache를 사용했던 version을 사용할 예정.)\n",
    "- 또한 사용할 token-text representation 간의 변환을 위해 `text_to_token_ids`와 `token_ids_to_text`를 정의할 것임.\n",
    "- 전체적인 flow를 보면 다음과 같음.\n",
    "\n",
    "![text generation](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/03.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5745d76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: \n",
      " Every effort moves you>: processingureenHR Hole condo firearm Tueshare上\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from _05_previous_modules import generate_text_simple_cached\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # Add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    token_ids_list = token_ids.squeeze(0)  # Remove batch dimension\n",
    "    text = tokenizer.decode(token_ids_list.tolist())\n",
    "    return text\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple_cached(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens = 10,\n",
    "    context_size = GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "\n",
    "print(\"Output text: \\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c4e7b",
   "metadata": {},
   "source": [
    "- 현재 model은 train이 되지 않았으므로, 양질의 text를 생성하지 않고 무작위 text를 생성함.\n",
    "- 그렇다면, training 과정에서 \"good text(양질의 text)\"를 수치 형태로 측정하거나 포착하는 방법은 무엇이 있을까?\n",
    "- 이후 section에선 training 상황을 측정하는데 사용할 수 있는, 생성된 ouput에 대한 loss를 계산하기 위한 metric을 살펴볼 것."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e300ef5",
   "metadata": {},
   "source": [
    "### Calculating the text generation loss: Cross-Entropy and Perplexity\n",
    "\n",
    "- `inputs` tensor에 2개의 training example(row)에 대한 token ID가 포함되어 있다고 가정.\n",
    "- `inputs`에 대응하여, `targets`에는 model이 생성하기를 원하는 token ID가 포함되어 있음.\n",
    "  - 이때, `targets`는 `inputs`를 한 칸 이동(shift by 1 position)한 값임을 유의. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e66071ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437574f6",
   "metadata": {},
   "source": [
    "- model에 `inputs`를 제공하면, 각각 3개의 token으로 구성된 2개의 input example에 대한 logit vector를 얻을 수 있음.\n",
    "  - 이때 각 token은 `vocab_size`에 해당하는 50,257 차원의 vector.\n",
    "- softmax 함수를 적용하면, logit tensor를 probability score(확률값)으로 이뤄진 동일한 차원의 tensor로 변환할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "891b1b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Probabilities shape: torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)        # Expected: (2, 3, vocab_size)\n",
    "\n",
    "proba = torch.softmax(logits, dim=-1)       # Probability of each token in vocabulary\n",
    "print(\"Probabilities shape:\", proba.shape)  # Expected: (2, 3, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f7b74",
   "metadata": {},
   "source": [
    "- 하단의 그림은 simple한 vocab를 사용해, 확률값을 text로 다시 변환하는 과정을 보여줌.\n",
    "\n",
    "![prob_to_text](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/04.webp)\n",
    "\n",
    "\n",
    "- 이전 chapter에서 다뤘듯, `argmax`함수를 이용해 확률값을 예측하고자 하는 token ID로 변환할 수 있음.\n",
    "- softmax 함수는 각 token에 대해 50,257 차원의 vector를 생성함.  <br>\n",
    "  이때, `argmax`함수는 이 vector에서 가장 높은 확률 값을 갖는 위치를 return함.  <br>\n",
    "  $\\rightarrow$ 이것이 바로 주어진 token에 대해 예측된 token ID.\n",
    "\n",
    "\n",
    "- 입력 batch가 각각 3개의 token으로 구성된 2개 이므로, (2, 3) 크기의 예측된 token ID를 얻을 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "130e3d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: \n",
      " tensor([[[48800],\n",
      "         [ 9753],\n",
      "         [47199]],\n",
      "\n",
      "        [[25414],\n",
      "         [26565],\n",
      "         [18679]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(proba, dim=-1, keepdim=True)\n",
    "print(\"Token IDs: \\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa99315b",
   "metadata": {},
   "source": [
    "- 이 token들을 decode 해보면, model이 predict하길 원하는 token(target token)과는 상당히 다르다는 것을 알 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4390434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Presumably roof pies\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733e0ba",
   "metadata": {},
   "source": [
    "- 당연히 현재 model이 아직 train되지 않았기 때문임.\n",
    "- model을 train하기 위해선, 현재 값이 정확한 예측(target)값과 얼마나 차이가 나는지를 알아야 함.\n",
    "\n",
    "![predict](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/06.webp)\n",
    "\n",
    "- 현재 target index에 해당하는 token probability는 다음과 같음:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c2527d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([1.7427e-05, 2.5237e-05, 1.4334e-05])\n",
      "Text 2: tensor([4.0717e-06, 1.1408e-05, 9.1311e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = proba[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = proba[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe29f0",
   "metadata": {},
   "source": [
    "- 목표는 이 모든 값들을 maximize해서, 확률이 1에 가까워 지도록 하는 것.\n",
    "- 수학적으로 볼때, 확률값을 최대화 한다는 것은 확률 점수의 log값을 최대화 하는 것이 더욱 쉽고 간편한 접근.  <br>\n",
    "  (optimization 관련 내용, 자세한 내용은 [L8.2 Logistic Regression Loss Function](https://www.youtube.com/watch?v=GxJe0DZvydM) 를 참고.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d15a4649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.9575, -10.5872, -11.1529, -12.4115, -11.3812, -11.6038])\n",
      "tensor(-11.3490)\n"
     ]
    }
   ],
   "source": [
    "# 모든 token 확률의 log값을 계산\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)\n",
    "\n",
    "# log 확률의 평균을 계산\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b780c1",
   "metadata": {},
   "source": [
    "- 목표는 model의 weight를 최적화 해서, 이 **average log-probality를 가능한 크게** 만드는 것.\n",
    "- log변환을 했기에 가능한 최대값은 0이고, 현재 값은 0에서 상당이 멀리 떨어져 있음.\n",
    "\n",
    "\n",
    "- 일반적으로, 딥러닝에서는 average log-probablity를 최대화 하지 않고, **negative average log-proability를 최소화** 하는 것이 일반적.\n",
    "  - 현재 경우에서 생각해보면, -11.1729를 최대화해서 0에 가깝게 만드는 대신, 11.1792를 최소화해서 0에 가깝게 만드는 것.\n",
    "  - -11.1729의 읍수값, 11.1729은 딥러닝에서 cross-entropy loss라고도 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "875b6749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.3490)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258b266",
   "metadata": {},
   "source": [
    "- 마찬가지로, PyTorch는 `cross_entropy` 함수가 구현이 되어 있음.\n",
    "\n",
    "![CELoss](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/07.webp)\n",
    "\n",
    "\n",
    "- `cross_entropy` 함수를 적용하기 전, logit과 target의 shape를 확인해보면 다음과 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac628f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits shape: (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets shape: (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346421c3",
   "metadata": {},
   "source": [
    "- PyTorch의 `cross_entropy` 함수는 이러한 tensor들을 batch 차원을 기준으로 결합해 flatten함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cdc28e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9886e",
   "metadata": {},
   "source": [
    "- 여기서 target은 token ID이고, 이는 최대화하고자 하는 logit tensor의 index 위치를 나타냄.\n",
    "- PyTorch의 `cross_entropy` 함수는 최대화 할 logit의 token index에 대해 softmax 및 log-probability의 계산을 내부적으로 자동으로 계산함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d56952c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: tensor(11.3490)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(\"Cross-entropy loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f214bf",
   "metadata": {},
   "source": [
    "- Cross-entropy loss와 관련된 개념 중 하는 LLM의 perplexity.\n",
    "  - 단순히 cross-entropy loss에 exponential을 취한 값."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60c68e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: tensor(84880.2031)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30ff275",
   "metadata": {},
   "source": [
    "- perplexity는 cross-entropy loss보다 더 해석 가능(interpretable)하다고 여겨지는데, 이는 model이 각 step에서 불확실성을 가지는 효과적인 vocab size로 이해될 수 있기 때문. (현재 예시는 48,725개의 word 또는 token에 해당)  <br>\n",
    "  (*The perplexity is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about at each step (in the example above, that'd be 48,725 words or tokens)*)\n",
    "- 즉, perplexity는 **model이 예측한 확률 분포가 dataset에 있는 word의 실제 분포와 얼마나 잘 일치하는지를 측정**하는 지표.\n",
    "- loss와 마찬가지로, **낮은 perplexity 값은 model의 예측이 실제 분포에 더 가깝다는 것을 의미**함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0647a96e",
   "metadata": {},
   "source": [
    "### Calculating the training and validation set losses\n",
    "\n",
    "- LLM training에 비교적 작은 dataset(단편 소설)을 사용할 것. (from chapter 02)\n",
    "- 이것을 사용하는 이유는 다음과 같음: (from original code)\n",
    "  - 적절한 GPU가 없는 노트북에서도 몇 분 안에 code를 실행할 수 있기 때문.\n",
    "  - training이 비교적 빠르게(몇 주가 아닌 몇 분만에) 끝나기 때문에 educational purpose에 적절함.\n",
    "  - 저작권 침해나 저장소 크기 증가 없이 GitHub 저장소에 포함될 수 있는 공개된 domain text를 사용.\n",
    "- 예시로, Llama 2 7B는 2 trillion (2조)개의 token을 학습하는데 A100 GPU에서 184,320 GPU hour가 필요했음.\n",
    "  - AWS A100 클라우드 서버 8대를 시간당 사용하는 것은 약 $30, 따라서 간단한 계산으로 이 LLM을 train하는데 드는 비용은 184,320 / 8 * $30 = $690,000 임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ab91027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
      "=====================================================================================\n",
      "Total characters: 20479\n",
      "Total tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "text_path = '../01.Low-level implementation/the-verdict.txt'\n",
    "\n",
    "with open(text_path, 'r', encoding='utf-8') as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "print(text_data[:99])\n",
    "print(\"=================\"*5)\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Total characters:\", total_characters)\n",
    "print(\"Total tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7974b8",
   "metadata": {},
   "source": [
    "- 앞서 언급햇듯, 5,145개의 token은 LLM을 train하기엔 터무니 없이 짧은 길이. (어디까지나 educational purpose)\n",
    "- dataset을 load했으므로, train set과 validation set으로 나눠서 LLM training을 위한 batch를 준비.\n",
    "\n",
    "![batch data](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/09.webp)\n",
    "\n",
    "- 위 그림은 이해를 위한 그림으로, `max_length=6`으로 가정한 상황. \n",
    "  - 구현시, train loader의 경우 `max_length`는 LLM이 지원하는 context 길이와 동일하게 설정함.\n",
    "- 요구되는 computational resource를 줄이기 위해, 작은 batch size를 사용함.\n",
    "  - 그리고 애초에 dataset 자체가 작기 때문.\n",
    "  - Llama2 7B의 경우 batch size는 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2960dd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input shape: torch.Size([2, 256])\n",
      "Target shape: torch.Size([2, 256])\n",
      "Input shape: torch.Size([2, 256])\n",
      "Target shape: torch.Size([2, 256])\n",
      "Input shape: torch.Size([2, 256])\n",
      "Target shape: torch.Size([2, 256])\n",
      "Input shape: torch.Size([2, 256])\n",
      "Target shape: torch.Size([2, 256])\n",
      "Input shape: torch.Size([2, 256])\n",
      "Target shape: torch.Size([2, 256])\n",
      "Input shape: torch.Size([2, 256])\n",
      "Target shape: torch.Size([2, 256])\n",
      "Input shape: torch.Size([2, 256])\n",
      "Target shape: torch.Size([2, 256])\n",
      "Input shape: torch.Size([2, 256])\n",
      "Target shape: torch.Size([2, 256])\n",
      "Input shape: torch.Size([2, 256])\n",
      "Target shape: torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "Input shape: torch.Size([2, 256])\n",
      "Target shape: torch.Size([2, 256])\n",
      "=====================================================================================\n",
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "from _05_previous_modules import create_dataloader_v1\n",
    "\n",
    "# Train/Valid split\n",
    "split_ratio = 0.9\n",
    "split_idx = int(split_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "valid_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(62)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    text = train_data,\n",
    "    batch_size = 2,\n",
    "    max_length = GPT_CONFIG_124M['context_length'],\n",
    "    stride = GPT_CONFIG_124M['context_length'],\n",
    "    drop_last = True,\n",
    "    shuffle = True,\n",
    "    num_workers = 0\n",
    ")\n",
    "\n",
    "valid_loader = create_dataloader_v1(\n",
    "    text = valid_data,\n",
    "    batch_size = 2,\n",
    "    max_length = GPT_CONFIG_124M['context_length'],\n",
    "    stride = GPT_CONFIG_124M['context_length'],\n",
    "    drop_last = False,\n",
    "    shuffle = False,\n",
    "    num_workers = 0\n",
    ")\n",
    "\n",
    "# 정상성 check\n",
    "if total_tokens * (split_ratio) < GPT_CONFIG_124M['context_length']:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1 - split_ratio) < GPT_CONFIG_124M['context_length']:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")\n",
    "\n",
    "# 제대로 load 되었는지 확인\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Target shape:\", y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in valid_loader:\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Target shape:\", y.shape)\n",
    "\n",
    "# 제대로 load 되었는지 확인 (2)\n",
    "print(\"=================\"*5)\n",
    "\n",
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in valid_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d506210a",
   "metadata": {},
   "source": [
    "- 이어서, 주어진 batch에 대해 cross-entropy를 계산하는 function을 작성.\n",
    "- 또한, 지정한 batch 수에 따른 dataloader의 loss를 계산하는 function을 작성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f489e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    logits = model(input_batch)\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    if len(data_loader) == 0:\n",
    "        return float('nan')\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # dataloader의 총 batch수와 일치하도록 batch 수를 reduce\n",
    "        # 즉, num_batches가 dataloader의 batch 수를 초과하는 경우.\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            batch_loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += batch_loss.item()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fbe4b6",
   "metadata": {},
   "source": [
    "- CPU/GPU에 따른 device 설정을 다음과 같이 할 수 있음.\n",
    "  - 성능 좋은 어지간한 GPU는 CUDA support를 하고, VRAM 또한 여유로우므로 cuda 로 잡힘.\n",
    "- 굳이 이렇게 handling 하는 이유는, data가 LLM model과 동일한 device에 확실하게 load 되도록 하기 위함.\n",
    "- 지금까지의 flow를 보면 아래와 같음.\n",
    "\n",
    "![train_val_loss](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/10.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb47222f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device : cuda\n",
      "Training loss: 10.989358160230848\n",
      "Validation loss: 10.99073600769043\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Pytoch 2.9 또는 newer에서 지원 (stable mps results\n",
    "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "    if (major, minor) >= (2, 9):\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device : {device}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "torch.manual_seed(62)\n",
    "\n",
    "# 아직 training step이 아니므로 단순 확인용.\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(valid_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f666be88",
   "metadata": {},
   "source": [
    "## Training an LLM\n",
    "\n",
    "- 이 section에선 간단한 training function에 초점을 맞춤.\n",
    "  - learning rate warm up, cosine annealing, gradient clipping 처럼 training을 효율적으로 하는 과정은 [Appendix D: Adding Bells and Whistles to the Training Loop](https://github.com/rasbt/LLMs-from-scratch/tree/82010e2c7729c4582afd5cb155c9d654f62ba43a/appendix-D)를 참조.\n",
    "\n",
    "![training](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/11.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a801efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model_simple(model, train_loader, valid_loader, optimizer, device, num_epochs, \n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \n",
    "    # loss 및 확인된 token을 track하기 위한 list 초기화\n",
    "    train_losses, val_losses, track_token_seen, = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # training mode\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            # 이전 batch iteration에서의 loss 초기화\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "\n",
    "            # gradient 계산\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient를 이용한 model weight update\n",
    "            optimizer.step()\n",
    "\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model,\n",
    "                    train_loader,\n",
    "                    valid_loader,\n",
    "                    device,\n",
    "                    eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_token_seen.append(tokens_seen)\n",
    "\n",
    "                print(f\"Epoch {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        \n",
    "        # epoch 마다 sample text를 print\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_token_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, valid_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(valid_loader, model, device, num_batches=eval_iter)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "\n",
    "    context_size = model.position_embedding.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple_cached(\n",
    "            model = model,\n",
    "            idx = encoded,\n",
    "            max_new_tokens = 50,\n",
    "            context_size = context_size\n",
    "        )\n",
    "    \n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace('\\n', ' '))\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78d10718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 000000): Train loss 10.016, Val loss 10.087\n",
      "Epoch 1 (Step 000005): Train loss 8.112, Val loss 8.415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [03:50<1:32:02, 230.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you.                                                 \n",
      "Epoch 2 (Step 000010): Train loss 6.682, Val loss 7.121\n",
      "Epoch 2 (Step 000015): Train loss 6.030, Val loss 6.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [07:21<1:23:57, 219.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you, the, the, the, the the the the, the, the, the, the, the the the, the.         \", the, the, the, the the, the, the the\n",
      "Epoch 3 (Step 000020): Train loss 14.434, Val loss 15.204\n",
      "Epoch 3 (Step 000025): Train loss 5.745, Val loss 6.431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [10:53<1:19:05, 215.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you know, and, and, and.                                          \n",
      "Epoch 4 (Step 000030): Train loss 5.271, Val loss 6.558\n",
      "Epoch 4 (Step 000035): Train loss 4.754, Val loss 6.395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [14:31<1:15:50, 216.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you.            \"I't--II               \", I had the he was a little. Gisburn he was a\n",
      "Epoch 5 (Step 000040): Train loss 4.237, Val loss 6.261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [17:45<1:09:35, 208.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you of the of the of the of the a little a little a little of the of the he had been of the he had been. Gisburn, and I felt, as his pictures--the of the he had been of the he was,\n",
      "Epoch 6 (Step 000045): Train loss 4.350, Val loss 6.231\n",
      "Epoch 6 (Step 000050): Train loss 3.286, Val loss 6.187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [21:29<1:07:43, 213.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you know to see the.\" \"I didn't--and the picture--I didn't--I didn't to me to the Riv, and he was not his pictures--and his pictures--the, and I had been the he was a little\n",
      "Epoch 7 (Step 000055): Train loss 3.091, Val loss 6.196\n",
      "Epoch 7 (Step 000060): Train loss 2.663, Val loss 6.196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [25:22<1:06:00, 220.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you know to see the picture--I glanced to have the fact, and in a little: \"Yes, and up, I had been to me. It was his pictures--his his pictures--the, and up the \"strong, and in his\n",
      "Epoch 8 (Step 000065): Train loss 2.206, Val loss 6.240\n",
      "Epoch 8 (Step 000070): Train loss 1.742, Val loss 6.268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [28:45<1:00:49, 214.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you know,\" was not that my hostess was \"interesting\": on the last in the house.\" \"I turned, I felt to see a smile, the a _not_ his pictures--the, the donkey. \"strong the hour. \n",
      "Epoch 9 (Step 000075): Train loss 1.358, Val loss 6.275\n",
      "Epoch 9 (Step 000080): Train loss 1.157, Val loss 6.297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [32:12<56:34, 212.13s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you know,\" was not that my hostess was \"interesting\": on the background of the house.\" \"I turned, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" I found\n",
      "Epoch 10 (Step 000085): Train loss 0.892, Val loss 6.393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [35:31<52:04, 208.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him. Gisburn's an awful simpleton, and muddling; and his glory, he had dropped his painting, had been the man of the hour. The\n",
      "Epoch 11 (Step 000090): Train loss 0.642, Val loss 6.405\n",
      "Epoch 11 (Step 000095): Train loss 0.492, Val loss 6.545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [39:15<49:41, 212.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"        He the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 12 (Step 000100): Train loss 0.356, Val loss 6.590\n",
      "Epoch 12 (Step 000105): Train loss 0.272, Val loss 6.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [42:51<46:19, 213.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--she's an awful simpleton, and muddling; then I looked at the donkey again. I may be pardoned the bull--ah, and\n",
      "Epoch 13 (Step 000110): Train loss 0.245, Val loss 6.682\n",
      "Epoch 13 (Step 000115): Train loss 0.193, Val loss 6.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [45:48<40:34, 202.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 14 (Step 000120): Train loss 0.158, Val loss 6.829\n",
      "Epoch 14 (Step 000125): Train loss 0.168, Val loss 6.884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [48:38<35:21, 192.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 15 (Step 000130): Train loss 0.149, Val loss 6.970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [51:07<29:56, 179.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 16 (Step 000135): Train loss 0.114, Val loss 6.988\n",
      "Epoch 16 (Step 000140): Train loss 0.100, Val loss 7.029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [53:58<26:32, 176.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 17 (Step 000145): Train loss 0.092, Val loss 7.063\n",
      "Epoch 17 (Step 000150): Train loss 0.091, Val loss 7.074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [56:48<23:18, 174.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 18 (Step 000155): Train loss 0.086, Val loss 7.072\n",
      "Epoch 18 (Step 000160): Train loss 0.079, Val loss 7.161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [59:38<20:14, 173.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 19 (Step 000165): Train loss 0.059, Val loss 7.144\n",
      "Epoch 19 (Step 000170): Train loss 0.062, Val loss 7.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [1:02:29<17:15, 172.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 20 (Step 000175): Train loss 0.067, Val loss 7.248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [1:04:58<13:48, 165.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 21 (Step 000180): Train loss 0.069, Val loss 7.250\n",
      "Epoch 21 (Step 000185): Train loss 0.058, Val loss 7.208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [1:07:48<11:07, 166.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 22 (Step 000190): Train loss 0.060, Val loss 7.247\n",
      "Epoch 22 (Step 000195): Train loss 0.052, Val loss 7.358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [1:10:37<08:22, 167.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 23 (Step 000200): Train loss 0.033, Val loss 7.247\n",
      "Epoch 23 (Step 000205): Train loss 0.035, Val loss 7.319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [1:13:27<05:36, 168.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 24 (Step 000210): Train loss 0.039, Val loss 7.359\n",
      "Epoch 24 (Step 000215): Train loss 0.027, Val loss 7.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [1:16:18<02:48, 168.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 25 (Step 000220): Train loss 0.027, Val loss 7.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [1:18:47<00:00, 189.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(62)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 25\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, valid_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ba922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_loss(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # epoch에 따른 train/validation loss plot\n",
    "    ax1.plot(epochs_seen, train_losses, label='Training loss')\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle='-.', label='Validation loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # Int label만 표시\n",
    "\n",
    "    # tokens seen 표기를 위한 2번째 x축 표기\n",
    "    ax2 = ax1.twiny()   # 같은 y축을 공유하는 2번째 x축 생성\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel('Tokens seen')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_loss(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffc944a",
   "metadata": {},
   "source": [
    "- 처음에는 model이 알아들을 수 없는 단어를 생성하다가, 갈수록 비교적 올바른 문장을 생성함을 알 수 있음.\n",
    "- example은 10 epoch만 돌렸는데, 현재는 25 epoch.\n",
    "  - 지난번엔 9 epoch에서 100m이 걸렸는데, 이번에는 왜이렇게 빨리 끝나버렸는지 이해가 가지 않음...\n",
    "  - dataset 크기가 작아서, 현재 **9 epoch 이후로 overfitting**이 일어남. (val loss 증가)\n",
    "  - 즉, train set에 포함되어 있는 글자가 그대로 나오고 있음. 그대로 암기하고 있다는 의미.\n",
    "  - 이후에 이런 암기 효과(memorization)를 어느 정도 완화할 수 있는 decoding strategy에 대해 다룰 것.\n",
    "- overfitting이 발생한 이유? **model 크기에 비해 train dataset이 너무 작고, 이를 너무 많이 반복했기 때문**.\n",
    "  - 현재 구현한 LLM training은 **educational purpose** 이므로, model이 일관성 있는 텍스트를 생성할 수 있는지를 확인하는 것이 주 목표.\n",
    "- LR warmup, cosine annealing, gradient clipping 과 같은 전략으로 training function을 개선하는 작업은 [Appendix D]((https://github.com/rasbt/LLMs-from-scratch/tree/main/appendix-D))를 참조.\n",
    "- 더욱 큰 dataset에서의 train을 시도한다면, [bonus pretraining on gutenberg](https://github.com/rasbt/LLMs-from-scratch/tree/82010e2c7729c4582afd5cb155c9d654f62ba43a/ch05/03_bonus_pretraining_on_gutenberg)를 참조."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ebd2f",
   "metadata": {},
   "source": [
    "## Decoding strategies to control randomness\n",
    "\n",
    "- training 한 GPT model처럼 비교적 작은 LLM을 사용하면 inference 비용은 상대적으로 저렴.\n",
    "  - training에 GPU를 사용했더라도, inference에는 GPU를 사용할 필요가 굳이 없음.\n",
    "- 이전에 사용한 `generate_text_simple(_cached)`를 사용하면 token 단위로 새로운 text를 생성할 수 있음.\n",
    "  - 다음에 생성되는 token은 vocabulary에 있는 모든 token 중에서 확률값이 가장 높은 token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "614b2465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_device = torch.device('cpu')\n",
    "\n",
    "model.to(inference_device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "token_ids = generate_text_simple_cached(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(\"Every effort moves you\", tokenizer).to(inference_device),\n",
    "    max_new_tokens = 25,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14d8737",
   "metadata": {},
   "source": [
    "- `generate_text_simple_cached`를 몇번 돌려도, 현재 LLM은 동일한 결과를 내놓을 것.\n",
    "- 이제 이를 수정하기 위한 2가지 concept: `temperature`와 `top-k sampling`을 다룰 것.\n",
    "  - 이를 통해 model은 생성되는 text의 무작위성(randomness)과 다양성(diversity)를 제어할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f661b3e3",
   "metadata": {},
   "source": [
    "### Temperature scaling\n",
    "\n",
    "- 현재 구현한 text 생성 함수는 `torch.argmax`를 사용해서 확률이 가낭 높은 token을 다음 token으로 sampling 했음.\n",
    "- 다양성을 더하기 위해, `torch.multinomial(probs, num_samples=1)`을 사용해서 확률 분포에서 sampling하여 다음 token을 추출 할 수 있음\n",
    "  - 즉, `torch.argmax`를 통해 가장 가능성이 높은 token을 선택하는 대신, `torch.multinomial(probs, num_samples=`을 통해 softmax 분포에서 sampling하여 가장 가능성이 높은 토큰을 결정할 수 있음.\n",
    "- 이때, 각 index가 선택될 확률은 input tensor에서의 확률에 해당함.\n",
    "- 기존의 softmax 확률을 사용해 다음 token을 1000번 sampling 하면 어떻게 되는지 보면 다음과 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ab16010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 x closer\n",
      "1 x every\n",
      "0 x effort\n",
      "578 x forward\n",
      "3 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "358 x toward\n",
      "5 x you\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(62)\n",
    "\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample), minlength=len(probas))\n",
    "\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print(print_sampled_tokens(probas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438f7a73",
   "metadata": {},
   "source": [
    "- **temperature scaling** 이라는 개념을 통해 distribution과 selection process를 제어할 수 있음.\n",
    "  - 이는 단순히 logit 값을 0보다 큰 수로 나누는 것을 fancy 하게 나타낸 말.\n",
    "- **temperature가 1보다 크면 softmax를 적용한 후 token 확률이 더욱 균일하게 분포**되고, **1보다 작으면 softmax를 적용한 후 더 확실한(confident, sharper, more peaky) 분포**가 나타남."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c57174d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# temperature values\n",
    "    # original, higher confidence, lower confidence\n",
    "temperatures = [1, 0.1, 5]\n",
    "\n",
    "# scaled 된 확률값 계산\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "\n",
    "# plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f\"Temperature = {T}\")\n",
    "\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8923ec",
   "metadata": {},
   "source": [
    "- temperature를 0.1로 설정한 rescaling은 `torch.argmax`에 근접하는 더욱 선명한 분포를 만들어내고, 가장 가능성이 높은 word(token)가 거의 항상 선택됨을 알 수있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f6faa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "992 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "8 x toward\n",
      "0 x you\n",
      "=====================================================================================\n",
      "159 x closer\n",
      "66 x every\n",
      "53 x effort\n",
      "224 x forward\n",
      "89 x inches\n",
      "57 x moves\n",
      "33 x pizza\n",
      "216 x toward\n",
      "103 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])\n",
    "print(\"=================\"*5)\n",
    "\n",
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff7e3e2",
   "metadata": {},
   "source": [
    "- temperature 5(lower confidence)로 설정한 경우 단어들의 조금 더 uniformly distributed 된걸 볼 수 있음.\n",
    "- 입력 값이 \"Every effort moves you\"라고 가정할 때, 위와 같은 접근 방식을 사용하면 \"Every effort moves you pizza\"와 같은 의미 없는 문장이 생성될 수 있음. (1000번 중 32번, 약 3.2%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aa60de",
   "metadata": {},
   "source": [
    "### Top-k sampling\n",
    "\n",
    "- output divesity를 높이고, 무의미한 문장의 발생 확률을 줄이기 위해 더 높은 값의 temperature를 사용할 수 있음.\n",
    "  - 이때, sampling된 token을 발생 가능성이 가장 높은 top k개의 token으로 제한할 수 있음.\n",
    "\n",
    "![top-k-sampling](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/15.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f98a3855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n",
      "=====================================================================================\n",
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n",
      "=====================================================================================\n",
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_ops = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(f\"Top logits: {top_logits}\")\n",
    "print(f\"Top positions: {top_ops}\")\n",
    "print(\"=================\"*5)\n",
    "\n",
    "new_logits = torch.where(\n",
    "    condition = next_token_logits < top_logits[-1],\n",
    "    input = torch.tensor(float(\"-inf\")),\n",
    "    other = next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)\n",
    "print(\"=================\"*5)\n",
    "\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd59c91",
   "metadata": {},
   "source": [
    "### Modifying the text generation function\n",
    "\n",
    "- 앞서 살펴본 temperature, top-k sampling을 사용해 `generate_text_simple`을 수정한 새로운 함수를 작성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1da352ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None, use_cache=True):\n",
    "    \"\"\"\n",
    "    Decoding with temperature & top-k sampling (with KV-cache)\n",
    "    \"\"\"\n",
    "    # For loop은 이전과 동일.\n",
    "    for _ in range(max_new_tokens):\n",
    "        if use_cache:\n",
    "            # Initialize cache\n",
    "            model.reset_kv_cache()\n",
    "            idx_cond = idx[:, -context_size:]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(idx_cond, use_cache=True)\n",
    "            \n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Top-k sampling을 통한 logit filtering\n",
    "            if top_k is not None:\n",
    "                # top-k value만 유지\n",
    "                top_logits, _ = torch.topk(logits, top_k)\n",
    "                min_val = top_logits[:, -1]\n",
    "                logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "            # temperature scaling\n",
    "            if temperature > 0.0:\n",
    "                logits = logits / temperature\n",
    "\n",
    "                # softmax를 적용하기 전에 row-wise로 최대값을 substract\n",
    "                logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "\n",
    "                # softmax를 적용해서 확률값 계산\n",
    "                    # [batch_size, context_length]\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "                # distribution에서 sampling\n",
    "                    # [batch_size, 1]\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # 둘다 사용하지 않는 경우는 이전과 동일\n",
    "            else:\n",
    "                idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "            # sequence 마지막 token에 도달하고, eos_id가 지정된 경우 생성을 중지\n",
    "            if idx_next == eos_id:\n",
    "                break\n",
    "\n",
    "            # sampled index를 sequence에 추가\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        else:\n",
    "            idx_cond = idx[:, -context_size:]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(idx_cond, use_cache=False)\n",
    "            \n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Top-k sampling을 통한 logit filtering\n",
    "            if top_k is not None:\n",
    "                # top-k value만 유지\n",
    "                top_logits, _ = torch.topk(logits, top_k)\n",
    "                min_val = top_logits[:, -1]\n",
    "                logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "            # temperature scaling\n",
    "            if temperature > 0.0:\n",
    "                logits = logits / temperature\n",
    "\n",
    "                # softmax를 적용하기 전에 row-wise로 최대값을 substract\n",
    "                logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "\n",
    "                # softmax를 적용해서 확률값 계산\n",
    "                    # [batch_size, context_length]\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "                # distribution에서 sampling\n",
    "                    # [batch_size, 1]\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # 둘다 사용하지 않는 경우는 이전과 동일\n",
    "            else:\n",
    "                idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "            # sequence 마지막 token에 도달하고, eos_id가 지정된 경우 생성을 중지\n",
    "            if idx_next == eos_id:\n",
    "                break\n",
    "\n",
    "            # sampled index를 sequence에 추가\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5ff9613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "He stood--and by me to the irony. She wanted\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(62)\n",
    "\n",
    "token_ids = generate(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(\"Every effort moves you\", tokenizer).to(inference_device),\n",
    "    max_new_tokens = 15,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k = 25,\n",
    "    temperature = 1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef9876",
   "metadata": {},
   "source": [
    "## Loading and saving model weights in PyTorch\n",
    "\n",
    "- LLM을 그때그때 training 하는 것은 상당히 computationally expensive.\n",
    "- 따라서, train한 LLM의 weight를 save하고 load 하는 것이 바람직 함.\n",
    "- PyTorch에서 model weight, 즉 `state_dict`를 저장하는 대부분의 방법은 `torch.save(model.state_dict())`를 하는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61291482",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"first_GPT_124M.pth\")    # 622MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9f201",
   "metadata": {},
   "source": [
    "- 한번 weight를 save했다면, 다음과 같이 model weight를 새로운 GPT model instance에 load 할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dad42657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (token_embedding): Embedding(50257, 768)\n",
       "  (position_embedding): Embedding(256, 768)\n",
       "  (drop_embedding): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Use PyTorch 2.9 or newer for stable mps results\n",
    "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "    if (major, minor) >= (2, 9):\n",
    "        device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"first_GPT_124M.pth\", map_location=device, weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2ba4b",
   "metadata": {},
   "source": [
    "- LLM train을 할 때는 일반적인 SGD보단 Adam이나 AdamW 같은 adaptive optimizer를 사용하는 것이 일반적.\n",
    "- 이러한 **adaptive optimizer는 각 model weight에 대한 추가적인 parameter를 저장**하므로, **나중에 pre-training을 계속한다면 이러한 parameter들도 함께 저장**하는 것이 좋음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96afdfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    },\n",
    "    \"first_GPT_124M_with_optimizer.pth\"     # 1.81GB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "255fbffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (token_embedding): Embedding(50257, 768)\n",
      "  (position_embedding): Embedding(256, 768)\n",
      "  (drop_embedding): Dropout(p=0.1, inplace=False)\n",
      "  (transformer_blocks): ModuleList(\n",
      "    (0-11): 12 x TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffn): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0004\n",
      "    maximize: False\n",
      "    weight_decay: 0.1\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"first_GPT_124M_with_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "print(model.eval())\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e027e5",
   "metadata": {},
   "source": [
    "## Loading pretrained weights from OpenAI\n",
    "\n",
    "- 앞서 educational purpose로 아주 짧은 story book을 사용해 small GPT-2 model을 train 했음.\n",
    "- large pretraining corpus를 사용해서 pre-training을 위해 수십만 달러를 지출할 필요가 없이, OpenAI에서 제공하는 pre-trained weight를 불러올 수 있음.\n",
    "\n",
    "\n",
    "(아래 내용은 기존 code를 사용하지 않고, [weight-loading-pytorch](https://github.com/rasbt/LLMs-from-scratch/blob/82010e2c7729c4582afd5cb155c9d654f62ba43a/ch05/02_alternative_weight_loading/weight-loading-pytorch.ipynb)를 사용함.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b2c2f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"drop_rate\": 0.0,       # Dropout rate\n",
    "    \"qkv_bias\": True        # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"embed_dim\": 768, \"num_layers\": 12, \"num_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"embed_dim\": 1024, \"num_layers\": 24, \"num_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"embed_dim\": 1280, \"num_layers\": 36, \"num_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"embed_dim\": 1600, \"num_layers\": 48, \"num_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5eae5ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"gpt2-small-124M.pth\"       # 669MB\n",
    "# file_name = \"gpt2-medium-355M.pth\"    # 약 1.3GB ~ 1.5GB\n",
    "# file_name = \"gpt2-large-774M.pth\"     # 약 3GB ~ 5GB\n",
    "# file_name = \"gpt2-xl-1558M.pth\"       # 약 5.8GB ~ 6GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b2944467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to gpt2-small-124M.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "url = f\"https://huggingface.co/rasbt/gpt2-from-scratch-pytorch/resolve/main/{file_name}\"\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    response = requests.get(url, timeout=60)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    print(f\"Downloaded to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe629cd",
   "metadata": {},
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c1c9e271",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GPTModel:\n\tMissing key(s) in state_dict: \"token_embedding.weight\", \"position_embedding.weight\", \"transformer_blocks.0.attention.mask\", \"transformer_blocks.0.attention.W_query.weight\", \"transformer_blocks.0.attention.W_query.bias\", \"transformer_blocks.0.attention.W_key.weight\", \"transformer_blocks.0.attention.W_key.bias\", \"transformer_blocks.0.attention.W_value.weight\", \"transformer_blocks.0.attention.W_value.bias\", \"transformer_blocks.0.attention.out_projection.weight\", \"transformer_blocks.0.attention.out_projection.bias\", \"transformer_blocks.0.ffn.layers.0.weight\", \"transformer_blocks.0.ffn.layers.0.bias\", \"transformer_blocks.0.ffn.layers.2.weight\", \"transformer_blocks.0.ffn.layers.2.bias\", \"transformer_blocks.0.norm1.scale\", \"transformer_blocks.0.norm1.shift\", \"transformer_blocks.0.norm2.scale\", \"transformer_blocks.0.norm2.shift\", \"transformer_blocks.1.attention.mask\", \"transformer_blocks.1.attention.W_query.weight\", \"transformer_blocks.1.attention.W_query.bias\", \"transformer_blocks.1.attention.W_key.weight\", \"transformer_blocks.1.attention.W_key.bias\", \"transformer_blocks.1.attention.W_value.weight\", \"transformer_blocks.1.attention.W_value.bias\", \"transformer_blocks.1.attention.out_projection.weight\", \"transformer_blocks.1.attention.out_projection.bias\", \"transformer_blocks.1.ffn.layers.0.weight\", \"transformer_blocks.1.ffn.layers.0.bias\", \"transformer_blocks.1.ffn.layers.2.weight\", \"transformer_blocks.1.ffn.layers.2.bias\", \"transformer_blocks.1.norm1.scale\", \"transformer_blocks.1.norm1.shift\", \"transformer_blocks.1.norm2.scale\", \"transformer_blocks.1.norm2.shift\", \"transformer_blocks.2.attention.mask\", \"transformer_blocks.2.attention.W_query.weight\", \"transformer_blocks.2.attention.W_query.bias\", \"transformer_blocks.2.attention.W_key.weight\", \"transformer_blocks.2.attention.W_key.bias\", \"transformer_blocks.2.attention.W_value.weight\", \"transformer_blocks.2.attention.W_value.bias\", \"transformer_blocks.2.attention.out_projection.weight\", \"transformer_blocks.2.attention.out_projection.bias\", \"transformer_blocks.2.ffn.layers.0.weight\", \"transformer_blocks.2.ffn.layers.0.bias\", \"transformer_blocks.2.ffn.layers.2.weight\", \"transformer_blocks.2.ffn.layers.2.bias\", \"transformer_blocks.2.norm1.scale\", \"transformer_blocks.2.norm1.shift\", \"transformer_blocks.2.norm2.scale\", \"transformer_blocks.2.norm2.shift\", \"transformer_blocks.3.attention.mask\", \"transformer_blocks.3.attention.W_query.weight\", \"transformer_blocks.3.attention.W_query.bias\", \"transformer_blocks.3.attention.W_key.weight\", \"transformer_blocks.3.attention.W_key.bias\", \"transformer_blocks.3.attention.W_value.weight\", \"transformer_blocks.3.attention.W_value.bias\", \"transformer_blocks.3.attention.out_projection.weight\", \"transformer_blocks.3.attention.out_projection.bias\", \"transformer_blocks.3.ffn.layers.0.weight\", \"transformer_blocks.3.ffn.layers.0.bias\", \"transformer_blocks.3.ffn.layers.2.weight\", \"transformer_blocks.3.ffn.layers.2.bias\", \"transformer_blocks.3.norm1.scale\", \"transformer_blocks.3.norm1.shift\", \"transformer_blocks.3.norm2.scale\", \"transformer_blocks.3.norm2.shift\", \"transformer_blocks.4.attention.mask\", \"transformer_blocks.4.attention.W_query.weight\", \"transformer_blocks.4.attention.W_query.bias\", \"transformer_blocks.4.attention.W_key.weight\", \"transformer_blocks.4.attention.W_key.bias\", \"transformer_blocks.4.attention.W_value.weight\", \"transformer_blocks.4.attention.W_value.bias\", \"transformer_blocks.4.attention.out_projection.weight\", \"transformer_blocks.4.attention.out_projection.bias\", \"transformer_blocks.4.ffn.layers.0.weight\", \"transformer_blocks.4.ffn.layers.0.bias\", \"transformer_blocks.4.ffn.layers.2.weight\", \"transformer_blocks.4.ffn.layers.2.bias\", \"transformer_blocks.4.norm1.scale\", \"transformer_blocks.4.norm1.shift\", \"transformer_blocks.4.norm2.scale\", \"transformer_blocks.4.norm2.shift\", \"transformer_blocks.5.attention.mask\", \"transformer_blocks.5.attention.W_query.weight\", \"transformer_blocks.5.attention.W_query.bias\", \"transformer_blocks.5.attention.W_key.weight\", \"transformer_blocks.5.attention.W_key.bias\", \"transformer_blocks.5.attention.W_value.weight\", \"transformer_blocks.5.attention.W_value.bias\", \"transformer_blocks.5.attention.out_projection.weight\", \"transformer_blocks.5.attention.out_projection.bias\", \"transformer_blocks.5.ffn.layers.0.weight\", \"transformer_blocks.5.ffn.layers.0.bias\", \"transformer_blocks.5.ffn.layers.2.weight\", \"transformer_blocks.5.ffn.layers.2.bias\", \"transformer_blocks.5.norm1.scale\", \"transformer_blocks.5.norm1.shift\", \"transformer_blocks.5.norm2.scale\", \"transformer_blocks.5.norm2.shift\", \"transformer_blocks.6.attention.mask\", \"transformer_blocks.6.attention.W_query.weight\", \"transformer_blocks.6.attention.W_query.bias\", \"transformer_blocks.6.attention.W_key.weight\", \"transformer_blocks.6.attention.W_key.bias\", \"transformer_blocks.6.attention.W_value.weight\", \"transformer_blocks.6.attention.W_value.bias\", \"transformer_blocks.6.attention.out_projection.weight\", \"transformer_blocks.6.attention.out_projection.bias\", \"transformer_blocks.6.ffn.layers.0.weight\", \"transformer_blocks.6.ffn.layers.0.bias\", \"transformer_blocks.6.ffn.layers.2.weight\", \"transformer_blocks.6.ffn.layers.2.bias\", \"transformer_blocks.6.norm1.scale\", \"transformer_blocks.6.norm1.shift\", \"transformer_blocks.6.norm2.scale\", \"transformer_blocks.6.norm2.shift\", \"transformer_blocks.7.attention.mask\", \"transformer_blocks.7.attention.W_query.weight\", \"transformer_blocks.7.attention.W_query.bias\", \"transformer_blocks.7.attention.W_key.weight\", \"transformer_blocks.7.attention.W_key.bias\", \"transformer_blocks.7.attention.W_value.weight\", \"transformer_blocks.7.attention.W_value.bias\", \"transformer_blocks.7.attention.out_projection.weight\", \"transformer_blocks.7.attention.out_projection.bias\", \"transformer_blocks.7.ffn.layers.0.weight\", \"transformer_blocks.7.ffn.layers.0.bias\", \"transformer_blocks.7.ffn.layers.2.weight\", \"transformer_blocks.7.ffn.layers.2.bias\", \"transformer_blocks.7.norm1.scale\", \"transformer_blocks.7.norm1.shift\", \"transformer_blocks.7.norm2.scale\", \"transformer_blocks.7.norm2.shift\", \"transformer_blocks.8.attention.mask\", \"transformer_blocks.8.attention.W_query.weight\", \"transformer_blocks.8.attention.W_query.bias\", \"transformer_blocks.8.attention.W_key.weight\", \"transformer_blocks.8.attention.W_key.bias\", \"transformer_blocks.8.attention.W_value.weight\", \"transformer_blocks.8.attention.W_value.bias\", \"transformer_blocks.8.attention.out_projection.weight\", \"transformer_blocks.8.attention.out_projection.bias\", \"transformer_blocks.8.ffn.layers.0.weight\", \"transformer_blocks.8.ffn.layers.0.bias\", \"transformer_blocks.8.ffn.layers.2.weight\", \"transformer_blocks.8.ffn.layers.2.bias\", \"transformer_blocks.8.norm1.scale\", \"transformer_blocks.8.norm1.shift\", \"transformer_blocks.8.norm2.scale\", \"transformer_blocks.8.norm2.shift\", \"transformer_blocks.9.attention.mask\", \"transformer_blocks.9.attention.W_query.weight\", \"transformer_blocks.9.attention.W_query.bias\", \"transformer_blocks.9.attention.W_key.weight\", \"transformer_blocks.9.attention.W_key.bias\", \"transformer_blocks.9.attention.W_value.weight\", \"transformer_blocks.9.attention.W_value.bias\", \"transformer_blocks.9.attention.out_projection.weight\", \"transformer_blocks.9.attention.out_projection.bias\", \"transformer_blocks.9.ffn.layers.0.weight\", \"transformer_blocks.9.ffn.layers.0.bias\", \"transformer_blocks.9.ffn.layers.2.weight\", \"transformer_blocks.9.ffn.layers.2.bias\", \"transformer_blocks.9.norm1.scale\", \"transformer_blocks.9.norm1.shift\", \"transformer_blocks.9.norm2.scale\", \"transformer_blocks.9.norm2.shift\", \"transformer_blocks.10.attention.mask\", \"transformer_blocks.10.attention.W_query.weight\", \"transformer_blocks.10.attention.W_query.bias\", \"transformer_blocks.10.attention.W_key.weight\", \"transformer_blocks.10.attention.W_key.bias\", \"transformer_blocks.10.attention.W_value.weight\", \"transformer_blocks.10.attention.W_value.bias\", \"transformer_blocks.10.attention.out_projection.weight\", \"transformer_blocks.10.attention.out_projection.bias\", \"transformer_blocks.10.ffn.layers.0.weight\", \"transformer_blocks.10.ffn.layers.0.bias\", \"transformer_blocks.10.ffn.layers.2.weight\", \"transformer_blocks.10.ffn.layers.2.bias\", \"transformer_blocks.10.norm1.scale\", \"transformer_blocks.10.norm1.shift\", \"transformer_blocks.10.norm2.scale\", \"transformer_blocks.10.norm2.shift\", \"transformer_blocks.11.attention.mask\", \"transformer_blocks.11.attention.W_query.weight\", \"transformer_blocks.11.attention.W_query.bias\", \"transformer_blocks.11.attention.W_key.weight\", \"transformer_blocks.11.attention.W_key.bias\", \"transformer_blocks.11.attention.W_value.weight\", \"transformer_blocks.11.attention.W_value.bias\", \"transformer_blocks.11.attention.out_projection.weight\", \"transformer_blocks.11.attention.out_projection.bias\", \"transformer_blocks.11.ffn.layers.0.weight\", \"transformer_blocks.11.ffn.layers.0.bias\", \"transformer_blocks.11.ffn.layers.2.weight\", \"transformer_blocks.11.ffn.layers.2.bias\", \"transformer_blocks.11.norm1.scale\", \"transformer_blocks.11.norm1.shift\", \"transformer_blocks.11.norm2.scale\", \"transformer_blocks.11.norm2.shift\". \n\tUnexpected key(s) in state_dict: \"tok_emb.weight\", \"pos_emb.weight\", \"trf_blocks.0.att.mask\", \"trf_blocks.0.att.W_query.weight\", \"trf_blocks.0.att.W_query.bias\", \"trf_blocks.0.att.W_key.weight\", \"trf_blocks.0.att.W_key.bias\", \"trf_blocks.0.att.W_value.weight\", \"trf_blocks.0.att.W_value.bias\", \"trf_blocks.0.att.out_proj.weight\", \"trf_blocks.0.att.out_proj.bias\", \"trf_blocks.0.ff.layers.0.weight\", \"trf_blocks.0.ff.layers.0.bias\", \"trf_blocks.0.ff.layers.2.weight\", \"trf_blocks.0.ff.layers.2.bias\", \"trf_blocks.0.norm1.scale\", \"trf_blocks.0.norm1.shift\", \"trf_blocks.0.norm2.scale\", \"trf_blocks.0.norm2.shift\", \"trf_blocks.1.att.mask\", \"trf_blocks.1.att.W_query.weight\", \"trf_blocks.1.att.W_query.bias\", \"trf_blocks.1.att.W_key.weight\", \"trf_blocks.1.att.W_key.bias\", \"trf_blocks.1.att.W_value.weight\", \"trf_blocks.1.att.W_value.bias\", \"trf_blocks.1.att.out_proj.weight\", \"trf_blocks.1.att.out_proj.bias\", \"trf_blocks.1.ff.layers.0.weight\", \"trf_blocks.1.ff.layers.0.bias\", \"trf_blocks.1.ff.layers.2.weight\", \"trf_blocks.1.ff.layers.2.bias\", \"trf_blocks.1.norm1.scale\", \"trf_blocks.1.norm1.shift\", \"trf_blocks.1.norm2.scale\", \"trf_blocks.1.norm2.shift\", \"trf_blocks.2.att.mask\", \"trf_blocks.2.att.W_query.weight\", \"trf_blocks.2.att.W_query.bias\", \"trf_blocks.2.att.W_key.weight\", \"trf_blocks.2.att.W_key.bias\", \"trf_blocks.2.att.W_value.weight\", \"trf_blocks.2.att.W_value.bias\", \"trf_blocks.2.att.out_proj.weight\", \"trf_blocks.2.att.out_proj.bias\", \"trf_blocks.2.ff.layers.0.weight\", \"trf_blocks.2.ff.layers.0.bias\", \"trf_blocks.2.ff.layers.2.weight\", \"trf_blocks.2.ff.layers.2.bias\", \"trf_blocks.2.norm1.scale\", \"trf_blocks.2.norm1.shift\", \"trf_blocks.2.norm2.scale\", \"trf_blocks.2.norm2.shift\", \"trf_blocks.3.att.mask\", \"trf_blocks.3.att.W_query.weight\", \"trf_blocks.3.att.W_query.bias\", \"trf_blocks.3.att.W_key.weight\", \"trf_blocks.3.att.W_key.bias\", \"trf_blocks.3.att.W_value.weight\", \"trf_blocks.3.att.W_value.bias\", \"trf_blocks.3.att.out_proj.weight\", \"trf_blocks.3.att.out_proj.bias\", \"trf_blocks.3.ff.layers.0.weight\", \"trf_blocks.3.ff.layers.0.bias\", \"trf_blocks.3.ff.layers.2.weight\", \"trf_blocks.3.ff.layers.2.bias\", \"trf_blocks.3.norm1.scale\", \"trf_blocks.3.norm1.shift\", \"trf_blocks.3.norm2.scale\", \"trf_blocks.3.norm2.shift\", \"trf_blocks.4.att.mask\", \"trf_blocks.4.att.W_query.weight\", \"trf_blocks.4.att.W_query.bias\", \"trf_blocks.4.att.W_key.weight\", \"trf_blocks.4.att.W_key.bias\", \"trf_blocks.4.att.W_value.weight\", \"trf_blocks.4.att.W_value.bias\", \"trf_blocks.4.att.out_proj.weight\", \"trf_blocks.4.att.out_proj.bias\", \"trf_blocks.4.ff.layers.0.weight\", \"trf_blocks.4.ff.layers.0.bias\", \"trf_blocks.4.ff.layers.2.weight\", \"trf_blocks.4.ff.layers.2.bias\", \"trf_blocks.4.norm1.scale\", \"trf_blocks.4.norm1.shift\", \"trf_blocks.4.norm2.scale\", \"trf_blocks.4.norm2.shift\", \"trf_blocks.5.att.mask\", \"trf_blocks.5.att.W_query.weight\", \"trf_blocks.5.att.W_query.bias\", \"trf_blocks.5.att.W_key.weight\", \"trf_blocks.5.att.W_key.bias\", \"trf_blocks.5.att.W_value.weight\", \"trf_blocks.5.att.W_value.bias\", \"trf_blocks.5.att.out_proj.weight\", \"trf_blocks.5.att.out_proj.bias\", \"trf_blocks.5.ff.layers.0.weight\", \"trf_blocks.5.ff.layers.0.bias\", \"trf_blocks.5.ff.layers.2.weight\", \"trf_blocks.5.ff.layers.2.bias\", \"trf_blocks.5.norm1.scale\", \"trf_blocks.5.norm1.shift\", \"trf_blocks.5.norm2.scale\", \"trf_blocks.5.norm2.shift\", \"trf_blocks.6.att.mask\", \"trf_blocks.6.att.W_query.weight\", \"trf_blocks.6.att.W_query.bias\", \"trf_blocks.6.att.W_key.weight\", \"trf_blocks.6.att.W_key.bias\", \"trf_blocks.6.att.W_value.weight\", \"trf_blocks.6.att.W_value.bias\", \"trf_blocks.6.att.out_proj.weight\", \"trf_blocks.6.att.out_proj.bias\", \"trf_blocks.6.ff.layers.0.weight\", \"trf_blocks.6.ff.layers.0.bias\", \"trf_blocks.6.ff.layers.2.weight\", \"trf_blocks.6.ff.layers.2.bias\", \"trf_blocks.6.norm1.scale\", \"trf_blocks.6.norm1.shift\", \"trf_blocks.6.norm2.scale\", \"trf_blocks.6.norm2.shift\", \"trf_blocks.7.att.mask\", \"trf_blocks.7.att.W_query.weight\", \"trf_blocks.7.att.W_query.bias\", \"trf_blocks.7.att.W_key.weight\", \"trf_blocks.7.att.W_key.bias\", \"trf_blocks.7.att.W_value.weight\", \"trf_blocks.7.att.W_value.bias\", \"trf_blocks.7.att.out_proj.weight\", \"trf_blocks.7.att.out_proj.bias\", \"trf_blocks.7.ff.layers.0.weight\", \"trf_blocks.7.ff.layers.0.bias\", \"trf_blocks.7.ff.layers.2.weight\", \"trf_blocks.7.ff.layers.2.bias\", \"trf_blocks.7.norm1.scale\", \"trf_blocks.7.norm1.shift\", \"trf_blocks.7.norm2.scale\", \"trf_blocks.7.norm2.shift\", \"trf_blocks.8.att.mask\", \"trf_blocks.8.att.W_query.weight\", \"trf_blocks.8.att.W_query.bias\", \"trf_blocks.8.att.W_key.weight\", \"trf_blocks.8.att.W_key.bias\", \"trf_blocks.8.att.W_value.weight\", \"trf_blocks.8.att.W_value.bias\", \"trf_blocks.8.att.out_proj.weight\", \"trf_blocks.8.att.out_proj.bias\", \"trf_blocks.8.ff.layers.0.weight\", \"trf_blocks.8.ff.layers.0.bias\", \"trf_blocks.8.ff.layers.2.weight\", \"trf_blocks.8.ff.layers.2.bias\", \"trf_blocks.8.norm1.scale\", \"trf_blocks.8.norm1.shift\", \"trf_blocks.8.norm2.scale\", \"trf_blocks.8.norm2.shift\", \"trf_blocks.9.att.mask\", \"trf_blocks.9.att.W_query.weight\", \"trf_blocks.9.att.W_query.bias\", \"trf_blocks.9.att.W_key.weight\", \"trf_blocks.9.att.W_key.bias\", \"trf_blocks.9.att.W_value.weight\", \"trf_blocks.9.att.W_value.bias\", \"trf_blocks.9.att.out_proj.weight\", \"trf_blocks.9.att.out_proj.bias\", \"trf_blocks.9.ff.layers.0.weight\", \"trf_blocks.9.ff.layers.0.bias\", \"trf_blocks.9.ff.layers.2.weight\", \"trf_blocks.9.ff.layers.2.bias\", \"trf_blocks.9.norm1.scale\", \"trf_blocks.9.norm1.shift\", \"trf_blocks.9.norm2.scale\", \"trf_blocks.9.norm2.shift\", \"trf_blocks.10.att.mask\", \"trf_blocks.10.att.W_query.weight\", \"trf_blocks.10.att.W_query.bias\", \"trf_blocks.10.att.W_key.weight\", \"trf_blocks.10.att.W_key.bias\", \"trf_blocks.10.att.W_value.weight\", \"trf_blocks.10.att.W_value.bias\", \"trf_blocks.10.att.out_proj.weight\", \"trf_blocks.10.att.out_proj.bias\", \"trf_blocks.10.ff.layers.0.weight\", \"trf_blocks.10.ff.layers.0.bias\", \"trf_blocks.10.ff.layers.2.weight\", \"trf_blocks.10.ff.layers.2.bias\", \"trf_blocks.10.norm1.scale\", \"trf_blocks.10.norm1.shift\", \"trf_blocks.10.norm2.scale\", \"trf_blocks.10.norm2.shift\", \"trf_blocks.11.att.mask\", \"trf_blocks.11.att.W_query.weight\", \"trf_blocks.11.att.W_query.bias\", \"trf_blocks.11.att.W_key.weight\", \"trf_blocks.11.att.W_key.bias\", \"trf_blocks.11.att.W_value.weight\", \"trf_blocks.11.att.W_value.bias\", \"trf_blocks.11.att.out_proj.weight\", \"trf_blocks.11.att.out_proj.bias\", \"trf_blocks.11.ff.layers.0.weight\", \"trf_blocks.11.ff.layers.0.bias\", \"trf_blocks.11.ff.layers.2.weight\", \"trf_blocks.11.ff.layers.2.bias\", \"trf_blocks.11.norm1.scale\", \"trf_blocks.11.norm1.shift\", \"trf_blocks.11.norm2.scale\", \"trf_blocks.11.norm2.shift\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_05_previous_modules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPTModel\n\u001b[32m      3\u001b[39m gpt = GPTModel(BASE_CONFIG)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mgpt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m gpt.eval()\n\u001b[32m      7\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\skdbs\\.conda\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2215\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2210\u001b[39m         error_msgs.insert(\n\u001b[32m   2211\u001b[39m             \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m'\u001b[39m.format(\n\u001b[32m   2212\u001b[39m                 \u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[32m   2214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2215\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   2216\u001b[39m                        \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)))\n\u001b[32m   2217\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for GPTModel:\n\tMissing key(s) in state_dict: \"token_embedding.weight\", \"position_embedding.weight\", \"transformer_blocks.0.attention.mask\", \"transformer_blocks.0.attention.W_query.weight\", \"transformer_blocks.0.attention.W_query.bias\", \"transformer_blocks.0.attention.W_key.weight\", \"transformer_blocks.0.attention.W_key.bias\", \"transformer_blocks.0.attention.W_value.weight\", \"transformer_blocks.0.attention.W_value.bias\", \"transformer_blocks.0.attention.out_projection.weight\", \"transformer_blocks.0.attention.out_projection.bias\", \"transformer_blocks.0.ffn.layers.0.weight\", \"transformer_blocks.0.ffn.layers.0.bias\", \"transformer_blocks.0.ffn.layers.2.weight\", \"transformer_blocks.0.ffn.layers.2.bias\", \"transformer_blocks.0.norm1.scale\", \"transformer_blocks.0.norm1.shift\", \"transformer_blocks.0.norm2.scale\", \"transformer_blocks.0.norm2.shift\", \"transformer_blocks.1.attention.mask\", \"transformer_blocks.1.attention.W_query.weight\", \"transformer_blocks.1.attention.W_query.bias\", \"transformer_blocks.1.attention.W_key.weight\", \"transformer_blocks.1.attention.W_key.bias\", \"transformer_blocks.1.attention.W_value.weight\", \"transformer_blocks.1.attention.W_value.bias\", \"transformer_blocks.1.attention.out_projection.weight\", \"transformer_blocks.1.attention.out_projection.bias\", \"transformer_blocks.1.ffn.layers.0.weight\", \"transformer_blocks.1.ffn.layers.0.bias\", \"transformer_blocks.1.ffn.layers.2.weight\", \"transformer_blocks.1.ffn.layers.2.bias\", \"transformer_blocks.1.norm1.scale\", \"transformer_blocks.1.norm1.shift\", \"transformer_blocks.1.norm2.scale\", \"transformer_blocks.1.norm2.shift\", \"transformer_blocks.2.attention.mask\", \"transformer_blocks.2.attention.W_query.weight\", \"transformer_blocks.2.attention.W_query.bias\", \"transformer_blocks.2.attention.W_key.weight\", \"transformer_blocks.2.attention.W_key.bias\", \"transformer_blocks.2.attention.W_value.weight\", \"transformer_blocks.2.attention.W_value.bias\", \"transformer_blocks.2.attention.out_projection.weight\", \"transformer_blocks.2.attention.out_projection.bias\", \"transformer_blocks.2.ffn.layers.0.weight\", \"transformer_blocks.2.ffn.layers.0.bias\", \"transformer_blocks.2.ffn.layers.2.weight\", \"transformer_blocks.2.ffn.layers.2.bias\", \"transformer_blocks.2.norm1.scale\", \"transformer_blocks.2.norm1.shift\", \"transformer_blocks.2.norm2.scale\", \"transformer_blocks.2.norm2.shift\", \"transformer_blocks.3.attention.mask\", \"transformer_blocks.3.attention.W_query.weight\", \"transformer_blocks.3.attention.W_query.bias\", \"transformer_blocks.3.attention.W_key.weight\", \"transformer_blocks.3.attention.W_key.bias\", \"transformer_blocks.3.attention.W_value.weight\", \"transformer_blocks.3.attention.W_value.bias\", \"transformer_blocks.3.attention.out_projection.weight\", \"transformer_blocks.3.attention.out_projection.bias\", \"transformer_blocks.3.ffn.layers.0.weight\", \"transformer_blocks.3.ffn.layers.0.bias\", \"transformer_blocks.3.ffn.layers.2.weight\", \"transformer_blocks.3.ffn.layers.2.bias\", \"transformer_blocks.3.norm1.scale\", \"transformer_blocks.3.norm1.shift\", \"transformer_blocks.3.norm2.scale\", \"transformer_blocks.3.norm2.shift\", \"transformer_blocks.4.attention.mask\", \"transformer_blocks.4.attention.W_query.weight\", \"transformer_blocks.4.attention.W_query.bias\", \"transformer_blocks.4.attention.W_key.weight\", \"transformer_blocks.4.attention.W_key.bias\", \"transformer_blocks.4.attention.W_value.weight\", \"transformer_blocks.4.attention.W_value.bias\", \"transformer_blocks.4.attention.out_projection.weight\", \"transformer_blocks.4.attention.out_projection.bias\", \"transformer_blocks.4.ffn.layers.0.weight\", \"transformer_blocks.4.ffn.layers.0.bias\", \"transformer_blocks.4.ffn.layers.2.weight\", \"transformer_blocks.4.ffn.layers.2.bias\", \"transformer_blocks.4.norm1.scale\", \"transformer_blocks.4.norm1.shift\", \"transformer_blocks.4.norm2.scale\", \"transformer_blocks.4.norm2.shift\", \"transformer_blocks.5.attention.mask\", \"transformer_blocks.5.attention.W_query.weight\", \"transformer_blocks.5.attention.W_query.bias\", \"transformer_blocks.5.attention.W_key.weight\", \"transformer_blocks.5.attention.W_key.bias\", \"transformer_blocks.5.attention.W_value.weight\", \"transformer_blocks.5.attention.W_value.bias\", \"transformer_blocks.5.attention.out_projection.weight\", \"transformer_blocks.5.attention.out_projection.bias\", \"transformer_blocks.5.ffn.layers.0.weight\", \"transformer_blocks.5.ffn.layers.0.bias\", \"transformer_blocks.5.ffn.layers.2.weight\", \"transformer_blocks.5.ffn.layers.2.bias\", \"transformer_blocks.5.norm1.scale\", \"transformer_blocks.5.norm1.shift\", \"transformer_blocks.5.norm2.scale\", \"transformer_blocks.5.norm2.shift\", \"transformer_blocks.6.attention.mask\", \"transformer_blocks.6.attention.W_query.weight\", \"transformer_blocks.6.attention.W_query.bias\", \"transformer_blocks.6.attention.W_key.weight\", \"transformer_blocks.6.attention.W_key.bias\", \"transformer_blocks.6.attention.W_value.weight\", \"transformer_blocks.6.attention.W_value.bias\", \"transformer_blocks.6.attention.out_projection.weight\", \"transformer_blocks.6.attention.out_projection.bias\", \"transformer_blocks.6.ffn.layers.0.weight\", \"transformer_blocks.6.ffn.layers.0.bias\", \"transformer_blocks.6.ffn.layers.2.weight\", \"transformer_blocks.6.ffn.layers.2.bias\", \"transformer_blocks.6.norm1.scale\", \"transformer_blocks.6.norm1.shift\", \"transformer_blocks.6.norm2.scale\", \"transformer_blocks.6.norm2.shift\", \"transformer_blocks.7.attention.mask\", \"transformer_blocks.7.attention.W_query.weight\", \"transformer_blocks.7.attention.W_query.bias\", \"transformer_blocks.7.attention.W_key.weight\", \"transformer_blocks.7.attention.W_key.bias\", \"transformer_blocks.7.attention.W_value.weight\", \"transformer_blocks.7.attention.W_value.bias\", \"transformer_blocks.7.attention.out_projection.weight\", \"transformer_blocks.7.attention.out_projection.bias\", \"transformer_blocks.7.ffn.layers.0.weight\", \"transformer_blocks.7.ffn.layers.0.bias\", \"transformer_blocks.7.ffn.layers.2.weight\", \"transformer_blocks.7.ffn.layers.2.bias\", \"transformer_blocks.7.norm1.scale\", \"transformer_blocks.7.norm1.shift\", \"transformer_blocks.7.norm2.scale\", \"transformer_blocks.7.norm2.shift\", \"transformer_blocks.8.attention.mask\", \"transformer_blocks.8.attention.W_query.weight\", \"transformer_blocks.8.attention.W_query.bias\", \"transformer_blocks.8.attention.W_key.weight\", \"transformer_blocks.8.attention.W_key.bias\", \"transformer_blocks.8.attention.W_value.weight\", \"transformer_blocks.8.attention.W_value.bias\", \"transformer_blocks.8.attention.out_projection.weight\", \"transformer_blocks.8.attention.out_projection.bias\", \"transformer_blocks.8.ffn.layers.0.weight\", \"transformer_blocks.8.ffn.layers.0.bias\", \"transformer_blocks.8.ffn.layers.2.weight\", \"transformer_blocks.8.ffn.layers.2.bias\", \"transformer_blocks.8.norm1.scale\", \"transformer_blocks.8.norm1.shift\", \"transformer_blocks.8.norm2.scale\", \"transformer_blocks.8.norm2.shift\", \"transformer_blocks.9.attention.mask\", \"transformer_blocks.9.attention.W_query.weight\", \"transformer_blocks.9.attention.W_query.bias\", \"transformer_blocks.9.attention.W_key.weight\", \"transformer_blocks.9.attention.W_key.bias\", \"transformer_blocks.9.attention.W_value.weight\", \"transformer_blocks.9.attention.W_value.bias\", \"transformer_blocks.9.attention.out_projection.weight\", \"transformer_blocks.9.attention.out_projection.bias\", \"transformer_blocks.9.ffn.layers.0.weight\", \"transformer_blocks.9.ffn.layers.0.bias\", \"transformer_blocks.9.ffn.layers.2.weight\", \"transformer_blocks.9.ffn.layers.2.bias\", \"transformer_blocks.9.norm1.scale\", \"transformer_blocks.9.norm1.shift\", \"transformer_blocks.9.norm2.scale\", \"transformer_blocks.9.norm2.shift\", \"transformer_blocks.10.attention.mask\", \"transformer_blocks.10.attention.W_query.weight\", \"transformer_blocks.10.attention.W_query.bias\", \"transformer_blocks.10.attention.W_key.weight\", \"transformer_blocks.10.attention.W_key.bias\", \"transformer_blocks.10.attention.W_value.weight\", \"transformer_blocks.10.attention.W_value.bias\", \"transformer_blocks.10.attention.out_projection.weight\", \"transformer_blocks.10.attention.out_projection.bias\", \"transformer_blocks.10.ffn.layers.0.weight\", \"transformer_blocks.10.ffn.layers.0.bias\", \"transformer_blocks.10.ffn.layers.2.weight\", \"transformer_blocks.10.ffn.layers.2.bias\", \"transformer_blocks.10.norm1.scale\", \"transformer_blocks.10.norm1.shift\", \"transformer_blocks.10.norm2.scale\", \"transformer_blocks.10.norm2.shift\", \"transformer_blocks.11.attention.mask\", \"transformer_blocks.11.attention.W_query.weight\", \"transformer_blocks.11.attention.W_query.bias\", \"transformer_blocks.11.attention.W_key.weight\", \"transformer_blocks.11.attention.W_key.bias\", \"transformer_blocks.11.attention.W_value.weight\", \"transformer_blocks.11.attention.W_value.bias\", \"transformer_blocks.11.attention.out_projection.weight\", \"transformer_blocks.11.attention.out_projection.bias\", \"transformer_blocks.11.ffn.layers.0.weight\", \"transformer_blocks.11.ffn.layers.0.bias\", \"transformer_blocks.11.ffn.layers.2.weight\", \"transformer_blocks.11.ffn.layers.2.bias\", \"transformer_blocks.11.norm1.scale\", \"transformer_blocks.11.norm1.shift\", \"transformer_blocks.11.norm2.scale\", \"transformer_blocks.11.norm2.shift\". \n\tUnexpected key(s) in state_dict: \"tok_emb.weight\", \"pos_emb.weight\", \"trf_blocks.0.att.mask\", \"trf_blocks.0.att.W_query.weight\", \"trf_blocks.0.att.W_query.bias\", \"trf_blocks.0.att.W_key.weight\", \"trf_blocks.0.att.W_key.bias\", \"trf_blocks.0.att.W_value.weight\", \"trf_blocks.0.att.W_value.bias\", \"trf_blocks.0.att.out_proj.weight\", \"trf_blocks.0.att.out_proj.bias\", \"trf_blocks.0.ff.layers.0.weight\", \"trf_blocks.0.ff.layers.0.bias\", \"trf_blocks.0.ff.layers.2.weight\", \"trf_blocks.0.ff.layers.2.bias\", \"trf_blocks.0.norm1.scale\", \"trf_blocks.0.norm1.shift\", \"trf_blocks.0.norm2.scale\", \"trf_blocks.0.norm2.shift\", \"trf_blocks.1.att.mask\", \"trf_blocks.1.att.W_query.weight\", \"trf_blocks.1.att.W_query.bias\", \"trf_blocks.1.att.W_key.weight\", \"trf_blocks.1.att.W_key.bias\", \"trf_blocks.1.att.W_value.weight\", \"trf_blocks.1.att.W_value.bias\", \"trf_blocks.1.att.out_proj.weight\", \"trf_blocks.1.att.out_proj.bias\", \"trf_blocks.1.ff.layers.0.weight\", \"trf_blocks.1.ff.layers.0.bias\", \"trf_blocks.1.ff.layers.2.weight\", \"trf_blocks.1.ff.layers.2.bias\", \"trf_blocks.1.norm1.scale\", \"trf_blocks.1.norm1.shift\", \"trf_blocks.1.norm2.scale\", \"trf_blocks.1.norm2.shift\", \"trf_blocks.2.att.mask\", \"trf_blocks.2.att.W_query.weight\", \"trf_blocks.2.att.W_query.bias\", \"trf_blocks.2.att.W_key.weight\", \"trf_blocks.2.att.W_key.bias\", \"trf_blocks.2.att.W_value.weight\", \"trf_blocks.2.att.W_value.bias\", \"trf_blocks.2.att.out_proj.weight\", \"trf_blocks.2.att.out_proj.bias\", \"trf_blocks.2.ff.layers.0.weight\", \"trf_blocks.2.ff.layers.0.bias\", \"trf_blocks.2.ff.layers.2.weight\", \"trf_blocks.2.ff.layers.2.bias\", \"trf_blocks.2.norm1.scale\", \"trf_blocks.2.norm1.shift\", \"trf_blocks.2.norm2.scale\", \"trf_blocks.2.norm2.shift\", \"trf_blocks.3.att.mask\", \"trf_blocks.3.att.W_query.weight\", \"trf_blocks.3.att.W_query.bias\", \"trf_blocks.3.att.W_key.weight\", \"trf_blocks.3.att.W_key.bias\", \"trf_blocks.3.att.W_value.weight\", \"trf_blocks.3.att.W_value.bias\", \"trf_blocks.3.att.out_proj.weight\", \"trf_blocks.3.att.out_proj.bias\", \"trf_blocks.3.ff.layers.0.weight\", \"trf_blocks.3.ff.layers.0.bias\", \"trf_blocks.3.ff.layers.2.weight\", \"trf_blocks.3.ff.layers.2.bias\", \"trf_blocks.3.norm1.scale\", \"trf_blocks.3.norm1.shift\", \"trf_blocks.3.norm2.scale\", \"trf_blocks.3.norm2.shift\", \"trf_blocks.4.att.mask\", \"trf_blocks.4.att.W_query.weight\", \"trf_blocks.4.att.W_query.bias\", \"trf_blocks.4.att.W_key.weight\", \"trf_blocks.4.att.W_key.bias\", \"trf_blocks.4.att.W_value.weight\", \"trf_blocks.4.att.W_value.bias\", \"trf_blocks.4.att.out_proj.weight\", \"trf_blocks.4.att.out_proj.bias\", \"trf_blocks.4.ff.layers.0.weight\", \"trf_blocks.4.ff.layers.0.bias\", \"trf_blocks.4.ff.layers.2.weight\", \"trf_blocks.4.ff.layers.2.bias\", \"trf_blocks.4.norm1.scale\", \"trf_blocks.4.norm1.shift\", \"trf_blocks.4.norm2.scale\", \"trf_blocks.4.norm2.shift\", \"trf_blocks.5.att.mask\", \"trf_blocks.5.att.W_query.weight\", \"trf_blocks.5.att.W_query.bias\", \"trf_blocks.5.att.W_key.weight\", \"trf_blocks.5.att.W_key.bias\", \"trf_blocks.5.att.W_value.weight\", \"trf_blocks.5.att.W_value.bias\", \"trf_blocks.5.att.out_proj.weight\", \"trf_blocks.5.att.out_proj.bias\", \"trf_blocks.5.ff.layers.0.weight\", \"trf_blocks.5.ff.layers.0.bias\", \"trf_blocks.5.ff.layers.2.weight\", \"trf_blocks.5.ff.layers.2.bias\", \"trf_blocks.5.norm1.scale\", \"trf_blocks.5.norm1.shift\", \"trf_blocks.5.norm2.scale\", \"trf_blocks.5.norm2.shift\", \"trf_blocks.6.att.mask\", \"trf_blocks.6.att.W_query.weight\", \"trf_blocks.6.att.W_query.bias\", \"trf_blocks.6.att.W_key.weight\", \"trf_blocks.6.att.W_key.bias\", \"trf_blocks.6.att.W_value.weight\", \"trf_blocks.6.att.W_value.bias\", \"trf_blocks.6.att.out_proj.weight\", \"trf_blocks.6.att.out_proj.bias\", \"trf_blocks.6.ff.layers.0.weight\", \"trf_blocks.6.ff.layers.0.bias\", \"trf_blocks.6.ff.layers.2.weight\", \"trf_blocks.6.ff.layers.2.bias\", \"trf_blocks.6.norm1.scale\", \"trf_blocks.6.norm1.shift\", \"trf_blocks.6.norm2.scale\", \"trf_blocks.6.norm2.shift\", \"trf_blocks.7.att.mask\", \"trf_blocks.7.att.W_query.weight\", \"trf_blocks.7.att.W_query.bias\", \"trf_blocks.7.att.W_key.weight\", \"trf_blocks.7.att.W_key.bias\", \"trf_blocks.7.att.W_value.weight\", \"trf_blocks.7.att.W_value.bias\", \"trf_blocks.7.att.out_proj.weight\", \"trf_blocks.7.att.out_proj.bias\", \"trf_blocks.7.ff.layers.0.weight\", \"trf_blocks.7.ff.layers.0.bias\", \"trf_blocks.7.ff.layers.2.weight\", \"trf_blocks.7.ff.layers.2.bias\", \"trf_blocks.7.norm1.scale\", \"trf_blocks.7.norm1.shift\", \"trf_blocks.7.norm2.scale\", \"trf_blocks.7.norm2.shift\", \"trf_blocks.8.att.mask\", \"trf_blocks.8.att.W_query.weight\", \"trf_blocks.8.att.W_query.bias\", \"trf_blocks.8.att.W_key.weight\", \"trf_blocks.8.att.W_key.bias\", \"trf_blocks.8.att.W_value.weight\", \"trf_blocks.8.att.W_value.bias\", \"trf_blocks.8.att.out_proj.weight\", \"trf_blocks.8.att.out_proj.bias\", \"trf_blocks.8.ff.layers.0.weight\", \"trf_blocks.8.ff.layers.0.bias\", \"trf_blocks.8.ff.layers.2.weight\", \"trf_blocks.8.ff.layers.2.bias\", \"trf_blocks.8.norm1.scale\", \"trf_blocks.8.norm1.shift\", \"trf_blocks.8.norm2.scale\", \"trf_blocks.8.norm2.shift\", \"trf_blocks.9.att.mask\", \"trf_blocks.9.att.W_query.weight\", \"trf_blocks.9.att.W_query.bias\", \"trf_blocks.9.att.W_key.weight\", \"trf_blocks.9.att.W_key.bias\", \"trf_blocks.9.att.W_value.weight\", \"trf_blocks.9.att.W_value.bias\", \"trf_blocks.9.att.out_proj.weight\", \"trf_blocks.9.att.out_proj.bias\", \"trf_blocks.9.ff.layers.0.weight\", \"trf_blocks.9.ff.layers.0.bias\", \"trf_blocks.9.ff.layers.2.weight\", \"trf_blocks.9.ff.layers.2.bias\", \"trf_blocks.9.norm1.scale\", \"trf_blocks.9.norm1.shift\", \"trf_blocks.9.norm2.scale\", \"trf_blocks.9.norm2.shift\", \"trf_blocks.10.att.mask\", \"trf_blocks.10.att.W_query.weight\", \"trf_blocks.10.att.W_query.bias\", \"trf_blocks.10.att.W_key.weight\", \"trf_blocks.10.att.W_key.bias\", \"trf_blocks.10.att.W_value.weight\", \"trf_blocks.10.att.W_value.bias\", \"trf_blocks.10.att.out_proj.weight\", \"trf_blocks.10.att.out_proj.bias\", \"trf_blocks.10.ff.layers.0.weight\", \"trf_blocks.10.ff.layers.0.bias\", \"trf_blocks.10.ff.layers.2.weight\", \"trf_blocks.10.ff.layers.2.bias\", \"trf_blocks.10.norm1.scale\", \"trf_blocks.10.norm1.shift\", \"trf_blocks.10.norm2.scale\", \"trf_blocks.10.norm2.shift\", \"trf_blocks.11.att.mask\", \"trf_blocks.11.att.W_query.weight\", \"trf_blocks.11.att.W_query.bias\", \"trf_blocks.11.att.W_key.weight\", \"trf_blocks.11.att.W_key.bias\", \"trf_blocks.11.att.W_value.weight\", \"trf_blocks.11.att.W_value.bias\", \"trf_blocks.11.att.out_proj.weight\", \"trf_blocks.11.att.out_proj.bias\", \"trf_blocks.11.ff.layers.0.weight\", \"trf_blocks.11.ff.layers.0.bias\", \"trf_blocks.11.ff.layers.2.weight\", \"trf_blocks.11.ff.layers.2.bias\", \"trf_blocks.11.norm1.scale\", \"trf_blocks.11.norm1.shift\", \"trf_blocks.11.norm2.scale\", \"trf_blocks.11.norm2.shift\". "
     ]
    }
   ],
   "source": [
    "from _05_previous_modules import GPTModel\n",
    "\n",
    "gpt = GPTModel(BASE_CONFIG)\n",
    "gpt.load_state_dict(torch.load(file_name, weights_only=True))\n",
    "gpt.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb0cd8",
   "metadata": {},
   "source": [
    "- 공식 example code랑 현재 작성한 model간의 인자 naming 차이 때문에 loading이 안된다 ㄱ-\n",
    "  - Gemini의 도움을 받아 key mapping 하고 load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9d2c1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['token_embedding.weight', 'position_embedding.weight', 'transformer_blocks.0.attention.mask', 'transformer_blocks.0.attention.W_query.weight', 'transformer_blocks.0.attention.W_key.weight', 'transformer_blocks.0.attention.W_value.weight', 'transformer_blocks.0.attention.out_projection.weight', 'transformer_blocks.0.attention.out_projection.bias', 'transformer_blocks.0.ffn.layers.0.weight', 'transformer_blocks.0.ffn.layers.0.bias', 'transformer_blocks.0.ffn.layers.2.weight', 'transformer_blocks.0.ffn.layers.2.bias', 'transformer_blocks.0.norm1.scale', 'transformer_blocks.0.norm1.shift', 'transformer_blocks.0.norm2.scale', 'transformer_blocks.0.norm2.shift', 'transformer_blocks.1.attention.mask', 'transformer_blocks.1.attention.W_query.weight', 'transformer_blocks.1.attention.W_key.weight', 'transformer_blocks.1.attention.W_value.weight', 'transformer_blocks.1.attention.out_projection.weight', 'transformer_blocks.1.attention.out_projection.bias', 'transformer_blocks.1.ffn.layers.0.weight', 'transformer_blocks.1.ffn.layers.0.bias', 'transformer_blocks.1.ffn.layers.2.weight', 'transformer_blocks.1.ffn.layers.2.bias', 'transformer_blocks.1.norm1.scale', 'transformer_blocks.1.norm1.shift', 'transformer_blocks.1.norm2.scale', 'transformer_blocks.1.norm2.shift', 'transformer_blocks.2.attention.mask', 'transformer_blocks.2.attention.W_query.weight', 'transformer_blocks.2.attention.W_key.weight', 'transformer_blocks.2.attention.W_value.weight', 'transformer_blocks.2.attention.out_projection.weight', 'transformer_blocks.2.attention.out_projection.bias', 'transformer_blocks.2.ffn.layers.0.weight', 'transformer_blocks.2.ffn.layers.0.bias', 'transformer_blocks.2.ffn.layers.2.weight', 'transformer_blocks.2.ffn.layers.2.bias', 'transformer_blocks.2.norm1.scale', 'transformer_blocks.2.norm1.shift', 'transformer_blocks.2.norm2.scale', 'transformer_blocks.2.norm2.shift', 'transformer_blocks.3.attention.mask', 'transformer_blocks.3.attention.W_query.weight', 'transformer_blocks.3.attention.W_key.weight', 'transformer_blocks.3.attention.W_value.weight', 'transformer_blocks.3.attention.out_projection.weight', 'transformer_blocks.3.attention.out_projection.bias', 'transformer_blocks.3.ffn.layers.0.weight', 'transformer_blocks.3.ffn.layers.0.bias', 'transformer_blocks.3.ffn.layers.2.weight', 'transformer_blocks.3.ffn.layers.2.bias', 'transformer_blocks.3.norm1.scale', 'transformer_blocks.3.norm1.shift', 'transformer_blocks.3.norm2.scale', 'transformer_blocks.3.norm2.shift', 'transformer_blocks.4.attention.mask', 'transformer_blocks.4.attention.W_query.weight', 'transformer_blocks.4.attention.W_key.weight', 'transformer_blocks.4.attention.W_value.weight', 'transformer_blocks.4.attention.out_projection.weight', 'transformer_blocks.4.attention.out_projection.bias', 'transformer_blocks.4.ffn.layers.0.weight', 'transformer_blocks.4.ffn.layers.0.bias', 'transformer_blocks.4.ffn.layers.2.weight', 'transformer_blocks.4.ffn.layers.2.bias', 'transformer_blocks.4.norm1.scale', 'transformer_blocks.4.norm1.shift', 'transformer_blocks.4.norm2.scale', 'transformer_blocks.4.norm2.shift', 'transformer_blocks.5.attention.mask', 'transformer_blocks.5.attention.W_query.weight', 'transformer_blocks.5.attention.W_key.weight', 'transformer_blocks.5.attention.W_value.weight', 'transformer_blocks.5.attention.out_projection.weight', 'transformer_blocks.5.attention.out_projection.bias', 'transformer_blocks.5.ffn.layers.0.weight', 'transformer_blocks.5.ffn.layers.0.bias', 'transformer_blocks.5.ffn.layers.2.weight', 'transformer_blocks.5.ffn.layers.2.bias', 'transformer_blocks.5.norm1.scale', 'transformer_blocks.5.norm1.shift', 'transformer_blocks.5.norm2.scale', 'transformer_blocks.5.norm2.shift', 'transformer_blocks.6.attention.mask', 'transformer_blocks.6.attention.W_query.weight', 'transformer_blocks.6.attention.W_key.weight', 'transformer_blocks.6.attention.W_value.weight', 'transformer_blocks.6.attention.out_projection.weight', 'transformer_blocks.6.attention.out_projection.bias', 'transformer_blocks.6.ffn.layers.0.weight', 'transformer_blocks.6.ffn.layers.0.bias', 'transformer_blocks.6.ffn.layers.2.weight', 'transformer_blocks.6.ffn.layers.2.bias', 'transformer_blocks.6.norm1.scale', 'transformer_blocks.6.norm1.shift', 'transformer_blocks.6.norm2.scale', 'transformer_blocks.6.norm2.shift', 'transformer_blocks.7.attention.mask', 'transformer_blocks.7.attention.W_query.weight', 'transformer_blocks.7.attention.W_key.weight', 'transformer_blocks.7.attention.W_value.weight', 'transformer_blocks.7.attention.out_projection.weight', 'transformer_blocks.7.attention.out_projection.bias', 'transformer_blocks.7.ffn.layers.0.weight', 'transformer_blocks.7.ffn.layers.0.bias', 'transformer_blocks.7.ffn.layers.2.weight', 'transformer_blocks.7.ffn.layers.2.bias', 'transformer_blocks.7.norm1.scale', 'transformer_blocks.7.norm1.shift', 'transformer_blocks.7.norm2.scale', 'transformer_blocks.7.norm2.shift', 'transformer_blocks.8.attention.mask', 'transformer_blocks.8.attention.W_query.weight', 'transformer_blocks.8.attention.W_key.weight', 'transformer_blocks.8.attention.W_value.weight', 'transformer_blocks.8.attention.out_projection.weight', 'transformer_blocks.8.attention.out_projection.bias', 'transformer_blocks.8.ffn.layers.0.weight', 'transformer_blocks.8.ffn.layers.0.bias', 'transformer_blocks.8.ffn.layers.2.weight', 'transformer_blocks.8.ffn.layers.2.bias', 'transformer_blocks.8.norm1.scale', 'transformer_blocks.8.norm1.shift', 'transformer_blocks.8.norm2.scale', 'transformer_blocks.8.norm2.shift', 'transformer_blocks.9.attention.mask', 'transformer_blocks.9.attention.W_query.weight', 'transformer_blocks.9.attention.W_key.weight', 'transformer_blocks.9.attention.W_value.weight', 'transformer_blocks.9.attention.out_projection.weight', 'transformer_blocks.9.attention.out_projection.bias', 'transformer_blocks.9.ffn.layers.0.weight', 'transformer_blocks.9.ffn.layers.0.bias', 'transformer_blocks.9.ffn.layers.2.weight', 'transformer_blocks.9.ffn.layers.2.bias', 'transformer_blocks.9.norm1.scale', 'transformer_blocks.9.norm1.shift', 'transformer_blocks.9.norm2.scale', 'transformer_blocks.9.norm2.shift', 'transformer_blocks.10.attention.mask', 'transformer_blocks.10.attention.W_query.weight', 'transformer_blocks.10.attention.W_key.weight', 'transformer_blocks.10.attention.W_value.weight', 'transformer_blocks.10.attention.out_projection.weight', 'transformer_blocks.10.attention.out_projection.bias', 'transformer_blocks.10.ffn.layers.0.weight', 'transformer_blocks.10.ffn.layers.0.bias', 'transformer_blocks.10.ffn.layers.2.weight', 'transformer_blocks.10.ffn.layers.2.bias', 'transformer_blocks.10.norm1.scale', 'transformer_blocks.10.norm1.shift', 'transformer_blocks.10.norm2.scale', 'transformer_blocks.10.norm2.shift', 'transformer_blocks.11.attention.mask', 'transformer_blocks.11.attention.W_query.weight', 'transformer_blocks.11.attention.W_key.weight', 'transformer_blocks.11.attention.W_value.weight', 'transformer_blocks.11.attention.out_projection.weight', 'transformer_blocks.11.attention.out_projection.bias', 'transformer_blocks.11.ffn.layers.0.weight', 'transformer_blocks.11.ffn.layers.0.bias', 'transformer_blocks.11.ffn.layers.2.weight', 'transformer_blocks.11.ffn.layers.2.bias', 'transformer_blocks.11.norm1.scale', 'transformer_blocks.11.norm1.shift', 'transformer_blocks.11.norm2.scale', 'transformer_blocks.11.norm2.shift', 'final_norm.scale', 'final_norm.shift', 'out_head.weight'])\n",
      "=====================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_1568\\1162063402.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"gpt2-small-124M.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['tok_emb.weight', 'pos_emb.weight', 'trf_blocks.0.att.mask', 'trf_blocks.0.att.W_query.weight', 'trf_blocks.0.att.W_query.bias', 'trf_blocks.0.att.W_key.weight', 'trf_blocks.0.att.W_key.bias', 'trf_blocks.0.att.W_value.weight', 'trf_blocks.0.att.W_value.bias', 'trf_blocks.0.att.out_proj.weight', 'trf_blocks.0.att.out_proj.bias', 'trf_blocks.0.ff.layers.0.weight', 'trf_blocks.0.ff.layers.0.bias', 'trf_blocks.0.ff.layers.2.weight', 'trf_blocks.0.ff.layers.2.bias', 'trf_blocks.0.norm1.scale', 'trf_blocks.0.norm1.shift', 'trf_blocks.0.norm2.scale', 'trf_blocks.0.norm2.shift', 'trf_blocks.1.att.mask', 'trf_blocks.1.att.W_query.weight', 'trf_blocks.1.att.W_query.bias', 'trf_blocks.1.att.W_key.weight', 'trf_blocks.1.att.W_key.bias', 'trf_blocks.1.att.W_value.weight', 'trf_blocks.1.att.W_value.bias', 'trf_blocks.1.att.out_proj.weight', 'trf_blocks.1.att.out_proj.bias', 'trf_blocks.1.ff.layers.0.weight', 'trf_blocks.1.ff.layers.0.bias', 'trf_blocks.1.ff.layers.2.weight', 'trf_blocks.1.ff.layers.2.bias', 'trf_blocks.1.norm1.scale', 'trf_blocks.1.norm1.shift', 'trf_blocks.1.norm2.scale', 'trf_blocks.1.norm2.shift', 'trf_blocks.2.att.mask', 'trf_blocks.2.att.W_query.weight', 'trf_blocks.2.att.W_query.bias', 'trf_blocks.2.att.W_key.weight', 'trf_blocks.2.att.W_key.bias', 'trf_blocks.2.att.W_value.weight', 'trf_blocks.2.att.W_value.bias', 'trf_blocks.2.att.out_proj.weight', 'trf_blocks.2.att.out_proj.bias', 'trf_blocks.2.ff.layers.0.weight', 'trf_blocks.2.ff.layers.0.bias', 'trf_blocks.2.ff.layers.2.weight', 'trf_blocks.2.ff.layers.2.bias', 'trf_blocks.2.norm1.scale', 'trf_blocks.2.norm1.shift', 'trf_blocks.2.norm2.scale', 'trf_blocks.2.norm2.shift', 'trf_blocks.3.att.mask', 'trf_blocks.3.att.W_query.weight', 'trf_blocks.3.att.W_query.bias', 'trf_blocks.3.att.W_key.weight', 'trf_blocks.3.att.W_key.bias', 'trf_blocks.3.att.W_value.weight', 'trf_blocks.3.att.W_value.bias', 'trf_blocks.3.att.out_proj.weight', 'trf_blocks.3.att.out_proj.bias', 'trf_blocks.3.ff.layers.0.weight', 'trf_blocks.3.ff.layers.0.bias', 'trf_blocks.3.ff.layers.2.weight', 'trf_blocks.3.ff.layers.2.bias', 'trf_blocks.3.norm1.scale', 'trf_blocks.3.norm1.shift', 'trf_blocks.3.norm2.scale', 'trf_blocks.3.norm2.shift', 'trf_blocks.4.att.mask', 'trf_blocks.4.att.W_query.weight', 'trf_blocks.4.att.W_query.bias', 'trf_blocks.4.att.W_key.weight', 'trf_blocks.4.att.W_key.bias', 'trf_blocks.4.att.W_value.weight', 'trf_blocks.4.att.W_value.bias', 'trf_blocks.4.att.out_proj.weight', 'trf_blocks.4.att.out_proj.bias', 'trf_blocks.4.ff.layers.0.weight', 'trf_blocks.4.ff.layers.0.bias', 'trf_blocks.4.ff.layers.2.weight', 'trf_blocks.4.ff.layers.2.bias', 'trf_blocks.4.norm1.scale', 'trf_blocks.4.norm1.shift', 'trf_blocks.4.norm2.scale', 'trf_blocks.4.norm2.shift', 'trf_blocks.5.att.mask', 'trf_blocks.5.att.W_query.weight', 'trf_blocks.5.att.W_query.bias', 'trf_blocks.5.att.W_key.weight', 'trf_blocks.5.att.W_key.bias', 'trf_blocks.5.att.W_value.weight', 'trf_blocks.5.att.W_value.bias', 'trf_blocks.5.att.out_proj.weight', 'trf_blocks.5.att.out_proj.bias', 'trf_blocks.5.ff.layers.0.weight', 'trf_blocks.5.ff.layers.0.bias', 'trf_blocks.5.ff.layers.2.weight', 'trf_blocks.5.ff.layers.2.bias', 'trf_blocks.5.norm1.scale', 'trf_blocks.5.norm1.shift', 'trf_blocks.5.norm2.scale', 'trf_blocks.5.norm2.shift', 'trf_blocks.6.att.mask', 'trf_blocks.6.att.W_query.weight', 'trf_blocks.6.att.W_query.bias', 'trf_blocks.6.att.W_key.weight', 'trf_blocks.6.att.W_key.bias', 'trf_blocks.6.att.W_value.weight', 'trf_blocks.6.att.W_value.bias', 'trf_blocks.6.att.out_proj.weight', 'trf_blocks.6.att.out_proj.bias', 'trf_blocks.6.ff.layers.0.weight', 'trf_blocks.6.ff.layers.0.bias', 'trf_blocks.6.ff.layers.2.weight', 'trf_blocks.6.ff.layers.2.bias', 'trf_blocks.6.norm1.scale', 'trf_blocks.6.norm1.shift', 'trf_blocks.6.norm2.scale', 'trf_blocks.6.norm2.shift', 'trf_blocks.7.att.mask', 'trf_blocks.7.att.W_query.weight', 'trf_blocks.7.att.W_query.bias', 'trf_blocks.7.att.W_key.weight', 'trf_blocks.7.att.W_key.bias', 'trf_blocks.7.att.W_value.weight', 'trf_blocks.7.att.W_value.bias', 'trf_blocks.7.att.out_proj.weight', 'trf_blocks.7.att.out_proj.bias', 'trf_blocks.7.ff.layers.0.weight', 'trf_blocks.7.ff.layers.0.bias', 'trf_blocks.7.ff.layers.2.weight', 'trf_blocks.7.ff.layers.2.bias', 'trf_blocks.7.norm1.scale', 'trf_blocks.7.norm1.shift', 'trf_blocks.7.norm2.scale', 'trf_blocks.7.norm2.shift', 'trf_blocks.8.att.mask', 'trf_blocks.8.att.W_query.weight', 'trf_blocks.8.att.W_query.bias', 'trf_blocks.8.att.W_key.weight', 'trf_blocks.8.att.W_key.bias', 'trf_blocks.8.att.W_value.weight', 'trf_blocks.8.att.W_value.bias', 'trf_blocks.8.att.out_proj.weight', 'trf_blocks.8.att.out_proj.bias', 'trf_blocks.8.ff.layers.0.weight', 'trf_blocks.8.ff.layers.0.bias', 'trf_blocks.8.ff.layers.2.weight', 'trf_blocks.8.ff.layers.2.bias', 'trf_blocks.8.norm1.scale', 'trf_blocks.8.norm1.shift', 'trf_blocks.8.norm2.scale', 'trf_blocks.8.norm2.shift', 'trf_blocks.9.att.mask', 'trf_blocks.9.att.W_query.weight', 'trf_blocks.9.att.W_query.bias', 'trf_blocks.9.att.W_key.weight', 'trf_blocks.9.att.W_key.bias', 'trf_blocks.9.att.W_value.weight', 'trf_blocks.9.att.W_value.bias', 'trf_blocks.9.att.out_proj.weight', 'trf_blocks.9.att.out_proj.bias', 'trf_blocks.9.ff.layers.0.weight', 'trf_blocks.9.ff.layers.0.bias', 'trf_blocks.9.ff.layers.2.weight', 'trf_blocks.9.ff.layers.2.bias', 'trf_blocks.9.norm1.scale', 'trf_blocks.9.norm1.shift', 'trf_blocks.9.norm2.scale', 'trf_blocks.9.norm2.shift', 'trf_blocks.10.att.mask', 'trf_blocks.10.att.W_query.weight', 'trf_blocks.10.att.W_query.bias', 'trf_blocks.10.att.W_key.weight', 'trf_blocks.10.att.W_key.bias', 'trf_blocks.10.att.W_value.weight', 'trf_blocks.10.att.W_value.bias', 'trf_blocks.10.att.out_proj.weight', 'trf_blocks.10.att.out_proj.bias', 'trf_blocks.10.ff.layers.0.weight', 'trf_blocks.10.ff.layers.0.bias', 'trf_blocks.10.ff.layers.2.weight', 'trf_blocks.10.ff.layers.2.bias', 'trf_blocks.10.norm1.scale', 'trf_blocks.10.norm1.shift', 'trf_blocks.10.norm2.scale', 'trf_blocks.10.norm2.shift', 'trf_blocks.11.att.mask', 'trf_blocks.11.att.W_query.weight', 'trf_blocks.11.att.W_query.bias', 'trf_blocks.11.att.W_key.weight', 'trf_blocks.11.att.W_key.bias', 'trf_blocks.11.att.W_value.weight', 'trf_blocks.11.att.W_value.bias', 'trf_blocks.11.att.out_proj.weight', 'trf_blocks.11.att.out_proj.bias', 'trf_blocks.11.ff.layers.0.weight', 'trf_blocks.11.ff.layers.0.bias', 'trf_blocks.11.ff.layers.2.weight', 'trf_blocks.11.ff.layers.2.bias', 'trf_blocks.11.norm1.scale', 'trf_blocks.11.norm1.shift', 'trf_blocks.11.norm2.scale', 'trf_blocks.11.norm2.shift', 'final_norm.scale', 'final_norm.shift', 'out_head.weight'])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict().keys())\n",
    "print(\"=================\"*5)\n",
    "\n",
    "checkpoint = torch.load(\"gpt2-small-124M.pth\")\n",
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed807db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_1568\\1672549393.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"gpt2-small-124M.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "로드 완료!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. 모델과 체크포인트 준비 (가정)\n",
    "gpt = GPTModel(BASE_CONFIG)\n",
    "checkpoint = torch.load(\"gpt2-small-124M.pth\")\n",
    "\n",
    "def load_pretrained_gpt2(model, checkpoint):\n",
    "    new_state_dict = {}\n",
    "    \n",
    "    # 변환 규칙 정의 (찾을 단어: 바꿀 단어)\n",
    "    replacements = {\n",
    "        \"tok_emb\": \"token_embedding\",\n",
    "        \"pos_emb\": \"position_embedding\",\n",
    "        \"trf_blocks\": \"transformer_blocks\",\n",
    "        \".att.\": \".attention.\",\n",
    "        \"out_proj\": \"out_projection\",\n",
    "        \".ff.\": \".ffn.\",\n",
    "    }\n",
    "\n",
    "    for key, value in checkpoint.items():\n",
    "        new_key = key\n",
    "        \n",
    "        # 1. 규칙에 따라 이름 변경\n",
    "        for old, new in replacements.items():\n",
    "            new_key = new_key.replace(old, new)\n",
    "        \n",
    "        # 2. 내 모델에 해당 Key가 존재하는지 확인\n",
    "        if new_key in model.state_dict():\n",
    "            new_state_dict[new_key] = value\n",
    "        else:\n",
    "            # 매칭되지 않는 Key 출력 (디버깅용)\n",
    "            print(f\"Skipping: {key} -> {new_key} (Not found in your model)\")\n",
    "\n",
    "    # 3. 로드 (strict=False를 권장: 위에서 제외한 bias 등이 있을 수 있음)\n",
    "    model.load_state_dict(new_state_dict, strict=False)\n",
    "    print(\"\\n로드 완료!\")\n",
    "\n",
    "# 실행\n",
    "load_pretrained_gpt2(gpt, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67ea908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2ca6a509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (token_embedding): Embedding(50257, 768)\n",
       "  (position_embedding): Embedding(1024, 768)\n",
       "  (drop_embedding): Dropout(p=0.0, inplace=False)\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ab0237",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8703da09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves on.\n",
      "\n",
      "\n",
      "The first thing that comes to mind is that the government is trying to get rid of the \"free market\" and the \"free market is bad\" mantra. The government is\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(62)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate(\n",
    "    model = gpt.to(device),\n",
    "    idx = text_to_token_ids(\"Every effort moves\", tokenizer).to(device),\n",
    "    max_new_tokens = 40,\n",
    "    context_size = BASE_CONFIG[\"context_length\"],\n",
    "    top_k = 1,\n",
    "    temperature = 1.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
