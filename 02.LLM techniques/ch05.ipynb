{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 챕터의 목표 !\n",
    "- 만든 LLM을 Pretraining 시키자 !\n",
    "    - 이를 위한 train loop과 model evaluation code를 구현해볼거임 ..\n",
    "    - 위 과정을 해보고나서 OpenAI에서 공개한 Pretrained Weights를 우리 모델에 적용해볼거임 ..\n",
    "\n",
    "![LLM빌딩과정](https://camo.githubusercontent.com/137f57f6192fbcb6627e6ced1b5274c71924774dec18a4dea29b1c156619ef24/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f30312e77656270)\n",
    "\n",
    "- LLM 모델 만든 후 Finetuning 전까지의 과정 = Pretraining\n",
    "\n",
    "![LLM Pretraining 과정](https://camo.githubusercontent.com/01ebc99e37dddc617ba6dd20799f945fd6a562bac8b8abe5a4b82eb491ebbfca/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f30322e77656270)\n",
    "\n",
    "이번 ch05의 구현 과정을 보여주는 이미지 ..\n",
    "- Text generation\n",
    "    - 모델의 Preatraning에 사용할 무작위 텍스트 생성\n",
    "- Training & validation losses\n",
    "    - Pretraining 전 모델 데이터에 대한 손실 구하기 ..\n",
    "- LLM training function\n",
    "    - Training loop 구현 과정에서 옵티마이저 사용/Epoch 반복하는 동안 손실 그래프로 표현\n",
    "- Text generation strategies\n",
    "    - 다양한 Output을 위한 디코딩 전략 선택 및 텍스트 생성 함수 구현\n",
    "- Weight saving & loading\n",
    "    - 학습된 모델을 저장하고 불러와 볼거임 ..\n",
    "- Pretrained weights from OpenAI\n",
    "    - 이미 훈련 잘 된 좋은 모델을 불러와서 사용해 볼거임 ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapters import GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "# 죄송합니다 .. 레포에 있는거 그대로 썼읍니다 ..\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 저자는 우리가 예제에서 쓰는 GPT는 Dropout을 줬지만 요즘 LLM은 Dropout을 잘 쓰지 않는다고 한다.\n",
    "    - Gemini 괴롭혀보니 이유는 다음과 같음 `출처(Drop Dropout on Single-Epoch Language Model Pretraining H Liu 2025 )`\n",
    "        - 과거에는 비교적 데이터셋이 적고 모델이 작아서 여러번 학습하면서 과적합을 방지해야했음\n",
    "        - 데이터셋이 많아지고 모델이 커지면서 학습비용 증가로 1회만 학습하는 것이 일반적이 됨 \n",
    "    - 드롭아웃 없는 모델이 성능이 더 좋았음(?) \n",
    "        - 논문에서 드롭아웃 없이 학습된 모델이 언어 모델링(Perplexity), 문법 이해(BLiMP), 질의응답(SQuAD), 자연어 추론(MNLI) 모든 지표에서 좋은 성능을 보임\n",
    "    - 학습 시 랜덤하게 뉴런을 끄고 켜는 연산이 모델이 커지면서 연산량에 꽤 부담이 됨\n",
    "\n",
    "- 저자는 또 요즘 LLM은 bias를 잘 쓰지 않는다고 한다.\n",
    "    - 원래 bias 역할이 데이터의 분포를 조절하기 위해 사용되었다면.. 모델이 커지고 Normalization 기법도 좋아지면서 뺐을때 더 안정적인게 더 크다고 본듯..?\n",
    "\n",
    "- 원래 GPT2 모델(124M)은 1024개의 토큰을 사용하지만 예제에서는 우리의 똥컴을 위해 256개로 줄임\n",
    "- 이후 OpenAI에서 푼 Pretrained GPT2 Weights를 사용하게 되면 1024개의 토큰을 사용 예정 .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch04 import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 챕터에서 자주 사용할 함수 두개를 미리 정의함\n",
    "    - text_to_token_ids: 텍스트(문자열) -> 토큰 ID 리스트 (인코딩)\n",
    "    - token_ids_to_text: 토큰 ID 리스트 -> 텍스트(문자열) (디코딩)\n",
    "\n",
    "- 모델이 학습이 안되서 개판으로 Output이 나오는 걸 볼 수 있음\n",
    "\n",
    "- training 과정에서 위와 같은 개판 Output과 \"Good text\"를 수치적으로 비교하고 평가할 수 있는 손실 함수가 필요함 !\n",
    "    - 다음 챕터인 Fine Tuning에서는 손실함 외에도 사람의 평가를 반영할 수 있는 평가 방법을 배움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                        [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- inputs,targets 텐서는 token ID 로 이루어져있음\n",
    "    - inputs : 모델 다음 단어 예측에 사용할 입력\n",
    "    - targets : input 벡터를 한 칸 씩 댕긴걸로 모델 예측에 사용할 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- logits과 probas는 (2,3,50257) 차원을 가진 tensor\n",
    "    - 2 : 배치 사이즈 (인풋 문장 2개)\n",
    "    - 3 : 문장 길이 (단어/토큰 3개)\n",
    "    - 50257 : 단어 종류 \n",
    "- logits는 -무한대 ~ 무한대 까지의 범위를 가진 단어에 대한 점수\n",
    "- probas는 모든 단어에 대한 점수를 합하면 1이 되도록 변환된 점수 = 단어에 대한 확률\n",
    "\n",
    "\n",
    "- 문장을 이루는 token id로 매핑된 단어들이 softmax를 통해 확률을 변환되고 어떻게 출력 문장으로 변환되는지 보여주는 이미지 ..\n",
    "![token_prohabs](https://camo.githubusercontent.com/f98790fc96dfefdd3e61533ca406f241a893976c8cb7668afbcec178763c45a0/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f30342e77656270)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- argmax를 통해 가장 큰 확률을 가진 단어 토큰들을 찾아 모델이 생각하는 정답을 알 수 있음\n",
    "    - dim = -1  : 마지막 차원 기준\n",
    "    - keepdim = True : 차원수를 유지하게함 => 안하면 probas Shape의 (2,3,50257) 에서 최대값 찾는 대상차원인 (50257)이 사라지고 (2,3)됨\n",
    "        - 차원맞출때 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습되지 않은 모델로 예측을 돌리니 당연히 Output 문장이 개판으로 나옴\n",
    "- 학습을 위해서는 단순히 `틀렸다` 라는 사실이 중요한 것이 아닌 `얼마나 정답과 떨어져있냐(틀렸냐)`를 측정할 수 있어야함\n",
    "\n",
    "- 아래는 모델이 정답 단어 토큰의 확률을 어떻게 예측했는지 보여주는 그림\n",
    "    - 정답 단어 토큰들에 대한 확률을 높여야 겠구나 ~ 라는 걸 직관적으로 보여주는 너낌 ..\n",
    "\n",
    "![otken_probas](https://camo.githubusercontent.com/3dee5bf33ad015c683fa2a9a91ae611b263101027fa2f9415934ae1a1c38778e/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f30362e77656270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정답 토큰들에 대한 확률이 엄청 낮은걸 볼 수 있음 .....\n",
    "    - 우리는 이 확률을 maximize 해야한다 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.log를 통해 정답 확률을 로그화함\n",
    "    - 로그값 최대화는 확률의 최대화와 일치함 (단조 증가)\n",
    "    - 로그화를 통해 확률의 곱연산을 로그의 합연산으로 바꿀 수 있음 (연산과정에서 확률의 곱연산은 숫자가 너무 작아질 수 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정답 토큰들의 로그값의 평균을 구함 (정답 토큰들의 여러 로그값(텐서)들을 하나의 수로 표현)\n",
    "- 우리의 목표는 이 평균값을 가능한 크게 만들어야 한다 = 정답 토큰의 확률을 최대화 해야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 딥러닝에서는 average log probability를 최대화 하지 않고 음수화하여 negative average log probability를 최소화하는 접근을 주로 사용함   \n",
    "    - 딥러닝 학습 방법론들이 Gradient Descent에 맞춰져 있기 떄문..?\n",
    "- 이 negative average log probability를 딥러닝에서는 CLE(Cross Loss Entropy)라고 한다\n",
    "- Pytorch는 cross_entropy 함수를 통해 아래 과정을 이미 구현해놨음 !\n",
    "![CrossEntropy](https://camo.githubusercontent.com/3d6fb2ee91bebb246351570506a344d7d579fc161084faec6856c0659c815452/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f30372e77656270)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n",
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)\n",
    "\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- logit의 차원은 (2,3,50257)\n",
    "    - logits.flatten(0,1)을 할 경우 0차원과 1차원을 합쳐지므로(배치차원(문장)*토큰개수(단어)) (6*50257)\n",
    "- Targets의 차원은 (2,3)\n",
    "    - targets.flatten()을 할 경우 모든 차원이 합쳐지므로(0차원,1차원) (6)\n",
    "- 결과적으로 두개의 문장에 대한 정답토큰들을 1차원으로 나열함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)\n",
    "\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss는 모델이 사용하기 적합하지만 사람에게는 와닿지 않는 부분이 있음 ..\n",
    "- Perplexity는 loss의 익스포넨셜 값 (e^loss)로 모델이 정답 단어로 평균적으로 몇개를 선택했는지 보여주는 값\n",
    "    - loss = - (확률의 로그값) = (확률 역수의 로그값) => e^(확률 역수의 로그값) = 확률 역수\n",
    "    - Perplexity = 1 모델이 정답을 평균적으로 1개로 확신하는 상태 (정답확률 100%)\n",
    "    - Perplexity = 48725 모델이 평균적으로 48725개를 정답 후보로 두고 잇음 .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    response = requests.get(url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    text_data = response.text\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "\n",
    "# The book originally used the following code below\n",
    "# However, urllib uses older protocol settings that\n",
    "# can cause problems for some readers using a VPN.\n",
    "# The `requests` version above is more robust\n",
    "# in that regard.\n",
    "\n",
    "        \n",
    "# import os\n",
    "# import urllib.request\n",
    "\n",
    "# file_path = \"the-verdict.txt\"\n",
    "# url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "# if not os.path.exists(file_path):\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         text_data = response.read().decode('utf-8')\n",
    "#     with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "#         file.write(text_data)\n",
    "# else:\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#         text_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우리가 학습에 사용할 데이터셋은 \"The Verdict\"라는 단편 소설임\n",
    "    - 저자는 저작권, 학습 시간, 연산량 등등의 이유로 짧은 데이터셋을 예제로 쓴다함 ~\n",
    "    - 저자가 든 예시로 AWS환경에서 Llama 2(7B)를 2조개의 토큰으로 학습시킨다는 가정하에 A100 GPU 1개 기준으로 184320시간이 필요함\n",
    "        - 184320시간 / 8 * 30불 = 69만불 = 1010022000원 (2026.02.06 공시기준)\n",
    "        - 결론 : 10억 없으면 이걸로 해라 ㄱ-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n",
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "# First 99 characters\n",
    "print(text_data[:99])\n",
    "\n",
    "# Last 99 characters\n",
    "print(text_data[-99:])\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대략 20479개의 문자들과 5145개의 단어들로 구성되어 있는 데이터셋임을 알 수 있음 (저자는 적지만 교육용이니 괜찮다함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import create_dataloader_v1\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch02 import create_dataloader_v1\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일단 모델 학습을 위해서 데이터를 학습용과 검증용으로 나눠야함\n",
    "\n",
    "- train_ratio = 0.90 전체 데이터셋의 학습 데이터 : 검증 데이터 비율을 9 : 1로 나눔\n",
    "\n",
    "- 나눈 데이터셋을 통해 데이터 로더를 생성함\n",
    "    ```python\n",
    "        class GPTDatasetV1(Dataset):\n",
    "        def __init__(self, txt, tokenizer, max_length, stride):\n",
    "            self.input_ids = []\n",
    "            self.target_ids = []\n",
    "\n",
    "            # Tokenize the entire text\n",
    "            token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "            # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "            for i in range(0, len(token_ids) - max_length, stride):\n",
    "                input_chunk = token_ids[i:i + max_length]\n",
    "                target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "                self.input_ids.append(torch.tensor(input_chunk))\n",
    "                self.target_ids.append(torch.tensor(target_chunk))     \n",
    "    ...\n",
    "        def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                                stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "            # Initialize the tokenizer\n",
    "            tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "            # Create dataset\n",
    "            dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "\n",
    "     ```\n",
    "    - 이전 챕터에서 만들었던 데이터셋 코드를 보면 target을 만들때 input 텐서를 한칸씩 밀어서 만든 것을 알 수 있음 !\n",
    "\n",
    "- 아래 이미지는 원본 데이터셋을 학습용/검증용으로 쪼개기 => 토큰화 => 학습를 위해 배치단위로 쪼개기 과정을 보여줌 ..\n",
    "    - 이미지는 6개의 토큰들의 청크로 구성되어있지만 예제 모델은 256개임을 주의 ..\n",
    "![dataset](https://camo.githubusercontent.com/216592059168cdb0fba2d8b9843bfc411a1898c407163cabb1d98a59e7cc584a/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f30392e77656270)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    \n",
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train Loader의 차원 (9,2,256)\n",
    "    - x의 차원 (input 모델 학습에 사용되는 입력) (2,256)\n",
    "    - y의 차원 (targets 모델 학습에 사용되는 정답) (2,256)\n",
    "    - 즉 9번의 배치의 2*256개의 토큰들로 구성됨\n",
    "- Validation Loader의 차원 (1,2,256)\n",
    "    - 즉 1번의 배치의 2*256개의 토큰들로 구성됨\n",
    "\n",
    "- 전체 토큰들이 5145개가 아닌이유?\n",
    "    - last_drop = true 를 통해 자투리 토큰들은 학습/검증에 사용하지 않음.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- calc_loss_batch 함수를 통해 배치 하나(예제 batch_size = 2)에 대해서만 loss를 계산함\n",
    "    - 이 단계에서 to(device) 함수를 통해 **손실함수 계산 전 입력,정답 텐서를 GPU로 옮김**\n",
    "\n",
    "- calc_loss_loader 함수를 통해 배치 단위로 계산한 loss의 합을 평균 loss 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n",
      "Training loss: 10.98758347829183\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Use PyTorch 2.9 or newer for stable mps results\n",
    "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "    if (major, minor) >= (2, 9):\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 코드 예제에서는 torch 객체를 통해 device 우선순위를 정함\n",
    "    - CUDA > MPS(cuda 못 쓰는 맥북러들 위해 ..) > CPU\n",
    "\n",
    "- model.to(device)를 사용하면 재할당해야하는 텐서와 달리 to(device) 호출시 GPU 이동\n",
    "    - data와 model의 device를 일치시키기 위해 손실함수(calc_loss_loader) 입력인자와 model의 device가 같은 걸 볼 수 있음\n",
    "\n",
    "- 학습하지 않아 train loss / validation loss 모두 개높음\n",
    "\n",
    "- 현재까지  Model Pretraining을 위한 과정들이었다는 일깨워주는 이미지 ..\n",
    "![trainingphase](https://camo.githubusercontent.com/de4c90a17bebd1ed30b5ae1724724d2fb8fec8233258eb2c6c931e17fb3a3b5a/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f31302e77656270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 학습에 사용되는 함수 train_model_simple()을 정의\n",
    "    - num_epoches\n",
    "        - 같은 데이터셋으로 학습을 몇 번 반복할건지\n",
    "    - eval_freq\n",
    "        - 배치단위 학습 몇 번에 한 번 모델을 평가할지\n",
    "    - eval_iter\n",
    "        - 한번에 배치 몇개를 평가할지 \n",
    "    - loss.backward()\n",
    "        - Loss에 대한 Gradient 구하기\n",
    "    - optimizer.step()\n",
    "        - 모델 Weight 수정\n",
    "    - model.train() model.eval() torch.no_grad()\n",
    "        - model.train()\n",
    "            - BatchNorm O / Dropout O / Gradient O\n",
    "        - model.eval()\n",
    "            - BatchNorm X / Dropout X / Gradient O\n",
    "        - torch.no_grad()\n",
    "            - BatchNorm O / Dropout X / Gradient X\n",
    "\n",
    "- 아래 이미지는 구현했던 모델의 훈련/검증 과정을 도표로 보여줌 ..\n",
    "![Training_phase](https://camo.githubusercontent.com/37533967fe3f55ee41068b1d695b4664c81b2b4b228d369f52a77148c3c01682/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f31312e77656270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cpu\n",
      "Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933\n",
      "Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048\n",
      "Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n",
      "Ep 3 (Step 000020): Train loss 5.726, Val loss 6.600\n",
      "Ep 3 (Step 000025): Train loss 5.201, Val loss 6.348\n",
      "Every effort moves you, and I had been.                                            \n",
      "Ep 4 (Step 000030): Train loss 4.417, Val loss 6.278\n",
      "Ep 4 (Step 000035): Train loss 4.069, Val loss 6.226\n",
      "Every effort moves you know the                          \"I he had the donkey and I had the and I had the donkey and down the room, I had\n",
      "Ep 5 (Step 000040): Train loss 3.732, Val loss 6.160\n",
      "Every effort moves you know it was not that the picture--I had the fact by the last I had been--his, and in the            \"Oh, and he said, and down the room, and in\n",
      "Ep 6 (Step 000045): Train loss 2.850, Val loss 6.179\n",
      "Ep 6 (Step 000050): Train loss 2.427, Val loss 6.141\n",
      "Every effort moves you know,\" was one of the picture. The--I had a little of a little: \"Yes, and in fact, and in the picture was, and I had been at my elbow and as his pictures, and down the room, I had\n",
      "Ep 7 (Step 000055): Train loss 2.104, Val loss 6.134\n",
      "Ep 7 (Step 000060): Train loss 1.882, Val loss 6.233\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no--as! The women had been, in the moment--as Jack himself, as once one had been the donkey, and were, and in his\n",
      "Ep 8 (Step 000065): Train loss 1.320, Val loss 6.238\n",
      "Ep 8 (Step 000070): Train loss 0.985, Val loss 6.242\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Ep 9 (Step 000075): Train loss 0.717, Val loss 6.293\n",
      "Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, I had the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\n",
      "Training completed in 5.12 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "print(torch.__version__)\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습이 진행될수록 문법적으로 그럴듯한 문장을 만들어 내지만 Validation Loss는 전혀 줄지 않음\n",
    "    - Training data에 있는 내용이 그대로 나옴 => 과적합이 발생했음을 알 수 있음\n",
    "    - 학습에 사용된 데이터가 너무 적어서 발생한 문제 ..\n",
    "    - 이후 OpenAI에서 제공한 Pretrained Weights를 적용시켜 차이를 볼거임\n",
    "- Gutenberg 데이터셋으로 더 많은 데이터 실험 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
     ]
    }
   ],
   "source": [
    "# NEW: use CPU here as inference is cheap with \n",
    "# this model and to ensure readers get same results in the\n",
    "# remaining sections of this book\n",
    "inference_device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(inference_device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(inference_device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델의 학습 과정에서는 loss,Gradient 구하는 연산량이 상당히 많았지만 모델의 검증과정에서는 훨씬 연산량이 적어 CPU로도 충분하다.\n",
    "    - 따라서 inference_device 에 cpu를 할당했음\n",
    "- generate_text_simple 함수를 통한 모델의 추론은 같은 input에 대해서 항상 같은 output을 출력함\n",
    "    - 이렇게 결정론적인 함수가 된 이유는 argmax를 통해 항상 가장 확률이 높은 토큰들만 출력하기 떄문\n",
    "        - 같은 Weights을 가진 모델의 같은 input에 대한 추론은 항상 같은 output이 나오게됨 ..\n",
    "    - 따라서 답변의 창의성, 다양성이 떨어지게됨\n",
    "- 이를 해결하기 위해 Decoding Strategies 디코딩 전략을 도입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- argmax를 사용한 디코딩은 다음 토큰을 예측할때 가장 큰 토큰확률을 가진 토큰을 선택하게됨\n",
    "    - **Greedy Decoding**\n",
    "    - 위 예제에서는 항상 가장 높은 확률을 가진 토큰인 'forward'를 예측하게됨\n",
    "\n",
    "- torch.multinomial(probas, num_samples=1).item() 을 통해 각 토큰이 가진 확률들에 따라 다음 토큰 하나를 뽑게 됨\n",
    "    - **Random Sampling**\n",
    "    - num_samples = 1 : 확률에 따라 1개만 선택하기\n",
    "    - .item() : 텐서가 아닌 int,float과 같은 숫자를 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n",
      "0 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample), minlength=len(probas))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실제로 torch.multinomial를 많은 횟수로 수행해본다면 토큰이 나오는 횟수가 토큰의 확률분포와 유사한걸 알 수 있다 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "\n",
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Temperature Scaling 기법은 Softmax를 계산하기 전 logit값에 특정 상수인 온도(T)를 나눠주는 기법\n",
    "    - **확률의 스케일링을 통해 토큰간의 차이**를 조정하기 위해 사용됨 !\n",
    "- T=1 일 땐 모델이 학습한 확률을 그대로 사용함\n",
    "- T<1 일 땐 확률들의 격차가 커진다. => 이미 높은 확률을 가진 토큰들은 더욱 확률이 높아지고 낮은 확률을 가진 토큰은 더욱 확률이 낮아진다.\n",
    "    - 거의 Greedy Decoding과 유사해짐\n",
    "- T>1 일 땐 확률들의 격차가 작아진다. => 모든 토큰들의 확률이 비슷해진다.\n",
    "\n",
    "\n",
    "![random_sampling_with_temperature](random_sampling_with_temperature.png))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "985 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "15 x toward\n",
      "0 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Temperature가 0.1 인 경우 argmax (Greedy Decoding)과 유사함을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 x closer\n",
      "75 x every\n",
      "42 x effort\n",
      "239 x forward\n",
      "71 x inches\n",
      "46 x moves\n",
      "32 x pizza\n",
      "227 x toward\n",
      "103 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Temperature가 5인 경우 고른 확률 분포를 볼 수 있다 .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output의 다양성을 높이기 위해 T 를 높였을때 말도 안되는 토큰을 모델이 선택할 확률도 높아진다 ..\n",
    "    - 이를 보완하기 위해 Top-K Sampling도 같이 사용한다. \n",
    "\n",
    "- 아래 이미지처럼 logit 값의 크기 순으로 k개의 토큰만 예측에 사용하는 기법이 Top-K Sampling이다.\n",
    "    - 높은 logit(높은 확률을)을 가진 토큰들만 예측에 사용되므로 이상한 토큰을 예측할 가능성이 많이 줄어든다\n",
    "\n",
    "![top_k_samling](https://camo.githubusercontent.com/7bc113d7a12a49a473a9059f425850564ce392aa34d8925e6edeb0a9558b73cb/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f31352e77656270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n",
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n",
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)\n",
    "\n",
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "new_logits_alt = torch.full_like( # create tensor containing -inf values\n",
    "   next_token_logits, -torch.inf\n",
    ")   \n",
    "new_logits[top_pos] = next_token_logits[top_pos] # copy top k values into the -inf tensor\n",
    "\n",
    "\n",
    "print(new_logits)\n",
    "\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 예제는 Top-K Sampling을 직접 구현함\n",
    "    - 모델이 계산한 logit값에 예측에 사용되지 않은 토큰 n-k개의 logit을 -torch.inf로 초기화함 (new_logits)\n",
    "        - 저자는 미리 -torch.inf로 구성된 텐서에서 k개의 토큰만 확률값을 넣어주는 방법이 더 빠를것 같다고 제안함 (new_logits_alt)\n",
    "- Top-K를 거친 logit값을 Softmax를 태워보면 K개의 토큰을 제외하고 확률이 0인걸 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # New (not in book): numerical stability tip to get equivalent results on mps device\n",
    "            # subtract rowwise max before softmax\n",
    "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generate 함수를 통해 Temperature/Top-K Sampling 이 적용된 모델 예측을 구현하였다\n",
    "    ```python\n",
    "    logits = logits[:, -1, :]\n",
    "    ```\n",
    "    - 우리가 이번에 필요한건 마지막 토큰 다음에 어떤 토큰이 나오는가임\n",
    "    ```python\n",
    "    logits = logits - logits.max(dim=-1, keepdim=True).values \n",
    "    ```\n",
    "    - Softmax 계산 시 값이 너무 커지는 걸 방지하기 위한 방법\n",
    "    ```python\n",
    "    idx = torch.cat((idx, idx_next), dim=1)\n",
    "    ```\n",
    "    - 예측한 토큰을 다음 input에 붙여넣음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델의 파라미터(Weights / Biases)를 파이썬 딕셔너리 형태로 저장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Use PyTorch 2.9 or newer for stable mps results\n",
    "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "    if (major, minor) >= (2, 9):\n",
    "        device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model.load_state_dict() 함수를 통해 현재 모델 instance에 이전에 학습된 가중치를 넣게 됨\n",
    "    - map_location=device : 모델에서 학습했을 때 환경과 현재 환경이 다를 경우 에러가 발생하므로 현재 사용가능한 device로 파라미터를 적용\n",
    "    - weights_only=True : 보안을 위해 파일에 존재하는 Weights만 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적으로 LLM 학습 시에는 단순 SGD로 학습하는거보다 Adam/AdamW 같은 Adapative Optimizer를 사용함\n",
    "    - Optimizer를 사용할 경우 이 후에도 이 모델을 Pretraining 하기 위해서는 모델과 함께 Optimizer도 같이 저장해야함\n",
    "\n",
    "- Optimizer를 저장하면 같이 저장했던 모델의 마지막 Pretraining 당시 momentum을 기억하게됨 ..\n",
    "    - 초기 loss도 안튀고 학습 속도도 빨라짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "\n",
    "file_name = \"gpt2-small-124M.pth\"\n",
    "url = f\"https://huggingface.co/rasbt/gpt2-from-scratch-pytorch/resolve/main/{file_name}\"\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(url, file_name) # 파일이 없으면 다운로드\n",
    "    print(f\"Downloaded to {file_name}\")\n",
    "\n",
    "GPT_CONFIG_124M.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "gpt = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 저자가 경고하길 GPT-2가 Tensorflow로 만들어져서 원본 모델 파일도 Tensorflow용임 ..\n",
    "- 근데 우리는 Torch를 쓰기 때문에 자기가 이미 편~안하게 GPT-2를 torch용으로 바꿔놨다 그니까 갖다 써라 ~ 라고 함\n",
    "    - 감사합니다 ..\n",
    "- gpt2-small-124M : GPT 모델 중 가장 작은거 (파라미터 1억개..)\n",
    "- 이전 예제에서 사용했던 GPT_CONFIG_124M 그대로 사용했다 ..\n",
    "- 우리가 사용할 gpt2-small-124M은 INPUT length가 토큰 1024개 이므로 CONFIG를 맞춰준다.\n",
    "\n",
    "- 아래는 우리가 사용할 gpt 2m 모델의 간략한 구조임\n",
    "![gpt2m_design](https://camo.githubusercontent.com/5435c19ef837e8a269b9f829af032ce0103519a6bd3d1030abdda58a6db498a1/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f31372e77656270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint = torch.load(\"gpt2-small-124M.pth\", map_location=device, weights_only=True)\n",
    "\n",
    "gpt.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward finding an ideal new way to practice something!\n",
      "\n",
      "What makes us want to be on top of that?\n",
      "\n",
      "\n",
      "GPT 2M validation loss 3.559627056121826\n",
      "Our model validation loss 6.452609062194824\n"
     ]
    }
   ],
   "source": [
    "gpt.eval()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=1024,\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "with torch.no_grad():\n",
    "    gpt2m_val_loss = calc_loss_loader(val_loader, gpt, device)\n",
    "    hommade_val_loss = calc_loss_loader(val_loader,model,device)\n",
    "    \n",
    "    \n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "print(\"GPT 2M validation loss\",gpt2m_val_loss)\n",
    "\n",
    "print(\"Our model validation loss\",hommade_val_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아주 정확한 OUTPUT을 보여주는걸 볼 수 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
