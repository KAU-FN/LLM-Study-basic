{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4573687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "# pytorch가 기록하는 Peak 메모리 초기화\n",
    "def start_memory_tracking():\n",
    "    \"\"\"Initialize GPU memory tracking.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    else:\n",
    "        print(\"This notebook is intended for CUDA GPUs but CUDA is not available.\")\n",
    "\n",
    "# 메모리 최대 사용량 확인\n",
    "def print_memory_usage():\n",
    "    max_gpu_memory = torch.cuda.max_memory_allocated() / (1024 ** 3)  # Convert bytes to GB\n",
    "    print(f\"Maximum GPU memory allocated: {max_gpu_memory:.1f} GB\")\n",
    "\n",
    "def cleanup():\n",
    "    # 파이썬 garbage collection\n",
    "    gc.collect()\n",
    "    # torch 캐시 비우기\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(3)  # some buffer time to allow memory to clear\n",
    "    #기록 초기화\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    max_memory_allocated = torch.cuda.max_memory_allocated(device) / (1024 ** 3)\n",
    "    print(f\"Maximum GPU memory allocated: {max_memory_allocated:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf62977",
   "metadata": {},
   "source": [
    "- 메모리 벤치마크를 위한 공통 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24b2cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import GPTModel\n",
    "# If the `previous_chapters.py` file is not available locally,\n",
    "# you can import it from the `llms-from-scratch` PyPI package.\n",
    "# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# E.g.,\n",
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-xl (1558M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265c3f2",
   "metadata": {},
   "source": [
    "- 모델은 GPT2에서 가장큰 버전인 gpt2-xl을 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab07a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_memory_tracking()\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbed966",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c278dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the model works (no need to track memory here)\n",
    "test_input = torch.tensor([[1, 2, 3]]).to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model(test_input)\n",
    "# Training code would go here...\n",
    "\n",
    "model.train()\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "del model, test_input\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b913333",
   "metadata": {},
   "source": [
    "- 모델 실행 후 GPU 메모리를 비우는 과정 .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e2cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then load pretrained weights\n",
    "\n",
    "start_memory_tracking()\n",
    "\n",
    "## BASE_CONFIG로 깡통 모델 생성\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "## GPU로 옮김 (GPU 현재 메모리 6.4GB)\n",
    "model.to(device)\n",
    "\n",
    "# map_location=device 로 인하여 파일에서 읽은 가중치 데이터가 곧장 GPU로 가게됨\n",
    "# (GPU 현재 메모리 12.8GB) (모델 초기 랜덤 Weight 6.4GB) + (불러온 Weights 6.4GB    )\n",
    "model.load_state_dict(\n",
    "    torch.load(\"model.pth\", map_location=device, weights_only=True)\n",
    ")\n",
    "model.to(device)\n",
    "model.eval();\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "# Test if the model works (no need to track memory here)\n",
    "test_input = torch.tensor([[1, 2, 3]]).to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model(test_input)\n",
    "\n",
    "del model, test_input\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c30cc7",
   "metadata": {},
   "source": [
    "- 위에서 같은 CONFIG로 생성된 모델임에도 메모리를 두배를 쳐먹는걸 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aa741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_memory_tracking()\n",
    "\n",
    "# 먼저 깡통 모델을 GPU에 넣음\n",
    "model = GPTModel(BASE_CONFIG).to(device)\n",
    "\n",
    "# 불러온 가중치를 CPU에 넣음\n",
    "state_dict = torch.load(\"model.pth\", map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "# Sequentially copy weights to the model's parameters\n",
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in state_dict:\n",
    "            # GPU에서 메모리를 점유하고 있는 깡통 모델의 가중치에 CPU에 있는 가중치를 복사함 \n",
    "            param.copy_(state_dict[name].to(device))\n",
    "        else:\n",
    "            print(f\"Warning: {name} not found in state_dict.\")\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6953c9d1",
   "metadata": {},
   "source": [
    "- 복사할 임시 가중치는 CPU에 저장한 후 GPU로 복사하는 과정을 통해 메모리를 아꼈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1481a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "def memory_usage_in_gb(func, *args, **kwargs):\n",
    "    process = psutil.Process(os.getpid())\n",
    "\n",
    "    # Measure the baseline memory usage before running the function\n",
    "    # 함수 실행 전 메모리 재기\n",
    "    baseline_mem = process.memory_info().rss / 1024 ** 3  # in GB\n",
    "\n",
    "    # Start monitoring memory in a separate thread\n",
    "    mem_usage = []\n",
    "    done = False\n",
    "\n",
    "    def monitor_memory():\n",
    "        while not done:\n",
    "            # 0.1 초마다 현재 메모리 사용량을 기록\n",
    "            mem_usage.append(process.memory_info().rss / 1024 ** 3)  # Convert to GB\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    # 메모리 감시 스레드 실행 (함수가 실행되는 와중에도 메모리 사용량을 기록하기 위해)\n",
    "    t = Thread(target=monitor_memory)\n",
    "    t.start()\n",
    "\n",
    "    # 메모리 측정에 사용될 함수 실행\n",
    "    func(*args, **kwargs)\n",
    "\n",
    "    # Stop monitoring\n",
    "    # 모니터링 종료\n",
    "    done = True\n",
    "    t.join()\n",
    "\n",
    "    peak_mem_usage_gb = max(mem_usage) - baseline_mem\n",
    "    return peak_mem_usage_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1f91e",
   "metadata": {},
   "source": [
    "- 쓰레드를 통해 작업이 시작 전, 끝난 후 두 순간의 메모리 사용량 뿐만 아니라 실행되는 도중 메모리 사용량을 체크할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06392901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequentially():\n",
    "    start_memory_tracking()\n",
    "\n",
    "    model = GPTModel(BASE_CONFIG).to(device)\n",
    "\n",
    "    state_dict = torch.load(\"model.pth\", map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "    print_memory_usage()\n",
    "\n",
    "    # Sequentially copy weights to the model's parameters\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in state_dict:\n",
    "                param.copy_(state_dict[name].to(device))\n",
    "            else:\n",
    "                print(f\"Warning: {name} not found in state_dict.\")\n",
    "\n",
    "    print_memory_usage()\n",
    "\n",
    "\n",
    "peak_memory_used = memory_usage_in_gb(load_sequentially)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75693bbd",
   "metadata": {},
   "source": [
    "- 앞서 했던것처럼 깡통 모델 GPU 담기 => 불러온 가중치 CPU에 담기 => \n",
    "- 최대 메모리가 비슷한걸 볼 수 있다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae08f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequentially_with_meta():\n",
    "    start_memory_tracking()\n",
    "\n",
    "\n",
    "## meta 디바이스는 임시로 실제 할당되는 메모리 없이 데이터 불러오기 전까지 할당할 수 있는 device\n",
    "    with torch.device(\"meta\"):\n",
    "        model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "# meta에만 있었던 깡통 모델(메모리 점유 0)이 실제 GPU에 할당됨\n",
    "    model = model.to_empty(device=device)\n",
    "\n",
    "    state_dict = torch.load(\"model.pth\", map_location=device, weights_only=True)\n",
    "\n",
    "    print_memory_usage()\n",
    "\n",
    "    # Sequentially copy weights to the model's parameters\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in state_dict:\n",
    "                param.copy_(state_dict[name])\n",
    "            else:\n",
    "                print(f\"Warning: {name} not found in state_dict.\")\n",
    "\n",
    "    print_memory_usage()\n",
    "\n",
    "peak_memory_used = memory_usage_in_gb(load_sequentially_with_meta)\n",
    "print(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388bb3f7",
   "metadata": {},
   "source": [
    "- meta device로 실제 가중치 로드 전까지 임시로 device객체를 부여 할 수 있다.\n",
    "    - 테스트할떄나 임시로 Model 초기화 없이 구조만 보고 싶을때 좋을듯 .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_practices():\n",
    "  # meta객체를 통해 깡통 모델 \n",
    "  with torch.device(\"meta\"):\n",
    "      model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "  # mmap=True 옵션을 통해 가상 메모리 주소랑 Mapping만 해둠..\n",
    "  # load_state_dict 함수의 assign=True를 통해 불러온 가중치로 replace\n",
    "  model.load_state_dict(\n",
    "      torch.load(\"model.pth\", map_location=device, weights_only=True, mmap=True),\n",
    "      assign=True\n",
    "  )\n",
    "\n",
    "  print_memory_usage()\n",
    "\n",
    "peak_memory_used = memory_usage_in_gb(best_practices)\n",
    "print(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5568164e",
   "metadata": {},
   "source": [
    "- 임시로 깡통모델을 meta divice에 할당\n",
    "- 이후 torch.load 인자인 **mmap = True** 설정을 활용하여 가중치 파일을 메모리에 전부 로드하지 않고 가상 메모리 주소랑 Mapping만 해둠\n",
    "- load_state_dict의 assign=True 옵션은 **파라미터를 Copy하는게 아닌 인자의 가중치로 Replace함**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
