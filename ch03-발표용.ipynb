{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae38945-39dd-45dc-ad4f-da7a4404241f",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa70ec-5c4c-40e8-b923-16f8167e3181",
   "metadata": {},
   "source": [
    "# Chapter 3: Coding Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29bcbe8-a034-43a2-b557-997b03c9882d",
   "metadata": {},
   "source": [
    "Packages that are being used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e58f33e8-5dc9-4dd5-ab84-5a011fa11d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4474d-7c68-4846-8702-37906cf08197",
   "metadata": {},
   "source": [
    "- 이 장에서는 LLM의 핵심 동력인 주의 메커니즘에 대해 다룹니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a11208-d9d3-44b1-8e0d-0c8414110b93",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/01.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e020fd-9690-4343-80df-da96678bef5e",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/02.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc4dcee-34ea-4c05-9085-2f8887f70363",
   "metadata": {},
   "source": [
    "## 3.1 The problem with modeling long sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55aa49c-36c2-48da-b1d9-70f416e46a6a",
   "metadata": {},
   "source": [
    "- No code in this section\n",
    "- Translating a text word by word isn't feasible due to the differences in grammatical structures between the source and target languages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c0c433-aa4b-491e-848a-54905ebb05ad",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/03.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db03c48a-3429-48ea-9d4a-2e53b0e516b1",
   "metadata": {},
   "source": [
    "- 트랜스포머 모델이 도입되기 전에는 기계 번역 작업에 인코더-디코더 RNN이 일반적으로 사용되었습니다.\n",
    "- 이 구성에서 인코더는 소스 언어의 토큰 시퀀스를 처리하고, 신경망 내의 중간 계층과 같은 은닉 상태를 사용하여 전체 입력 시퀀스의 압축된 표현을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8df2c-c1c2-4df0-9977-ade9713088b2",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/04.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602c585-b87a-41c7-a324-c5e8298849df",
   "metadata": {},
   "source": [
    "## 3.2 Capturing data dependencies with attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fde64c-6034-421d-81d9-8244932086ea",
   "metadata": {},
   "source": [
    "- 이 섹션에는 코드가 없습니다.\n",
    "- 어텐션 메커니즘을 통해 네트워크의 텍스트 생성 디코더 부분은 모든 입력 토큰에 선택적으로 접근할 수 있으며, 이는 특정 입력 토큰이 특정 출력 토큰 생성에 있어 다른 토큰보다 더 중요한 의미를 갖는다는 것을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f6293-8ab5-4aeb-a04c-50ee158485b1",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/05.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044be1f-e6a2-4a1f-a6dd-e325d3bad05e",
   "metadata": {},
   "source": [
    "\n",
    "- 트랜스포머에서 셀프 어텐션은 시퀀스의 각 위치가 동일 시퀀스 내의 다른 모든 위치와 상호 작용하고 그 관련성을 판단할 수 있도록 함으로써 입력 표현을 향상시키도록 설계된 기술입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6565dc9f-b1be-4c78-b503-42ccc743296c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/06.webp\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efe05ff-b441-408e-8d66-cde4eb3397e3",
   "metadata": {},
   "source": [
    "## 3.3 Attending to different parts of the input with self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9af516-7c37-4400-ab53-34936d5495a9",
   "metadata": {},
   "source": [
    "### 3.3.1 A simple self-attention mechanism without trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269e9f1-df11-4644-b575-df338cf46cdf",
   "metadata": {},
   "source": [
    "이 섹션에서는 **학습 가능한 가중치(trainable weights)**가 포함되지 않은, 매우 단순화된 형태의 셀프 어텐션 변형을 설명합니다.  \n",
    "이는 순수하게 설명을 돕기 위한 목적이며, 트랜스포머(Transformers)에서 실제로 사용되는 어텐션 메커니즘이 아닙니다.  \n",
    "다음 섹션인 3.3.2 섹션에서 이 단순한 어텐션 메커니즘을 확장하여, 실제 셀프 어텐션 메커니즘을 구현할 예정입니다.  \n",
    "$x^{(1)}$부터 $x^{(T)}$까지의 입력 시퀀스가 주어졌다고 가정해 봅시다.  \n",
    "입력은 2장에서 설명한 대로 이미 토큰 임베딩으로 변환된 텍스트(예: \"Your journey starts with one step\"과 같은 문장)입니다.  \n",
    "예를 들어, $x^{(1)}$은 \"Your\"라는 단어를 나타내는 $d$차원 벡터이며, 나머지 요소들도 이와 같습니다.  \n",
    "목표: $x^{(1)}$부터 $x^{(T)}$까지의 각 입력 시퀀스 요소 $x^{(i)}$에 대응하는 **컨텍스트 벡터(context vectors) $z^{(i)}$**를 계산하는 것입니다.  \n",
    "(여기서 $z$와 $x$는 동일한 차원을 가집니다.)컨텍스트 벡터 $z^{(i)}$는 입력 $x^{(1)}$부터 $x^{(T)}$까지의 **가중합(weighted sum)**입니다.  \n",
    "컨텍스트 벡터는 특정 입력에 대해 \"문맥(context)\"-특이적입니다.  \n",
    "임의의 입력 토큰을 나타내는 자리표시자(placeholder)인 $x^{(i)}$ 대신, 구체적으로 두 번째 입력인 $x^{(2)}$를 생각해 봅시다.또한 구체적인 예를 이어가기 위해, 자리표시자 $z^{(i)}$ 대신 두 번째 출력 컨텍스트 벡터인 $z^{(2)}$를 고려해 봅니다.  \n",
    "두 번째 컨텍스트 벡터 $z^{(2)}$는 두 번째 입력 요소 $x^{(2)}$를 기준으로, 모든 입력 $x^{(1)}$부터 $x^{(T)}$까지에 가중치를 부여하여 더한 가중합입니다.  \n",
    "**어텐션 가중치(attention weights)**는 $z^{(2)}$를 계산할 때 각 입력 요소가 가중합에 얼마나 기여할지를 결정하는 값입니다.  \n",
    "요약하자면, $z^{(2)}$를 **\"주어진 작업에 관련된 다른 모든 입력 요소의 정보를 통합하여 수정한 $x^{(2)}$의 버전\"**으로 생각하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc7c7a2-b6ab-478f-ae37-faa8eaa8049a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/07.webp\" width=\"400px\">\n",
    "\n",
    "- (이 그림의 숫자는 시각적 혼란을 줄이기 위해 소수점 이하 한 자리까지만 표시되도록 잘렸습니다. 마찬가지로 다른 그림에도 잘린 값이 포함될 수 있습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff856c58-8382-44c7-827f-798040e6e697",
   "metadata": {},
   "source": [
    "- By convention, the unnormalized attention weights are referred to as **\"attention scores\"** whereas the normalized attention scores, which sum to 1, are referred to as **\"attention weights\"**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b10344-128d-462a-823f-2178dff5fd58",
   "metadata": {},
   "source": [
    "\n",
    "- 아래 코드는 위 그림을 단계별로 따라갑니다.\n",
    "<br>\n",
    "\n",
    "- **1단계:** 정규화되지 않은 주의력 점수 $\\omega$를 계산합니다.\n",
    "- 두 번째 입력 토큰을 쿼리로 사용한다고 가정하면, 즉 $q^{(2)} = x^{(2)}$인 경우, 내적을 통해 정규화되지 않은 어텐션 점수를 계산합니다.\n",
    "    - $\\omega_{21} = x^{(1)} q^{(2)\\top}$\n",
    "    - $\\omega_{22} = x^{(2)} q^{(2)\\top}$\n",
    "    - $\\omega_{23} = x^{(3)} q^{(2)\\top}$\n",
    "    - ...\n",
    "    - $\\omega_{2T} = x^{(T)} q^{(2)\\top}$\n",
    "\n",
    "- 위에서 $\\omega$는 정규화되지 않은 주의력 점수를 나타내는 데 사용되는 그리스 문자 \"오메가\"입니다.\n",
    "\n",
    "- $\\omega_{21}$의 아래첨자 \"21\"은 입력 시퀀스 요소 2가 입력 시퀀스 요소 1에 대한 쿼리로 사용되었음을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e55f7a-f2d0-4f24-858b-228e4fe88fb3",
   "metadata": {},
   "source": [
    "- 3장에서 설명한 대로 3차원 벡터에 이미 임베딩된 다음 입력 문장이 있다고 가정해 보겠습니다(설명을 위해 줄 바꿈 없이 페이지에 맞도록 매우 작은 임베딩 차원을 사용합니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b9556a-aaf8-4ab4-a5b4-973372b0b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299baef3-b1a8-49ba-bad4-f62c8a416d83",
   "metadata": {},
   "source": [
    "- (본 책에서는 머신러닝 및 딥러닝에서 흔히 사용되는 관례에 따라 훈련 예제를 행으로, 특징 값을 열로 표현합니다. 위 그림의 텐서에서 각 행은 단어를 나타내고, 각 열은 임베딩 차원을 나타냅니다.)\n",
    "\n",
    "- 이 섹션의 주요 목적은 두 번째 입력 시퀀스 $x^{(2)}$를 쿼리로 사용하여 컨텍스트 벡터 $z^{(2)}$를 계산하는 방법을 보여주는 것입니다.\n",
    "\n",
    "- 그림은 이 과정의 첫 번째 단계를 나타냅니다. 이 단계에서는 내적 연산을 통해 $x^{(2)}$와 다른 모든 입력 요소 간의 어텐션 점수 ω를 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb3453a-58fa-42c4-b225-86850bc856f8",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/08.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be52fb-82fd-4886-a4c8-f24a9c87af22",
   "metadata": {},
   "source": [
    "- 입력 시퀀스 요소 2인 $x^{(2)}$를 예시로 사용하여 컨텍스트 벡터 $z^{(2)}$를 계산합니다. 이 섹션 후반부에서 이를 일반화하여 모든 컨텍스트 벡터를 계산할 것입니다.\n",
    "- 첫 번째 단계는 쿼리 $x^{(2)}$와 다른 모든 입력 토큰 간의 내적을 계산하여 정규화되지 않은 어텐션 점수를 계산하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fb5b2f8-dd2c-4a6d-94ef-a0e9ad163951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df09ae0-199f-4b6f-81a0-2f70546684b8",
   "metadata": {},
   "source": [
    "- 참고: 내적은 기본적으로 두 벡터의 요소를 곱하고 그 결과를 모두 더하는 것을 간략하게 표현한 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9842f39b-1654-410e-88bf-d1b899bf0241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n",
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "res = 0.\n",
    "\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "\n",
    "print(res)\n",
    "print(torch.dot(inputs[0], query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d444d76-e19e-4e9a-a268-f315d966609b",
   "metadata": {},
   "source": [
    "- **2단계:** 정규화되지 않은 어텐션 점수(\"오메가\", $\\omega$)의 합이 1이 되도록 정규화합니다.\n",
    "- 정규화되지 않은 어텐션 점수의 합을 1로 만드는 간단한 방법은 다음과 같습니다(해석에 유용하고 훈련 안정성에 중요한 관례입니다)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd965d6-980c-476a-93d8-9efe603b1b3b",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/09.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3ccc99c-33ce-4f11-b7f2-353cf1cbdaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc0a57-f53e-41bf-8793-daa77a819431",
   "metadata": {},
   "source": [
    "\n",
    "- 하지만 실제로는 극단값을 더 잘 처리하고 학습 중 기울기 특성이 더 바람직한 소프트맥스 함수를 정규화에 사용하는 것이 일반적이며 권장됩니다.\n",
    "- 다음은 벡터 요소의 합이 1이 되도록 정규화하는 동시에 스케일링을 위한 소프트맥스 함수의 간단한 구현입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b2e58d-a6ed-49f0-a1cd-2463e8d53a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1cbbb-4744-41cb-8910-f5c1355555fb",
   "metadata": {},
   "source": [
    "- 위와 같은 단순한 구현 방식은 오버플로우 및 언더플로우 문제로 인해 입력값이 크거나 작을 경우 수치적 불안정성을 보일 수 있습니다.\n",
    "- 따라서 실제로는 성능 최적화가 잘 되어 있는 PyTorch의 소프트맥스 구현을 사용하는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d99cac4-45ea-46b3-b3c1-e000ad16e158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e36c7-90b2-427f-94f6-bb9d31b2ab3f",
   "metadata": {},
   "source": [
    "- **3단계**: 내장된 입력 토큰 $x^{(i)}$와 어텐션 가중치를 곱하여 컨텍스트 벡터 $z^{(2)}$를 계산하고 결과 벡터들을 합산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c9f5ac-8d3d-4847-94e3-fd783b7d4d3d",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/10.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fcb96f0-14e5-4973-a50e-79ea7c6af99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a454262-40eb-430e-9ca4-e43fb8d6cd89",
   "metadata": {},
   "source": [
    "### 3.3.2 Computing attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a02bb73-fc19-4c88-b155-8314de5d63a8",
   "metadata": {},
   "source": [
    "#### 모든 입력 시퀀스 토큰으로 일반화:\n",
    "\n",
    "- 위에서 입력 2에 대한 어텐션 가중치와 컨텍스트 벡터를 계산했습니다(아래 그림의 강조 표시된 행 참조).\n",
    "- 다음으로, 이 계산을 일반화하여 모든 어텐션 가중치와 컨텍스트 벡터를 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0fb55-394f-42f4-ba07-d01ae5c98ab4",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/11.webp\" width=\"400px\">\n",
    "\n",
    "- (이 그림의 숫자는 시각적 혼란을 줄이기 위해 소수점 이하 두 자리까지만 표시되도록 잘렸습니다. 각 행의 값들을 더하면 1.0 또는 100%가 됩니다. 마찬가지로 다른 그림의 숫자도 잘렸습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b789b990-fb51-4beb-9212-bf58876b5983",
   "metadata": {},
   "source": [
    "- 셀프 어텐션에서는 먼저 어텐션 점수를 계산하고, 이를 정규화하여 합계가 1이 되는 어텐션 가중치를 도출합니다.\n",
    "- 이 어텐션 가중치는 입력값들의 가중 합산을 통해 컨텍스트 벡터를 생성하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bffe4b-56fe-4c37-9762-24bd924b7d3c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/12.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa652506-f2c8-473c-a905-85c389c842cc",
   "metadata": {},
   "source": [
    "\n",
    "- 이전 **1단계**를 모든 쌍 요소에 적용하여 정규화되지 않은 어텐션 점수 행렬을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04004be8-07a1-468b-ab33-32e16a551b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1539187f-1ece-47b7-bc9b-65a97115f1d4",
   "metadata": {},
   "source": [
    "- 위와 동일한 결과를 행렬 곱셈을 통해 더욱 효율적으로 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cea69d0-9a47-45da-8d5a-47ceef2df673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4bac4-acfd-427f-9b11-c436ac71748d",
   "metadata": {},
   "source": [
    "- 이전의 **2단계**와 유사하게, 각 행의 값들의 합이 1이 되도록 각 행을 정규화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa4ef062-de81-47ee-8415-bfe1708c81b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa6d02b-7f15-4eb4-83a7-0b8a819e7a0c",
   "metadata": {},
   "source": [
    "- 각 행의 값들의 합이 실제로 1인지 빠르게 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "112b492c-fb6f-4e6d-8df5-518ae83363d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b0b5c-d813-44c7-b373-fde9540ddfd1",
   "metadata": {},
   "source": [
    "- 이전 **3단계**를 적용하여 모든 컨텍스트 벡터를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba8eafcf-f7f7-4989-b8dc-61b50c4f81dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b245b8-7732-4fab-aa1c-e3d333195605",
   "metadata": {},
   "source": [
    "- 정상 작동 여부를 확인하기 위해 이전에 계산된 컨텍스트 벡터 $z^{(2)} = [0.4419, 0.6515, 0.5683]$는 위의 2번째 행에서 찾을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2570eb7d-aee1-457a-a61e-7544478219fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a303b6fb-9f7e-42bb-9fdb-2adabf0a6525",
   "metadata": {},
   "source": [
    "## 3.4 Implementing self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88363117-93d8-41fb-8240-f7cfe08b14a3",
   "metadata": {},
   "source": [
    "- 이 절에서 개발된 자기 주의 메커니즘이 이 책과 이 장의 전체적인 서술 및 구조에 어떻게 통합되는지를 보여주는 개념적 틀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9492ba-6f66-4f65-bd1d-87cf16d59928",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/13.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90a77e-d746-4704-9354-1ddad86e6298",
   "metadata": {},
   "source": [
    "### 3.4.1 Computing the attention weights step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e95a46-1f67-4b71-9e84-8e2db84ab036",
   "metadata": {},
   "source": [
    "- 이 섹션에서는 오리지널 트랜스포머 아키텍처, GPT 모델 및 대부분의 인기 있는 LLM에서 사용되는 셀프 어텐션 메커니즘을 구현합니다.\n",
    "- 이 셀프 어텐션 메커니즘은 \"스케일링된 도트 프로덕트 어텐션\"이라고도 합니다.\n",
    "- 전반적인 아이디어는 이전과 유사합니다.\n",
    "- 특정 입력 요소에 대한 입력 벡터의 가중 합으로 컨텍스트 벡터를 계산하고자 합니다.\n",
    "- 이를 위해서는 어텐션 가중치가 필요합니다.\n",
    "- 보시다시피, 이전에 소개된 기본 어텐션 메커니즘과 비교했을 때 약간의 차이점만 있습니다.\n",
    "\n",
    "- 가장 눈에 띄는 차이점은 모델 학습 중에 업데이트되는 가중치 행렬의 도입입니다.\n",
    "\n",
    "- 이러한 학습 가능한 가중치 행렬은 모델(특히 모델 내부의 어텐션 모듈)이 \"좋은\" 컨텍스트 벡터를 생성하도록 학습하는 데 매우 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db4093-93e8-4bee-be8f-c8fac8a08cdd",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/14.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d996671-87aa-45c9-b2e0-07a7bcc9060a",
   "metadata": {},
   "source": [
    "- 셀프 어텐션 메커니즘을 단계별로 구현하기 위해, 먼저 세 가지 학습 가중치 행렬 $W_q$, $W_k$, $W_v$를 소개합니다.\n",
    "- 이 세 행렬은 임베딩된 입력 토큰 $x^{(i)}$를 행렬 곱셈을 통해 쿼리, 키, 값 벡터로 변환하는 데 사용됩니다.\n",
    "\n",
    "- 쿼리 벡터: $q^{(i)} = x^{(i)}\\,W_q$\n",
    "\n",
    "- 키 벡터: $k^{(i)} = x^{(i)}\\,W_k$\n",
    "\n",
    "- 값 벡터: $v^{(i)} = x^{(i)}\\,W_v$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f334313-5fd0-477b-8728-04080a427049",
   "metadata": {},
   "source": [
    "- 입력 벡터 $x$와 쿼리 벡터 $q$의 임베딩 차원은 모델 설계 및 구체적인 구현 방식에 따라 같을 수도 있고 다를 수도 있습니다.\n",
    "- GPT 모델에서는 일반적으로 입력과 출력 차원이 같지만, 계산 과정을 더 잘 이해하기 위해 여기서는 입력과 출력 차원을 다르게 설정했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8250fdc6-6cd6-4c5b-b9c0-8c643aadb7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f528cfb3-e226-47dd-b363-cc2caaeba4bf",
   "metadata": {},
   "source": [
    "- 아래에서는 세 개의 가중치 행렬을 초기화합니다. 설명 목적으로 출력 화면을 간소화하기 위해 `requires_grad=False`로 설정했지만, 모델 학습에 가중치 행렬을 사용하려면 학습 중에 행렬이 업데이트되도록 `requires_grad=True`로 설정해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfd7259a-f26c-4cea-b8fc-282b5cae1e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd0b50-7701-4adb-821c-e5433622d9c4",
   "metadata": {},
   "source": [
    "- 다음으로 쿼리, 키 및 값 벡터를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73cedd62-01e1-4196-a575-baecc6095601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query # _2 because it's with respect to the 2nd input element\n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be308b3-aca3-421b-b182-19c3a03b71c7",
   "metadata": {},
   "source": [
    "- 아래에서 볼 수 있듯이, 6개의 입력 토큰을 3차원 공간에서 2차원 임베딩 공간으로 성공적으로 투영했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c1c3949-fc08-4d19-a41e-1c235b4e631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac5dfd6-ade8-4e7b-b0c1-bed40aa24481",
   "metadata": {},
   "source": [
    "- 다음 단계인 **2단계**에서는 쿼리 벡터와 각 키 벡터의 내적을 계산하여 정규화되지 않은 어텐션 점수를 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed0a2b7-5c50-4ede-90cf-7ad74412b3aa",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/15.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64cbc253-a182-4490-a765-246979ea0a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1] # Python starts index at 0\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d15c0-c24e-4e6f-a160-6349b418f935",
   "metadata": {},
   "source": [
    "- 입력값이 6개이므로, 주어진 쿼리 벡터에 대해 6개의 어텐션 스코어가 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b14e44b5-d170-40f9-8847-8990804af26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622cf39-155f-4eb5-a0c0-82a03ce9b999",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/16.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1609edb-f089-461a-8de2-c20c1bb29836",
   "metadata": {},
   "source": [
    "- 다음으로, **3단계**에서는 이전에 사용했던 소프트맥스 함수를 이용하여 어텐션 가중치(합이 1이 되는 정규화된 어텐션 점수)를 계산합니다.\n",
    "- 이전과 다른 점은 이제 어텐션 점수를 임베딩 차원의 제곱근, 즉 $\\sqrt{d_k}$(`d_k**0.5`)로 나누어 스케일링한다는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "146f5587-c845-4e30-9894-c7ed3a248153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f61a28-b103-434a-aee1-ae7cbd821126",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/17.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1890e3f9-db86-4ab8-9f3b-53113504a61f",
   "metadata": {},
   "source": [
    "\n",
    "- **4단계**에서는 입력 쿼리 벡터 2에 대한 컨텍스트 벡터를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e138f033-fa7e-4e3a-8764-b53a96b26397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b2907-e448-473e-b46c-77735a7281d8",
   "metadata": {},
   "source": [
    "### 3.4.2 Implementing a compact SelfAttention class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04313410-3155-4d90-a7a3-2f3386e73677",
   "metadata": {},
   "source": [
    "\n",
    "- 이 모든 것을 종합하면, 다음과 같이 셀프 어텐션 메커니즘을 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51590326-cdbe-4e62-93b1-17df71c11ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee1a024-84a5-425a-9567-54ab4e4ed445",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/18.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048e0c16-d911-4ec8-b0bc-45ceec75c081",
   "metadata": {},
   "source": [
    "- PyTorch의 Linear 레이어를 사용하면 위의 구현을 간소화할 수 있습니다. 바이어스 유닛을 비활성화하면 Linear 레이어는 행렬 곱셈과 동일한 기능을 합니다.\n",
    "- 수동으로 `nn.Parameter(torch.rand(...))`를 사용하는 방식 대신 `nn.Linear`를 사용하는 또 다른 큰 장점은 `nn.Linear`가 선호하는 가중치 초기화 방식을 제공하여 모델 학습을 더욱 안정적으로 만들어준다는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73f411e3-e231-464a-89fe-0a9035e5f839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915cd8a5-a895-42c9-8b8e-06b5ae19ffce",
   "metadata": {},
   "source": [
    "- `SelfAttention_v1`과 `SelfAttention_v2`는 가중치 행렬에 서로 다른 초기 가중치를 사용하기 때문에 서로 다른 출력을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5025b37-0f2c-4a67-a7cb-1286af7026ab",
   "metadata": {},
   "source": [
    "## 3.5 Hiding future words with causal attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef0a6b8-205a-45bf-9d26-8fd77a8a03c3",
   "metadata": {},
   "source": [
    "- 인과적 어텐션에서는 대각선 위의 어텐션 가중치가 마스킹되어, 주어진 입력에 대해 LLM이 어텐션 가중치를 사용하여 컨텍스트 벡터를 계산할 때 미래의 토큰을 활용할 수 없도록 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e91bb5-5aae-4f05-8a95-973b3f988a35",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/19.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f405de-cd86-4e72-8f3c-9ea0354946ba",
   "metadata": {},
   "source": [
    "### 3.5.1 Applying a causal attention mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f28d0-8218-48e4-8b9c-bdc5ce489218",
   "metadata": {},
   "source": [
    "- 이 섹션에서는 기존의 셀프 어텐션 메커니즘을 인과적 셀프 어텐션 메커니즘으로 변환합니다.\n",
    "- 인과적 셀프 어텐션은 시퀀스의 특정 위치에 대한 모델의 예측이 이전 위치의 알려진 출력에만 의존하고, 미래 위치에는 의존하지 않도록 합니다.\n",
    "- 간단히 말하면, 각 다음 단어 예측은 이전 단어에만 의존해야 합니다.\n",
    "- 이를 위해 각 토큰에 대해 미래 토큰(입력 텍스트에서 현재 토큰 다음에 오는 토큰)을 마스킹합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f99af3-32bc-48f5-8eb4-63504670ca0a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/20.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfaec7a-68f2-4157-a4b5-2aeceed199d9",
   "metadata": {},
   "source": [
    "- 인과적 자기 주의를 설명하고 구현하기 위해 이전 섹션에서 다룬 주의 점수와 가중치를 활용해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1933940d-0fa5-4b17-a3ce-388e5314a1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Reuse the query and key weight matrices of the\n",
    "# SelfAttention_v2 object from the previous section for convenience\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89020a96-b34d-41f8-9349-98c3e23fd5d6",
   "metadata": {},
   "source": [
    "- 향후 어텐션 가중치를 마스킹하는 가장 간단한 방법은 PyTorch의 tril 함수를 사용하여 마스크를 생성하는 것입니다. 이 마스크는 주 대각선 아래 요소(대각선 자체 포함)를 1로, 주 대각선 위 요소를 0으로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43f3d2e3-185b-4184-9f98-edde5e6df746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce2b08-3583-44da-b3fc-cabdd38761f6",
   "metadata": {},
   "source": [
    "- 그런 다음, 이 마스크를 어텐션 가중치에 곱하여 대각선 위의 어텐션 점수를 0으로 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f531e2e-f4d2-4fea-a87f-4c132e48b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb35787-cf12-4024-b66d-e7215e175500",
   "metadata": {},
   "source": [
    "- 하지만 위 예시처럼 소프트맥스 후에 마스크를 적용하면 소프트맥스가 생성한 확률 분포가 왜곡될 수 있습니다.\n",
    "- 소프트맥스는 모든 출력값의 합이 1이 되도록 합니다.\n",
    "- 소프트맥스 후에 마스킹을 적용하려면 출력값의 합이 다시 1이 되도록 재정규화해야 하므로 과정이 복잡해지고 의도치 않은 결과가 발생할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db92d7-c397-4e42-bd8a-6a2b3e237e0f",
   "metadata": {},
   "source": [
    "- 행들의 합이 1이 되도록 하려면 다음과 같이 어텐션 가중치를 정규화할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d392083-fd81-4f70-9bdf-8db985e673d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512e7cf4-dc0e-4cec-948e-c7a3c4eb6877",
   "metadata": {},
   "source": [
    "- 이제 인과적 어텐션 메커니즘 코딩은 사실상 완료되었지만, 위와 동일한 결과를 얻는 보다 효율적인 접근 방식을 간단히 살펴보겠습니다.\n",
    "- 대각선 위의 어텐션 가중치를 0으로 만들고 결과를 정규화하는 대신, 소프트맥스 함수에 들어가기 전에 대각선 위의 정규화되지 않은 어텐션 점수를 음의 무한대로 마스킹할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb682900-8df2-4767-946c-a82bee260188",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/21.webp\" width=\"450px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2be2f43-9cf0-44f6-8d8b-68ef2fb3cc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5f803-d735-4543-b9da-00ac10fb9c50",
   "metadata": {},
   "source": [
    "- 아래에서 볼 수 있듯이, 이제 각 행의 어텐션 가중치 합이 다시 1이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1cd6d7f-16f2-43c1-915e-0824f1a4bc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636fc5f-6bc6-461e-ac6a-99ec8e3c0912",
   "metadata": {},
   "source": [
    "### 3.5.2 Masking additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3dc7ee-6539-4fab-804a-8f31a890c85a",
   "metadata": {},
   "source": [
    "- 또한, 학습 중 과적합을 줄이기 위해 드롭아웃을 적용합니다.\n",
    "- 드롭아웃은 여러 위치에 적용할 수 있습니다.\n",
    "\n",
    "- 예를 들어, 어텐션 가중치를 계산한 후;\n",
    "\n",
    "- 또는 어텐션 가중치와 값 벡터를 곱한 후;\n",
    "\n",
    "- 여기서는 더 일반적인 방법인 어텐션 가중치 계산 후 드롭아웃 마스크를 적용합니다.\n",
    "\n",
    "- 또한, 이 특정 예제에서는 드롭아웃 비율을 50%로 설정했는데, 이는 어텐션 가중치의 절반을 무작위로 제거하는 것을 의미합니다. (나중에 GPT 모델을 학습할 때는 0.1 또는 0.2와 같은 더 낮은 드롭아웃 비율을 사용할 것입니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee799cf6-6175-45f2-827e-c174afedb722",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/22.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a575458-a6da-4e54-8688-83e155f2de06",
   "metadata": {},
   "source": [
    "- 만약 탈락률을 0.5(50%)로 적용하면, 탈락하지 않은 값은 1/0.5 = 2의 비율로 조정됩니다.\n",
    "- 조정 비율은 1 / (1 - `탈락률`) 공식으로 계산됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0de578db-8289-41d6-b377-ef645751e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) # dropout rate of 50%\n",
    "example = torch.ones(6, 6) # create a matrix of ones\n",
    "\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b16c5edb-942b-458c-8e95-25e4e355381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269df5c8-3e25-49d0-95d3-bb232287404f",
   "metadata": {},
   "source": [
    "- 드롭아웃 출력 결과는 운영 체제에 따라 다르게 보일 수 있습니다. 이러한 불일치에 대한 자세한 내용은 [PyTorch 이슈 트래커](https://github.com/pytorch/pytorch/issues/121595)에서 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc14639-5f0f-4840-aa9d-8eb36ea90fb7",
   "metadata": {},
   "source": [
    "### 3.5.3 Implementing a compact causal self-attention class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c41d29-1933-43dc-ada6-2dbb56287204",
   "metadata": {},
   "source": [
    "- 이제 인과 마스크와 드롭아웃 마스크를 포함한 셀프 어텐션의 실제 구현을 진행할 준비가 되었습니다.\n",
    "- 마지막으로, 2장에서 구현한 데이터 로더에서 생성된 배치 출력을 `CausalAttention` 클래스가 지원할 수 있도록 여러 개의 입력으로 구성된 배치를 처리하는 코드를 구현해야 합니다.\n",
    "- 간단하게 배치 입력을 시뮬레이션하기 위해 입력 텍스트 예제를 복제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "977a5fa7-a9d5-4e2e-8a32-8e0331ccfe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60d8c2eb-2d8e-4d2c-99bc-9eef8cc53ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        # For inputs where `num_tokens` exceeds `context_length`, this will result in errors\n",
    "        # in the mask creation further below.\n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forward method. \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4333d12-17e4-4bb5-9d83-54b3a32618cd",
   "metadata": {},
   "source": [
    "- 드롭아웃은 학습 과정에서만 적용되고 추론 과정에서는 적용되지 않는다는 점에 유의하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a554cf47-558c-4f45-84cd-bf9b839a8d50",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/23.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bef90f-cfd4-4289-b0e8-6a00dc9be44c",
   "metadata": {},
   "source": [
    "## 3.6 Extending single-head attention to multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11697757-9198-4a1c-9cee-f450d8bbd3b9",
   "metadata": {},
   "source": [
    "### 3.6.1 Stacking multiple single-head attention layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70766faf-cd53-41d9-8a17-f1b229756a5a",
   "metadata": {},
   "source": [
    "- 아래는 이전에 구현한 셀프 어텐션에 대한 요약입니다(간단히 설명하기 위해 인과 관계 및 드롭아웃 마스크는 생략했습니다).\n",
    "\n",
    "- 이를 싱글 헤드 어텐션이라고도 합니다.\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/24.webp\" width=\"400px\">\n",
    "\n",
    "- 단일 헤드 어텐션 모듈 여러 개를 쌓아서 멀티 헤드 어텐션 모듈을 만드는 것입니다.\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/25.webp\" width=\"400px\">\n",
    "\n",
    "- 멀티헤드 어텐션의 핵심 아이디어는 서로 다른 학습된 선형 투영을 사용하여 어텐션 메커니즘을 여러 번(병렬로) 실행하는 것입니다. 이를 통해 모델은 서로 다른 위치에 있는 다양한 표현 부분 공간의 정보에 동시에 주의를 기울일 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9a66e11-7105-4bb4-be84-041f1a1f3bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d3d2b-2578-40ba-b791-ea2d49328e48",
   "metadata": {},
   "source": [
    "- 위 구현에서 임베딩 차원은 4입니다. 키, 쿼리, 값 벡터와 컨텍스트 벡터 모두에 대해 `d_out=2`로 임베딩 차원을 설정했기 때문입니다. 또한 어텐션 헤드가 2개이므로 출력 임베딩 차원은 2*2=4가 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6836b5da-ef82-4b4c-bda1-72a462e48d4e",
   "metadata": {},
   "source": [
    "### 3.6.2 Implementing multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b48d0d-71ba-4fa0-b714-ca80cabcb6f7",
   "metadata": {},
   "source": [
    "- 위 코드는 직관적이고 완벽하게 작동하는 멀티 헤드 어텐션 구현(이전의 싱글 헤드 어텐션 `CausalAttention` 구현을 래핑한 것)이지만, 동일한 기능을 구현하는 `MultiHeadAttention`이라는 독립형 클래스를 작성할 수도 있습니다.\n",
    "\n",
    "- 이 독립형 `MultiHeadAttention` 클래스에서는 여러 개의 어텐션 헤드를 연결하지 않습니다.\n",
    "- 대신, W_query, W_key, W_value 가중치 행렬을 각각 생성한 다음, 각 어텐션 헤드에 대해 개별 행렬로 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "110b0188-6e9e-4e56-a988-10523c6c8538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, \n",
    "        # this will result in errors in the mask creation further below. \n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forward method.\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d334dfb5-2b6c-4c33-82d5-b4e9db5867bb",
   "metadata": {},
   "source": [
    "- 위 코드는 `MultiHeadAttentionWrapper`를 더욱 효율적으로 재작성한 버전입니다.\n",
    "- 무작위 가중치 초기화 방식이 다르기 때문에 출력 결과가 약간 다르게 보일 수 있지만, 두 코드 모두 향후 장에서 구현할 GPT 클래스에서 사용할 수 있는 완전한 기능을 갖춘 구현입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd41e1-32d4-4067-a6d0-fe756a6511a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**출력 차원에 대한 참고 사항**\n",
    "\n",
    "- 위의 `MultiHeadAttention`에서 `d_out=2`를 사용한 것은 앞서 설명한 `MultiHeadAttentionWrapper` 클래스와 동일한 설정을 사용하기 위함입니다.\n",
    "- `MultiHeadAttentionWrapper`는 헤드들을 연결하는 방식으로 인해 출력 헤드 차원을 `d_out * num_heads` (즉, `2*2 = 4`)로 반환합니다.\n",
    "- 하지만 `MultiHeadAttention` 클래스는 (사용자 편의성을 높이기 위해) `d_out`을 통해 출력 헤드 차원을 직접 제어할 수 있도록 합니다. 즉, `d_out = 2`로 설정하면 헤드 개수와 관계없이 출력 헤드 차원이 2가 됩니다.\n",
    "- 돌이켜보면, 독자들이 지적했듯이(https://github.com/rasbt/LLMs-from-scratch/pull/859), `MultiHeadAttention`을 `d_out = 4`로 사용하는 것이 `MultiHeadAttentionWrapper`를 `d_out = 2`로 사용할 때와 동일한 출력 차원을 생성하므로 더 직관적일 수 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9310bfa5-9aa9-40b4-8081-a5d8db5faf74",
   "metadata": {},
   "source": [
    "- 참고로, 위의 `MultiHeadAttention` 클래스에 선형 투영 레이어(`self.out_proj`)를 추가했습니다. 이는 차원을 변경하지 않는 단순한 선형 변환입니다. LLM 구현에서 이러한 투영 레이어를 사용하는 것은 일반적인 관례이지만, 반드시 필요한 것은 아닙니다(최근 연구에 따르면 모델링 성능에 영향을 미치지 않고 제거할 수 있습니다. 이 장 끝부분의 참고 문헌 섹션을 참조하십시오)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5d396-c990-45dc-9908-2c621461f851",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/26.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ed78c-e8ac-4f8f-a479-a98242ae8f65",
   "metadata": {},
   "source": [
    "- 참고로, 위 내용을 간결하고 효율적으로 구현하고 싶다면 PyTorch의 `torch.nn.MultiheadAttention` 클래스를 사용하는 것도 고려해 볼 수 있습니다(https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363701ad-2022-46c8-9972-390d2a2b9911",
   "metadata": {},
   "source": [
    "- 위 구현 방식이 처음 보면 다소 복잡해 보일 수 있으므로, `attn_scores = queries @ keys.transpose(2, 3)`를 실행했을 때 어떤 일이 발생하는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8cfc1ae-78ab-4faa-bc73-98bd054806c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "# (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4)\n",
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "\n",
    "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "\n",
    "print(a @ a.transpose(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0587b946-c8f2-4888-adbf-5a5032fbfd7b",
   "metadata": {},
   "source": [
    "- In this case, the matrix multiplication implementation in PyTorch will handle the 4-dimensional input tensor so that the matrix multiplication is carried out between the 2 last dimensions (num_tokens, head_dim) and then repeated for the individual heads \n",
    "\n",
    "- For instance, the following becomes a more compact way to compute the matrix multiplication for each head separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "053760f1-1a02-42f0-b3bf-3d939e407039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec671bf-7938-4304-ad1e-75d9920e7f43",
   "metadata": {},
   "source": [
    "# Summary and takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e4113-ffca-432c-b3ec-7a50bd15da25",
   "metadata": {},
   "source": [
    "- See the [./multihead-attention.ipynb](./multihead-attention.ipynb) code notebook, which is a concise version of the data loader (chapter 2) plus the multi-head attention class that we implemented in this chapter and will need for training the GPT model in upcoming chapters\n",
    "- You can find the exercise solutions in [./exercise-solutions.ipynb](./exercise-solutions.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
